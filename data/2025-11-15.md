<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Blind and Low-Vision Accessibility of Lightweight VLMs and Custom LLM-Evals](https://arxiv.org/abs/2511.10615)
*Shruti Singh Baghel,Yash Pratap Singh Rathore,Sushovan Jena,Anurag Pradhan,Amit Shukla,Arnav Bhavsar,Pawan Goyal*

Main category: cs.CV

TL;DR: 本文研究了不同参数规模（500M和2.2B）的SmolVLM2模型在盲人和低视力用户视频描述任务中的表现，引入了两个专门的可访问性评估框架，并评估了不同提示策略和移动设备部署方案。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型虽然能生成高质量视频描述，但其高内存、计算和部署需求限制了实际应用，特别是对依赖详细上下文感知描述的盲人和低视力用户。

Method: 使用SmolVLM2的500M和2.2B参数变体，在AVCaps（户外）和Charades（室内）数据集上评估；引入两个新评估框架：多上下文BLV框架和导航辅助框架；系统评估四种提示设计策略；在智能手机上部署FP32和INT8精度变体。

Result: 未在摘要中明确说明具体实验结果，但评估了模型在资源受限移动设备上的实际性能约束。

Conclusion: 研究模型规模对可访问性描述质量的影响，为盲人和低视力用户开发更实用的视频描述系统提供了重要参考。

Abstract: Large Vision-Language Models (VLMs) excel at understanding and generating video descriptions but their high memory, computation, and deployment demands hinder practical use particularly for blind and low-vision (BLV) users who depend on detailed, context-aware descriptions. To study the effect of model size on accessibility-focused description quality, we evaluate SmolVLM2 variants with 500M and 2.2B parameters across two diverse datasets: AVCaps (outdoor), and Charades (indoor). In this work, we introduce two novel evaluation frameworks specifically designed for BLV accessibility assessment: the Multi-Context BLV Framework evaluating spatial orientation, social interaction, action events, and ambience contexts; and the Navigational Assistance Framework focusing on mobility-critical information. Additionally, we conduct a systematic evaluation of four different prompt design strategies and deploy both models on a smartphone, evaluating FP32 and INT8 precision variants to assess real-world performance constraints on resource-limited mobile devices.

</details>


### [2] [Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling](https://arxiv.org/abs/2511.10648)
*Jiahao Wang,Weiye Xu,Aijun Yang,Wengang Zhou,Lewei Lu,Houqiang Li,Xiaohua Wang,Jinguo Zhu*

Main category: cs.CV

TL;DR: 本文提出自一致性采样（SCS）方法，解决多模态大语言模型在强化学习中因错误推理链却猜对答案而获得相同奖励的问题。


<details>
  <summary>Details</summary>
Motivation: 在多模态推理基准测试的多选题设置中，强化学习面临一个被忽视的障碍：即使推理链错误但猜对正确答案的轨迹与真正正确推理的轨迹获得相同奖励，这影响了学习效果。

Method: SCS方法通过（i）引入小的视觉扰动和（ii）对初始轨迹进行重复截断和重采样，基于结果轨迹的一致性产生可微一致性分数，在策略更新时降低不可靠轨迹的权重。

Result: 在Qwen2.5-VL-7B-Instruct上，将SCS集成到RLOO、GRPO和REINFORCE++系列中，在六个多模态基准测试上准确率提升高达7.7个百分点，且计算开销可忽略。SCS在Qwen2.5-VL-3B-Instruct和InternVL3-8B上也取得显著提升。

Conclusion: SCS为多模态大语言模型中的结果奖励强化学习提供了一种简单通用的解决方案。

Abstract: Outcome-reward reinforcement learning (RL) is a common and increasingly significant way to refine the step-by-step reasoning of multimodal large language models (MLLMs). In the multiple-choice setting - a dominant format for multimodal reasoning benchmarks - the paradigm faces a significant yet often overlooked obstacle: unfaithful trajectories that guess the correct option after a faulty chain of thought receive the same reward as genuine reasoning, which is a flaw that cannot be ignored. We propose Self-Consistency Sampling (SCS) to correct this issue. For each question, SCS (i) introduces small visual perturbations and (ii) performs repeated truncation and resampling of an initial trajectory; agreement among the resulting trajectories yields a differentiable consistency score that down-weights unreliable traces during policy updates. Based on Qwen2.5-VL-7B-Instruct, plugging SCS into RLOO, GRPO, and REINFORCE++ series improves accuracy by up to 7.7 percentage points on six multimodal benchmarks with negligible extra computation. SCS also yields notable gains on both Qwen2.5-VL-3B-Instruct and InternVL3-8B, offering a simple, general remedy for outcome-reward RL in MLLMs.

</details>
