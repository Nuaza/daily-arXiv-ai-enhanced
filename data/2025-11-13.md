<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 47]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants](https://arxiv.org/abs/2511.08609)
*I. Bailo,F. Buonora,G. Ciarfaglia,L. T. Consoli,A. Evangelista,M. Gabusi,M. Ghiani,C. Petracca Ciavarella,F. Picariello,F. Sarcina,F. Tuosto,V. Zullo,L. Airoldi,G. Bruno,D. D. Gobbo,S. Pezzenati,G. A. Tona*

Main category: cs.CV

TL;DR: 本文提出了一种基于生成式人工智能的解决方案，用于自动化SNAM能源基础设施的工厂结构数字化，通过结合OCR、视觉LLM、目标检测、关系推理和优化算法，实现了91%的设计数据提取准确率和93%的组件识别准确率。


<details>
  <summary>Details</summary>
Motivation: 能源转型背景下，需要数字化和创新技术工具来支持生态可持续发展。本文旨在设计基于人工智能的解决方案，自动化工厂数字化过程，简化MGM用户的日常工作。

Method: 采用OCR、视觉LLM、目标检测、关系推理和优化算法的协同组合，并扩展了最先进的场景图生成模型，引入全新的Transformer架构来深入分析工厂组件间的复杂关系。

Result: 设计数据文本信息提取准确率达到91%，组件识别准确率为93%，层次结构提取准确率约为80%。

Conclusion: 所提出的AI技术协同使用成功克服了数据多样性带来的挑战，为能源基础设施的自动化数字化提供了有效的解决方案。

Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.

</details>


### [2] [Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework](https://arxiv.org/abs/2511.08613)
*Dogucan Yaman,Fevziye Irem Eyiokur,Hazım Kemal Ekenel,Alexander Waibel*

Main category: cs.CV

TL;DR: 本文提出了一种系统性的评估方法来分析和量化基于修复的说话人脸生成中的嘴唇泄漏问题，该方法包含三种互补的测试设置和派生指标。


<details>
  <summary>Details</summary>
Motivation: 基于修复的说话人脸生成方法在保持视频细节的同时修改嘴唇运动，但会引入嘴唇泄漏问题，即生成的嘴唇受到参考图像影响而非仅由驱动音频决定，而标准指标难以检测这种泄漏。

Method: 提出包含三种测试设置的系统评估框架：静默输入生成、不匹配的音频-视频配对和匹配的音频-视频合成，并引入嘴唇同步差异和基于静默音频的嘴唇同步分数等派生指标。

Result: 该方法能够有效分析和量化嘴唇泄漏问题，并研究了不同身份参考选择对泄漏的影响，为参考设计提供见解。

Conclusion: 所提出的方法是模型无关的，为说话人脸生成领域的未来研究建立了更可靠的基准。

Abstract: Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.

</details>


### [3] [A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking](https://arxiv.org/abs/2511.08615)
*Kosta Dakic,Kanchana Thilakarathna,Rodrigo N. Calheiros,Teng Joon Lim*

Main category: cs.CV

TL;DR: 本文提出了MATRIX数据集和深度学习框架，用于解决多无人机动态监控中的行人跟踪挑战，在复杂遮挡环境下保持约90%的检测和跟踪精度。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理动态相机位置和复杂遮挡问题，多无人机监控系统需要更强大的解决方案来应对真实世界中的挑战。

Method: 提出MATRIX数据集（8个动态位置无人机同步视频）和深度学习框架，包括实时相机标定、基于特征的图像配准、鸟瞰图中的多视角特征融合。

Result: 在复杂环境下保持约90%检测跟踪精度，成功跟踪约80%轨迹，迁移学习显示强泛化能力，相机丢失实验显示性能优雅下降。

Conclusion: MATRIX数据集和框架为动态多视角监控系统提供了重要基准，在复杂环境中表现优于静态相机方法。

Abstract: Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\sim$90\% detection and tracking accuracy, as well as successfully tracks $\sim$80\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.

</details>


### [4] [CADIC: Continual Anomaly Detection Based on Incremental Coreset](https://arxiv.org/abs/2511.08634)
*Gen Yang,Zhipeng Deng,Junfeng Man*

Main category: cs.CV

TL;DR: 提出了一种新型持续异常检测框架，采用统一内存库而非任务特定内存库，通过最近邻匹配机制实现最先进的检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有基于嵌入的持续异常检测方法需要为每个任务构建特定内存库的限制，提高方法的灵活性和可扩展性。

Method: 使用统一内存库，在固定大小的核心集中增量更新嵌入，通过最近邻匹配机制计算异常分数。

Result: 在MVTec AD和Visa数据集上分别达到0.972和0.891的平均图像级AUROC分数，在真实电子纸数据集上实现100%异常样本检测准确率。

Conclusion: 该方法在保持高检测精度的同时显著提高了持续异常检测的灵活性和可扩展性，适用于实际应用场景。

Abstract: The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.

</details>


### [5] [Predict and Resist: Long-Term Accident Anticipation under Sensor Noise](https://arxiv.org/abs/2511.08640)
*Xingcheng Liu,Bin Rao,Yanchen Guan,Chengyue Wang,Haicheng Liao,Jiaxun Zhang,Chengyu Lin,Meixin Zhu,Zhenning Li*

Main category: cs.CV

TL;DR: 提出一个统一框架，结合扩散去噪和时间感知的actor-critic模型，用于自动驾驶中的事故预测，在传感器退化和噪声条件下实现早期可靠预警。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶事故预测中的两个关键挑战：噪声或退化的传感器输入，以及需要平衡早期预警与误报抑制的及时可靠预测。

Method: 集成扩散去噪模块（通过迭代细化重建噪声弹性特征）和时间感知actor-critic模型（利用长期时序推理和时间加权奖励确定最佳预警时机）。

Result: 在三个基准数据集（DAD、CCD、A3D）上实现最先进精度，显著提高平均事故时间，在高斯和脉冲噪声下保持鲁棒性能。

Conclusion: 该模型在常规和复杂交通场景中产生更早、更稳定且与人类对齐的预测，具有实际安全关键部署潜力。

Abstract: Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.

</details>


### [6] [Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding](https://arxiv.org/abs/2511.08666)
*Joseph Fioresi,Ishan Rajendrakumar Dave,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种在潜在空间中保护视频基础模型视觉隐私的新方法，通过轻量级匿名适配器模块从视频特征中移除私人信息，同时保持任务实用性。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方法主要关注输入像素级的匿名化，需要重新训练整个实用视频模型，导致任务特定的匿名化，不适合现代视频基础模型。视频特征共享会无意中泄露敏感个人信息。

Method: 引入轻量级匿名适配器模块（AAM），采用三种新设计的训练目标：剪辑级自监督隐私目标、协同训练目标和潜在一致性损失，可在冻结的视频编码器上即插即用。

Result: 评估显示隐私泄露显著减少35%，同时在动作识别、时序动作检测和异常检测等下游任务中保持接近基线的实用性能，有效减轻性别偏见。

Conclusion: 该方法为视频基础模型提供了一种高效、通用的隐私保护解决方案，能够在保护隐私的同时维持模型实用性，并促进更公平的视频理解。

Abstract: We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.

</details>


### [7] [Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?](https://arxiv.org/abs/2511.08704)
*Xinchen Yan,Chen Liang,Lijun Yu,Adams Wei Yu,Yifeng Lu,Quoc V. Le*

Main category: cs.CV

TL;DR: 本文研究了自回归逐像素预测的扩展特性，发现最优扩展策略高度依赖任务类型，且随着图像分辨率增加，模型规模需要比数据规模增长得更快。计算能力而非训练数据量是主要瓶颈。


<details>
  <summary>Details</summary>
Motivation: 探索自回归逐像素预测这一简单但未被充分研究的统一视觉模型框架的扩展特性，了解不同任务下的最优扩展策略。

Method: 从32x32分辨率图像开始，训练一系列Transformer模型，使用IsoFlops配置文件在高达7e19 FLOPs的计算预算下进行评估，考察三个目标指标：逐像素预测目标、ImageNet分类准确率和生成质量。

Result: 发现最优扩展策略高度任务依赖：在固定32x32分辨率下，图像分类和图像生成的最优扩展策略不同，生成任务需要数据规模比分类任务快3-5倍增长。随着分辨率增加，模型规模需要比数据规模增长得更快。计算能力是主要瓶颈而非训练数据量。

Conclusion: 基于研究结果预测，随着计算能力每年增长4-5倍，逐像素图像建模在未来五年内是可行的。

Abstract: This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.

</details>


### [8] [Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification](https://arxiv.org/abs/2511.08711)
*Abhipsa Basu,Aviral Gupta,Abhijnya Bhat,R. Venkatesh Babu*

Main category: cs.CV

TL;DR: 本文研究使用扩散模型微调技术（LoRA和DreamBooth）生成平衡的训练数据来解决图像分类中的群体偏见问题，通过聚类和分组训练提高数据生成质量，在多个基准测试中表现优于原始Stable Diffusion，并与最先进的去偏技术相媲美。


<details>
  <summary>Details</summary>
Motivation: 图像分类系统常因训练数据中群体分布不均而继承偏见，例如人脸数据集中金发与女性的过度关联。现有方法使用Stable Diffusion生成平衡数据但难以保持原始数据分布，因此需要更精确的数据生成技术。

Method: 探索多种扩散模型微调技术（LoRA和DreamBooth），对每个训练群体直接学习其样本特征。为防止单个DreamBooth模型被组内变异淹没，采用组内图像聚类并为每个聚类训练单独的DreamBooth模型，然后生成群体平衡数据用于预训练，最后在真实数据上进行微调。

Result: 在多个基准测试中，研究的微调方法平均优于原始Stable Diffusion，与最先进的去偏技术（如Group-DRO）结果相当，且随着数据集偏见严重程度的增加，表现超过这些方法。

Conclusion: 扩散模型微调技术能够有效生成更准确的平衡训练数据，缓解图像分类中的群体偏见问题，特别是在偏见严重的数据集上表现出优越性能。

Abstract: Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.

</details>


### [9] [DT-NVS: Diffusion Transformers for Novel View Synthesis](https://arxiv.org/abs/2511.08823)
*Wonbong Jang,Jonathan Tremblay,Lourdes Agapito*

Main category: cs.CV

TL;DR: DT-NVS是一个基于3D扩散模型的广义新视角合成方法，使用transformer架构从单张图像生成真实世界场景的多样化新视角，解决了现有方法在真实场景中视角变化有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法主要关注小范围相机移动或仅处理非自然的物体中心场景，限制了在真实世界环境中的应用潜力。本文旨在解决从单张图像生成真实世界日常场景新视角这一未充分探索的问题。

Method: 提出DT-NVS 3D感知扩散模型，采用transformer架构骨干，改进了transformer和自注意力架构以将图像转换为3D表示，并开发了新的相机条件策略以在真实世界未对齐数据集上训练。引入新颖的训练范式，在条件图像和采样噪声输入之间交换参考帧角色。

Result: 在单图像广义新视角合成的3D任务评估中，相比最先进的3D感知扩散模型和确定性方法有所改进，同时生成多样化的输出。

Conclusion: DT-NVS在真实世界未对齐数据集上成功训练，能够从单张图像生成多样化的新视角，在广义新视角合成任务上优于现有方法。

Abstract: Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.

</details>


### [10] [Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms](https://arxiv.org/abs/2511.08833)
*Jiaxun Guo,Manar Amayri,Nizar Bouguila,Xin Liu,Wentao Fan*

Main category: cs.CV

TL;DR: 本文提出了一种新的旋转不变学习方法，通过引入阴影信息姿态特征(SiPF)和旋转不变注意力卷积(RIAttnConv)，解决了现有方法因感受野受限导致的翼尖特征崩溃问题，能够在保持旋转不变性的同时保留全局姿态信息。


<details>
  <summary>Details</summary>
Motivation: 现有旋转不变学习方法使用手工制作的旋转不变特征替代原始坐标，虽然对任意旋转具有鲁棒性，但会丢失全局姿态信息，无法区分几何相似但空间位置不同的结构（如飞机的左右机翼）。

Method: 提出了阴影信息姿态特征(SiPF)，通过学习的共享旋转生成全局一致的参考点（阴影），将局部旋转不变描述符与全局姿态信息结合；设计了旋转不变注意力卷积(RIAttnConv)来集成SiPF；开发了基于Bingham分布的任务自适应阴影定位模块。

Result: 在3D分类和部件分割基准测试上的大量实验表明，该方法显著优于现有旋转不变方法，特别是在需要细粒度空间区分能力的任务中表现突出。

Conclusion: 通过引入阴影机制和注意力卷积，该方法成功解决了旋转不变学习中的全局姿态信息丢失问题，为3D点云处理提供了更有效的旋转不变表示。

Abstract: Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.

</details>


### [11] [Classifying Histopathologic Glioblastoma Sub-regions with EfficientNet](https://arxiv.org/abs/2511.08896)
*Sanyukta Adap,Ujjwal Baid,Spyridon Bakas*

Main category: cs.CV

TL;DR: 本研究开发了一种基于EfficientNet的四步深度学习方法来分类胶质母细胞瘤的6个组织病理学亚区域，在BraTS-Path 2024挑战数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤预后极差，需要自动、稳健且准确的组织病理学亚区域识别方法来大规模理解该疾病的形态学特征。

Method: 使用EfficientNet架构的多个变体（B0-B4），在BraTS-Path 2024挑战数据集上开发四步深度学习分类方法。

Result: EfficientNet-B1和B4在5折交叉验证中达到0.98的F1分数，但在验证集和测试集上分别降至0.546和0.517，显示模型泛化能力挑战。

Conclusion: 虽然模型在训练数据上表现优异，但在新数据上的泛化能力有限，这对临床应用至关重要，需要进一步改进。

Abstract: Glioblastoma (GBM) is the most common aggressive, fast-growing brain tumor, with a grim prognosis. Despite clinical diagnostic advancements, there have not been any substantial improvements to patient prognosis. Histopathological assessment of excised tumors is the first line of clinical diagnostic routine. We hypothesize that automated, robust, and accurate identification of distinct histological sub-regions within GBM could contribute to morphologically understanding this disease at scale. In this study, we designed a four-step deep learning approach to classify six (6) histopathological regions and quantitatively evaluated it on the BraTS-Path 2024 challenge dataset, which includes digitized Hematoxylin \& Eosin (H\&E) stained GBM tissue sections annotated for six distinct regions. We used the challenge's publicly available training dataset to develop and evaluate the effectiveness of several variants of EfficientNet architectures (i.e., B0, B1, B2, B3, B4). EfficientNet-B1 and EfficientNet-B4 achieved the best performance, achieving an F1 score of 0.98 in a 5-fold cross-validation configuration using the BraTS-Path training set. The quantitative performance evaluation of our proposed approach with EfficientNet-B1 on the BraTS-Path hold-out validation data and the final hidden testing data yielded F1 scores of 0.546 and 0.517, respectively, for the associated 6-class classification task. The difference in the performance on training, validation, and testing data highlights the challenge of developing models that generalize well to new data, which is crucial for clinical applications. The source code of the proposed approach can be found at the GitHub repository of Indiana University Division of Computational Pathology: https://github.com/IUCompPath/brats-path-2024-enet.

</details>


### [12] [Improving VisNet for Object Recognition](https://arxiv.org/abs/2511.08897)
*Mehdi Fatan Serj,C. Alejandro Parraga,Xavier Otazu*

Main category: cs.CV

TL;DR: 本研究探讨了VisNet及其增强变体在物体识别和对称性分类中的表现，通过引入径向基函数神经元、马氏距离学习和视网膜预处理等方法，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 生物视觉系统在物体识别方面表现出色，但在人工系统中重现这种能力仍具挑战性。研究旨在验证VisNet及其生物启发变体在视觉识别任务中的有效性。

Method: 采用VisNet模型及其增强变体，结合Hebbian学习、时间连续性、径向基函数神经元、马氏距离学习和视网膜预处理，构建不变性表征。

Result: 在MNIST、CIFAR10和自定义对称物体数据集上的实验表明，增强版VisNet变体相比基线模型显著提高了识别准确率。

Conclusion: VisNet启发架构具有适应性和生物相关性，为神经科学和人工智能中的视觉识别提供了强大且可解释的框架。

Abstract: Object recognition plays a fundamental role in how biological organisms perceive and interact with their environment. While the human visual system performs this task with remarkable efficiency, reproducing similar capabilities in artificial systems remains challenging. This study investigates VisNet, a biologically inspired neural network model, and several enhanced variants incorporating radial basis function neurons, Mahalanobis distance based learning, and retinal like preprocessing for both general object recognition and symmetry classification. By leveraging principles of Hebbian learning and temporal continuity associating temporally adjacent views to build invariant representations. VisNet and its extensions capture robust and transformation invariant features. Experimental results across multiple datasets, including MNIST, CIFAR10, and custom symmetric object sets, show that these enhanced VisNet variants substantially improve recognition accuracy compared with the baseline model. These findings underscore the adaptability and biological relevance of VisNet inspired architectures, offering a powerful and interpretable framework for visual recognition in both neuroscience and artificial intelligence.
  Keywords: VisNet, Object Recognition, Symmetry Detection, Hebbian Learning, RBF Neurons, Mahalanobis Distance, Biologically Inspired Models, Invariant Representations

</details>


### [13] [Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency](https://arxiv.org/abs/2511.08901)
*Riling Wei,Kelu Yao,Chuanguang Yang,Jin Wang,Zhuoyan Gao,Chao Li*

Main category: cs.CV

TL;DR: 本文提出了一种非对称跨模态知识蒸馏（ACKD）方法SemBridge，用于处理语义一致性较弱的不同模态之间的知识迁移，通过学生友好匹配模块和语义感知知识对齐模块解决知识传输成本问题。


<details>
  <summary>Details</summary>
Motivation: 现实场景中配对模态数据有限，对称跨模态知识蒸馏（SCKD）应用受限。本文旨在研究在弱语义一致性下的跨模态知识蒸馏，以连接语义重叠有限的模态。

Method: 提出SemBridge框架，包含两个模块：1）学生友好匹配模块，利用自监督学习获取语义知识并为每个学生样本动态选择相关教师样本；2）语义感知知识对齐模块，采用拉格朗日优化寻找最优传输路径。

Result: 在遥感场景分类任务中，使用多光谱和RGB图像构建基准数据集。实验表明，该框架在6种不同模型架构和多个数据集上相比7种现有方法达到了最先进性能。

Conclusion: SemBridge框架有效解决了弱语义一致性下的跨模态知识蒸馏问题，在遥感场景分类等任务中表现出优越性能。

Abstract: Cross-modal Knowledge Distillation has demonstrated promising performance on paired modalities with strong semantic connections, referred to as Symmetric Cross-modal Knowledge Distillation (SCKD). However, implementing SCKD becomes exceedingly constrained in real-world scenarios due to the limited availability of paired modalities. To this end, we investigate a general and effective knowledge learning concept under weak semantic consistency, dubbed Asymmetric Cross-modal Knowledge Distillation (ACKD), aiming to bridge modalities with limited semantic overlap. Nevertheless, the shift from strong to weak semantic consistency improves flexibility but exacerbates challenges in knowledge transmission costs, which we rigorously verified based on optimal transport theory. To mitigate the issue, we further propose a framework, namely SemBridge, integrating a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module. The former leverages self-supervised learning to acquire semantic-based knowledge and provide personalized instruction for each student sample by dynamically selecting the relevant teacher samples. The latter seeks the optimal transport path by employing Lagrangian optimization. To facilitate the research, we curate a benchmark dataset derived from two modalities, namely Multi-Spectral (MS) and asymmetric RGB images, tailored for remote sensing scene classification. Comprehensive experiments exhibit that our framework achieves state-of-the-art performance compared with 7 existing approaches on 6 different model architectures across various datasets.

</details>


### [14] [LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis](https://arxiv.org/abs/2511.08903)
*Ibne Farabi Shihab,Sanjeda Akter,Anuj Sharma*

Main category: cs.CV

TL;DR: 提出一种融合视觉预测与LLM结构先验的半监督文档布局理解框架，通过概率加权提升检测性能，在少量标注数据下达到接近全监督方法的性能。


<details>
  <summary>Details</summary>
Motivation: 文档布局理解任务仍然需要大量标注数据，现有半监督方法未能充分利用文本结构信息。

Method: 使用OCR-LLM流水线推断层次化区域，通过逆方差融合将教师检测器输出与LLM结构先验结合生成精炼伪标签。

Result: 在PubLayNet上，使用5%标注数据：轻量级SwiftFormer达到88.2±0.3 AP；LayoutLMv3达到89.7±0.4 AP，超越标准半监督方法。

Conclusion: LLM结构先验与视觉检测器互补，可显著提升半监督文档布局理解性能，同时支持隐私保护部署。

Abstract: Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\pm$0.3 AP using only 5\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\pm$0.4 AP, p=0.02) and matching UDOP~\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.

</details>


### [15] [HitoMi-Cam: A Shape-Agnostic Person Detection Method Using the Spectral Characteristics of Clothing](https://arxiv.org/abs/2511.08908)
*Shuji Ono*

Main category: cs.CV

TL;DR: 本文提出了HitoMi-Cam，一种基于光谱反射特性的轻量级、形状无关的人员检测方法，可在无GPU的边缘设备上实现实时检测，在救援场景中性能优于CNN模型。


<details>
  <summary>Details</summary>
Motivation: 解决CNN目标检测对形状的依赖性，特别是在训练数据中未包含的姿态会导致性能下降的问题，为灾难救援等形状不可预测的场景提供实用解决方案。

Method: 利用服装的光谱反射特性进行人员检测，在资源受限的边缘设备上实现HitoMi-Cam系统，无需GPU支持。

Result: 处理速度达到23.2 fps（253x190像素），在模拟搜救场景中平均精度达到93.5%，优于对比CNN模型的最佳精度53.8%，且误报率极低。

Conclusion: HitoMi-Cam不是替代CNN检测器的方案，而是在特定条件下的补充工具，光谱检测方法在形状不可预测的真实环境中具有实用价值。

Abstract: While convolutional neural network (CNN)-based object detection is widely used, it exhibits a shape dependency that degrades performance for postures not included in the training data. Building upon our previous simulation study published in this journal, this study implements and evaluates the spectral-based approach on physical hardware to address this limitation. Specifically, this paper introduces HitoMi-Cam, a lightweight and shape-agnostic person detection method that uses the spectral reflectance properties of clothing. The author implemented the system on a resource-constrained edge device without a GPU to assess its practical viability. The results indicate that a processing speed of 23.2 frames per second (fps) (253x190 pixels) is achievable, suggesting that the method can be used for real-time applications. In a simulated search and rescue scenario where the performance of CNNs declines, HitoMi-Cam achieved an average precision (AP) of 93.5%, surpassing that of the compared CNN models (best AP of 53.8%). Throughout all evaluation scenarios, the occurrence of false positives remained minimal. This study positions the HitoMi-Cam method not as a replacement for CNN-based detectors but as a complementary tool under specific conditions. The results indicate that spectral-based person detection can be a viable option for real-time operation on edge devices in real-world environments where shapes are unpredictable, such as disaster rescue.

</details>


### [16] [Negative Entity Suppression for Zero-Shot Captioning with Synthetic Images](https://arxiv.org/abs/2511.08909)
*Zimao Lu,Hui Xu,Bing Liu,Ke Wang*

Main category: cs.CV

TL;DR: 本文提出负实体抑制(NES)方法解决零样本图像描述中的幻觉问题，通过合成图像、负实体过滤和注意力级抑制来提升跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决仅文本训练在零样本图像描述中的跨域泛化差和幻觉问题，特别是检索方法可能加剧幻觉的挑战。

Method: 提出负实体抑制(NES)方法，包含三个阶段：使用合成图像确保图像到文本检索的一致性；过滤检索内容中的负实体；应用注意力级抑制来减少幻觉倾向特征的影响。

Result: 在多个基准测试中，NES保持了竞争力的域内性能，同时提升了跨域迁移能力并降低了幻觉率，在零样本图像描述中达到了新的最先进结果。

Conclusion: NES方法有效解决了零样本图像描述中的幻觉问题，通过负实体抑制机制显著提升了跨域泛化性能。

Abstract: Text-only training provides an attractive approach to address data scarcity challenges in zero-shot image captioning (ZIC), avoiding the expense of collecting paired image-text annotations. However, although these approaches perform well within training domains, they suffer from poor cross-domain generalization, often producing hallucinated content when encountering novel visual environments. Retrieval-based methods attempt to mitigate this limitation by leveraging external knowledge, but they can paradoxically exacerbate hallucination when retrieved captions contain entities irrelevant to the inputs. We introduce the concept of negative entities--objects that appear in generated caption but are absent from the input--and propose Negative Entity Suppression (NES) to tackle this challenge. NES seamlessly integrates three stages: (1) it employs synthetic images to ensure consistent image-to-text retrieval across both training and inference; (2) it filters negative entities from retrieved content to enhance accuracy; and (3) it applies attention-level suppression using identified negative entities to further minimize the impact of hallucination-prone features. Evaluation across multiple benchmarks demonstrates that NES maintains competitive in-domain performance while improving cross-domain transfer and reducing hallucination rates, achieving new state-of-the-art results in ZIC. Our code is available at https://github.com/nidongpinyinme/NESCap.

</details>


### [17] [SPEED-Q: Staged Processing with Enhanced Distillation towards Efficient Low-bit On-device VLM Quantization](https://arxiv.org/abs/2511.08914)
*Tianyu Guo,Shanwei Zhao,Shiai Zhu,Chenguang Ma*

Main category: cs.CV

TL;DR: SPEED-Q是一个针对1B-2B参数规模视觉语言模型(VLM)的低比特权重量化框架，通过分阶段敏感度自适应机制和蒸馏增强策略，解决了VLM中视觉和语言组件量化敏感度差异大、低比特量化训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署视觉语言模型需要低延迟和隐私保护，但由于资源限制，需要量化技术来提高内存效率和减少带宽需求。现有研究很少探索VLM的激进量化，特别是适合资源受限边缘设备的1B-2B参数模型。

Method: 提出SPEED-Q框架，包含：1)分阶段敏感度自适应机制，协调不同模态的性能；2)蒸馏增强量化策略，稳定训练过程并减少数据依赖。这是首个专门为小规模十亿参数VLM低比特量化设计的框架。

Result: 在多个基准测试上的广泛实验表明，SPEED-Q在2比特设置下比现有量化方法准确率提高6倍，在2比特和4比特设置下均优于先前的设备端VLM。

Conclusion: SPEED-Q能够实现复杂VLM的准确、稳定和数据高效的量化，为边缘设备部署提供了有效的解决方案。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices (e.g., smartphones and robots) is crucial for enabling low-latency and privacy-preserving intelligent applications. Given the resource constraints of these devices, quantization offers a promising solution by improving memory efficiency and reducing bandwidth requirements, thereby facilitating the deployment of VLMs. However, existing research has rarely explored aggressive quantization on VLMs, particularly for the models ranging from 1B to 2B parameters, which are more suitable for resource-constrained edge devices. In this paper, we propose SPEED-Q, a novel Staged Processing with Enhanced Distillation framework for VLM low-bit weight-only quantization that systematically addresses the following two critical obstacles: (1) significant discrepancies in quantization sensitivity between vision (ViT) and language (LLM) components in VLMs; (2) training instability arising from the reduced numerical precision inherent in low-bit quantization. In SPEED-Q, a staged sensitivity adaptive mechanism is introduced to effectively harmonize performance across different modalities. We further propose a distillation-enhanced quantization strategy to stabilize the training process and reduce data dependence. Together, SPEED-Q enables accurate, stable, and data-efficient quantization of complex VLMs. SPEED-Q is the first framework tailored for quantizing entire small-scale billion-parameter VLMs to low bits. Extensive experiments across multiple benchmarks demonstrate that SPEED-Q achieves up to 6x higher accuracy than existing quantization methods under 2-bit settings and consistently outperforms prior on-device VLMs under both 2-bit and 4-bit settings. Our code and models are available at https://github.com/antgroup/SPEED-Q.

</details>


### [18] [Machines Serve Human: A Novel Variable Human-machine Collaborative Compression Framework](https://arxiv.org/abs/2511.08915)
*Zifu Zhang,Shengxi Li,Xiancheng Sun,Mai Xu,Zhengyuan Liu,Jingyuan Xia*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器视觉压缩的人机协作压缩方法Diff-FCHM，通过将机器视觉作为基础，逐步聚合语义信息并利用扩散先验恢复高保真细节，在机器视觉和人类视觉压缩方面均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的人机协作压缩方法主要基于人类视觉压缩流程，在整合机器视觉压缩时存在复杂度和比特率方面的不足。机器视觉仅关注图像/视频中的核心区域，所需信息远少于人类视觉压缩信息。

Method: 提出基于机器视觉导向的压缩方法，采用即插即用的可变比特率策略，逐步聚合机器视觉压缩的语义信息，并利用扩散先验无缝恢复人类视觉的高保真细节。

Result: 实验结果表明，Diff-FCHM在机器视觉和人类视觉压缩方面均取得持续优异的性能，具有显著的性能优势。

Conclusion: 本文首次成功实现了基于机器视觉压缩的人机协作压缩方法，为同时服务于人类感知和机器智能的图像/视频数据压缩提供了新的解决方案。

Abstract: Human-machine collaborative compression has been receiving increasing research efforts for reducing image/video data, serving as the basis for both human perception and machine intelligence. Existing collaborative methods are dominantly built upon the de facto human-vision compression pipeline, witnessing deficiency on complexity and bit-rates when aggregating the machine-vision compression. Indeed, machine vision solely focuses on the core regions within the image/video, requiring much less information compared with the compressed information for human vision. In this paper, we thus set out the first successful attempt by a novel collaborative compression method based on the machine-vision-oriented compression, instead of human-vision pipeline. In other words, machine vision serves as the basis for human vision within collaborative compression. A plug-and-play variable bit-rate strategy is also developed for machine vision tasks. Then, we propose to progressively aggregate the semantics from the machine-vision compression, whilst seamlessly tailing the diffusion prior to restore high-fidelity details for human vision, thus named as diffusion-prior based feature compression for human and machine visions (Diff-FCHM). Experimental results verify the consistently superior performances of our Diff-FCHM, on both machine-vision and human-vision compression with remarkable margins. Our code will be released upon acceptance.

</details>


### [19] [From Structure to Detail: Hierarchical Distillation for Efficient Diffusion Model](https://arxiv.org/abs/2511.08930)
*Hanbo Cheng,Peng Wang,Kaixiang Lei,Qi Li,Zhen Zou,Pengfei Hu,Jun Du*

Main category: cs.CV

TL;DR: 本文提出了一种分层蒸馏（HD）框架，将轨迹蒸馏和分布蒸馏从独立范式转变为协同组件，通过轨迹蒸馏建立结构"草图"，再通过分布蒸馏进行细化，结合自适应加权判别器（AWD）实现高效细节优化，在单步扩散模型中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的推理延迟是实时应用的关键障碍。现有轨迹蒸馏和分布蒸馏方法存在根本性权衡：轨迹蒸馏保留全局结构但牺牲高频细节，分布蒸馏可实现更高保真度但易出现模式崩溃和不稳定训练。

Method: 提出分层蒸馏框架：1）使用轨迹蒸馏建立结构草图作为初始化；2）采用分布蒸馏进行细化；3）引入自适应加权判别器（AWD）动态分配token权重，专注于局部缺陷实现高效细节优化。

Result: 在ImageNet 256×256上，单步模型FID达到2.26，媲美250步教师模型；在高分辨率文本到图像MJHQ基准测试中取得有希望的结果，证明了方法的通用性。

Conclusion: 该方法为高保真单步扩散模型建立了强大的新范式，通过协同使用轨迹蒸馏和分布蒸馏，结合专门设计的判别器结构，实现了性能突破。

Abstract: The inference latency of diffusion models remains a critical barrier to their real-time application. While trajectory-based and distribution-based step distillation methods offer solutions, they present a fundamental trade-off. Trajectory-based methods preserve global structure but act as a "lossy compressor", sacrificing high-frequency details. Conversely, distribution-based methods can achieve higher fidelity but often suffer from mode collapse and unstable training. This paper recasts them from independent paradigms into synergistic components within our novel Hierarchical Distillation (HD) framework. We leverage trajectory distillation not as a final generator, but to establish a structural ``sketch", providing a near-optimal initialization for the subsequent distribution-based refinement stage. This strategy yields an ideal initial distribution that enhances the ceiling of overall performance. To further improve quality, we introduce and refine the adversarial training process. We find standard discriminator structures are ineffective at refining an already high-quality generator. To overcome this, we introduce the Adaptive Weighted Discriminator (AWD), tailored for the HD pipeline. By dynamically allocating token weights, AWD focuses on local imperfections, enabling efficient detail refinement. Our approach demonstrates state-of-the-art performance across diverse tasks. On ImageNet $256\times256$, our single-step model achieves an FID of 2.26, rivaling its 250-step teacher. It also achieves promising results on the high-resolution text-to-image MJHQ benchmark, proving its generalizability. Our method establishes a robust new paradigm for high-fidelity, single-step diffusion models.

</details>


### [20] [Neural B-frame Video Compression with Bi-directional Reference Harmonization](https://arxiv.org/abs/2511.08938)
*Yuxi Liu,Dengchao Jin,Shuai Huo,Jiawen Gu,Chao Zhou,Huihui Bai,Ming Lu,Zhan Ma*

Main category: cs.CV

TL;DR: 提出了一种新的双向参考协调视频压缩方法BRHVC，通过双向运动收敛和双向上下文融合技术，优化双向参考帧的利用，在HEVC数据集上超越了传统编码VTM-RA。


<details>
  <summary>Details</summary>
Motivation: 神经B帧视频压缩相对于P帧压缩研究较少，但双向参考帧能提供更好的压缩性能。然而层次编码中的大跨度帧会导致两个参考帧贡献不平衡，需要优化参考信息利用。

Method: 提出BRHVC方法，包含双向运动收敛(BMC)和双向上下文融合(BCF)。BMC收敛多个光流进行运动压缩，BCF在运动补偿精度指导下显式建模参考上下文的权重。

Result: 实验结果表明BRHVC超越了之前最先进的神经视频压缩方法，在HEVC数据集上甚至超过了传统编码VTM-RA（随机访问配置）。

Conclusion: 通过更高效的运动和上下文处理，BRHVC能够有效协调双向参考，为神经B帧视频压缩提供了有效的解决方案。

Abstract: Neural video compression (NVC) has made significant progress in recent years, while neural B-frame video compression (NBVC) remains underexplored compared to P-frame compression. NBVC can adopt bi-directional reference frames for better compression performance. However, NBVC's hierarchical coding may complicate continuous temporal prediction, especially at some hierarchical levels with a large frame span, which could cause the contribution of the two reference frames to be unbalanced. To optimize reference information utilization, we propose a novel NBVC method, termed Bi-directional Reference Harmonization Video Compression (BRHVC), with the proposed Bi-directional Motion Converge (BMC) and Bi-directional Contextual Fusion (BCF). BMC converges multiple optical flows in motion compression, leading to more accurate motion compensation on a larger scale. Then BCF explicitly models the weights of reference contexts under the guidance of motion compensation accuracy. With more efficient motions and contexts, BRHVC can effectively harmonize bi-directional references. Experimental results indicate that our BRHVC outperforms previous state-of-the-art NVC methods, even surpassing the traditional coding, VTM-RA (under random access configuration), on the HEVC datasets. The source code is released at https://github.com/kwai/NVC.

</details>


### [21] [FGM-HD: Boosting Generation Diversity of Fractal Generative Models through Hausdorff Dimension Induction](https://arxiv.org/abs/2511.08945)
*Haowei Zhang,Yuanpei Zhao,Jizhe Zhou,Mao Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于豪斯多夫维数（HD）的方法来提升分形生成模型（FGM）的生成多样性，通过可学习的HD估计、动量调度策略和HD引导的拒绝采样，在保持图像质量的同时显著提升了39%的多样性。


<details>
  <summary>Details</summary>
Motivation: 分形生成模型虽然能生成高质量图像，但其固有的自相似性限制了输出图像的多样性。为了解决这个问题，作者引入豪斯多夫维数这一分形几何概念来量化结构复杂性，从而增强生成输出的多样性。

Method: 1）提出可学习的HD估计方法，直接从图像嵌入预测HD以解决计算成本问题；2）在训练时采用基于HD的损失函数和单调动量驱动调度策略，逐步优化超参数；3）在推理时使用HD引导的拒绝采样来选择几何更丰富的输出。

Result: 在ImageNet数据集上的大量实验表明，FGM-HD框架相比原始FGM实现了39%的输出多样性提升，同时保持了相当的图像质量。

Conclusion: 这是首个将豪斯多夫维数引入分形生成模型的工作，有效增强了生成输出的多样性，并为FGM发展提供了理论贡献。

Abstract: Improving the diversity of generated results while maintaining high visual quality remains a significant challenge in image generation tasks. Fractal Generative Models (FGMs) are efficient in generating high-quality images, but their inherent self-similarity limits the diversity of output images. To address this issue, we propose a novel approach based on the Hausdorff Dimension (HD), a widely recognized concept in fractal geometry used to quantify structural complexity, which aids in enhancing the diversity of generated outputs. To incorporate HD into FGM, we propose a learnable HD estimation method that predicts HD directly from image embeddings, addressing computational cost concerns. However, simply introducing HD into a hybrid loss is insufficient to enhance diversity in FGMs due to: 1) degradation of image quality, and 2) limited improvement in generation diversity. To this end, during training, we adopt an HD-based loss with a monotonic momentum-driven scheduling strategy to progressively optimize the hyperparameters, obtaining optimal diversity without sacrificing visual quality. Moreover, during inference, we employ HD-guided rejection sampling to select geometrically richer outputs. Extensive experiments on the ImageNet dataset demonstrate that our FGM-HD framework yields a 39\% improvement in output diversity compared to vanilla FGMs, while preserving comparable image quality. To our knowledge, this is the very first work introducing HD into FGM. Our method effectively enhances the diversity of generated outputs while offering a principled theoretical contribution to FGM development.

</details>


### [22] [AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows](https://arxiv.org/abs/2511.08967)
*RuiQiang Zhang,Zehua Ma,Guanjie Wang,Chang Liu,Hengyi Wang,Weiming Zhang*

Main category: cs.CV

TL;DR: AuthSig是一个基于生成模型和水印的静态电子签名框架，通过风格嵌入隐式编码水印位，实现"一次签名，一次使用"策略，解决了静态扫描签名易被恶意复制和重用的安全问题。


<details>
  <summary>Details</summary>
Motivation: 随着无纸化工作流程的深入，签名作为身份认证手段正从传统纸质转向电子格式。尽管存在动态压力敏感和基于PKI的数字签名，但由于便利性，静态扫描签名在实践中仍然普遍。然而这些静态图像几乎失去了认证属性，无法可靠验证且易受恶意复制和重用攻击。

Method: AuthSig利用人类视觉系统对细微风格变化不敏感的特性，在生成过程中精细调节风格嵌入来隐式编码水印位。为克服手写签名数据稀缺和传统增强方法的局限性，引入了关键点驱动的数据增强策略，有效增强风格多样性以支持鲁棒的水印嵌入。

Result: 实验结果表明，AuthSig在数字域失真和签名特定退化下实现了超过98%的提取准确率，即使在打印扫描场景下仍然有效。

Conclusion: AuthSig通过将认证信息绑定到签名图像，为静态电子签名提供了可靠的安全保障，解决了当前静态签名易被滥用的安全问题。

Abstract: With the deepening trend of paperless workflows, signatures as a means of identity authentication are gradually shifting from traditional ink-on-paper to electronic formats.Despite the availability of dynamic pressure-sensitive and PKI-based digital signatures, static scanned signatures remain prevalent in practice due to their convenience. However, these static images, having almost lost their authentication attributes, cannot be reliably verified and are vulnerable to malicious copying and reuse. To address these issues, we propose AuthSig, a novel static electronic signature framework based on generative models and watermark, which binds authentication information to the signature image. Leveraging the human visual system's insensitivity to subtle style variations, AuthSig finely modulates style embeddings during generation to implicitly encode watermark bits-enforcing a One Signature, One Use policy.To overcome the scarcity of handwritten signature data and the limitations of traditional augmentation methods, we introduce a keypoint-driven data augmentation strategy that effectively enhances style diversity to support robust watermark embedding. Experimental results show that AuthSig achieves over 98% extraction accuracy under both digital-domain distortions and signature-specific degradations, and remains effective even in print-scan scenarios.

</details>


### [23] [Efficient and Effective In-context Demonstration Selection with Coreset](https://arxiv.org/abs/2511.08977)
*Zihua Wang,Jiarui Wang,Haiyang Xu,Ming Yan,Fei Huang,Xu Yang,Xiu-Shen Wei,Siya Mi,Yu Zhang*

Main category: cs.CV

TL;DR: 本文提出了基于核心集的双重检索框架（CoDR），通过构建多样化核心集和双重检索机制，解决了大视觉语言模型中上下文学习演示选择效率与效果平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 传统演示选择策略（随机、基于相似性、基于信息分数）在效率和效果上存在不足，无法有效平衡两者，而演示选择本身是NP难问题。

Method: 提出CoDR框架：1）使用聚类剪枝方法构建多样化核心集，提高期望互信息；2）设计双重检索机制，在保持效率的同时实现全局演示选择。

Result: 实验结果表明，该方法相比现有策略显著提升了上下文学习的性能。

Conclusion: CoDR为有效且高效的演示选择提供了一个稳健的解决方案，解决了传统方法在效率和效果平衡方面的挑战。

Abstract: In-context learning (ICL) has emerged as a powerful paradigm for Large Visual Language Models (LVLMs), enabling them to leverage a few examples directly from input contexts. However, the effectiveness of this approach is heavily reliant on the selection of demonstrations, a process that is NP-hard. Traditional strategies, including random, similarity-based sampling and infoscore-based sampling, often lead to inefficiencies or suboptimal performance, struggling to balance both efficiency and effectiveness in demonstration selection. In this paper, we propose a novel demonstration selection framework named Coreset-based Dual Retrieval (CoDR). We show that samples within a diverse subset achieve a higher expected mutual information. To implement this, we introduce a cluster-pruning method to construct a diverse coreset that aligns more effectively with the query while maintaining diversity. Additionally, we develop a dual retrieval mechanism that enhances the selection process by achieving global demonstration selection while preserving efficiency. Experimental results demonstrate that our method significantly improves the ICL performance compared to the existing strategies, providing a robust solution for effective and efficient demonstration selection.

</details>


### [24] [WDT-MD: Wavelet Diffusion Transformers for Microaneurysm Detection in Fundus Images](https://arxiv.org/abs/2511.08987)
*Yifei Sun,Yuzhi He,Junhao Jia,Jinhong Wang,Ruiquan Ge,Changmiao Wang,Hongxia Xu*

Main category: cs.CV

TL;DR: 本文提出了一种基于小波扩散Transformer的微动脉瘤检测框架（WDT-MD），解决了扩散模型在医学图像异常检测中的三个关键问题：身份映射、假阳性率高和正常特征重建不佳。


<details>
  <summary>Details</summary>
Motivation: 微动脉瘤是糖尿病视网膜病变的最早病理标志，但在眼底图像中表现为亚60微米病变，具有高度可变的光度和形态特征，使得人工筛查既耗时又容易出错。现有扩散模型在临床应用中存在身份映射、难以区分微动脉瘤与其他异常、正常特征重建不佳等三大限制。

Method: 提出WDT-MD框架，包含三个关键创新：噪声编码图像条件机制以避免身份映射；通过修复进行伪正常模式合成以引入像素级监督；结合扩散Transformer全局建模能力与多尺度小波分析的小波扩散Transformer架构。

Result: 在IDRiD和e-ophtha MA数据集上的综合实验表明，WDT-MD在像素级和图像级微动脉瘤检测方面均优于最先进的方法。

Conclusion: WDT-MD框架显著提升了微动脉瘤检测性能，对改善早期糖尿病视网膜病变筛查具有重要前景。

Abstract: Microaneurysms (MAs), the earliest pathognomonic signs of Diabetic Retinopathy (DR), present as sub-60 $μm$ lesions in fundus images with highly variable photometric and morphological characteristics, rendering manual screening not only labor-intensive but inherently error-prone. While diffusion-based anomaly detection has emerged as a promising approach for automated MA screening, its clinical application is hindered by three fundamental limitations. First, these models often fall prey to "identity mapping", where they inadvertently replicate the input image. Second, they struggle to distinguish MAs from other anomalies, leading to high false positives. Third, their suboptimal reconstruction of normal features hampers overall performance. To address these challenges, we propose a Wavelet Diffusion Transformer framework for MA Detection (WDT-MD), which features three key innovations: a noise-encoded image conditioning mechanism to avoid "identity mapping" by perturbing image conditions during training; pseudo-normal pattern synthesis via inpainting to introduce pixel-level supervision, enabling discrimination between MAs and other anomalies; and a wavelet diffusion Transformer architecture that combines the global modeling capability of diffusion Transformers with multi-scale wavelet analysis to enhance reconstruction of normal retinal features. Comprehensive experiments on the IDRiD and e-ophtha MA datasets demonstrate that WDT-MD outperforms state-of-the-art methods in both pixel-level and image-level MA detection. This advancement holds significant promise for improving early DR screening.

</details>


### [25] [An ICTM-RMSAV Framework for Bias-Field Aware Image Segmentation under Poisson and Multiplicative Noise](https://arxiv.org/abs/2511.08988)
*Xinyu Wang,Wenjun Yao,Fanghui Song,Zhichang Guo*

Main category: cs.CV

TL;DR: 提出了一种结合去噪项的变分分割模型，采用I-散度项和自适应TV正则化处理Gamma分布乘性噪声和泊松噪声，通过灰度指示器实现空间自适应权重，并估计偏置场解决强度不均匀性问题，使用ICTM和RMSAV方案进行高效优化。


<details>
  <summary>Details</summary>
Motivation: 现有图像分割方法在图像被严重噪声污染和存在强度不均匀性时性能下降，需要开发能够同时处理噪声和强度不均匀性的鲁棒分割方法。

Method: 在ICTM框架内集成去噪项，包含I-散度项和自适应TV正则化，使用灰度指示器生成空间自适应权重，估计平滑变化的偏置场，采用特征函数表示区域，结合ICTM和RMSAV方案进行优化。

Result: 在具有强度不均匀性和多种噪声类型的合成和真实图像上的广泛实验表明，所提模型相比竞争方法实现了更优越的准确性和鲁棒性。

Conclusion: 该模型能够有效处理Gamma分布乘性噪声、泊松噪声和强度不均匀性问题，在图像分割任务中表现出优异的性能。

Abstract: Image segmentation is a core task in image processing, yet many methods degrade when images are heavily corrupted by noise and exhibit intensity inhomogeneity. Within the iterative-convolution thresholding method (ICTM) framework, we propose a variational segmentation model that integrates denoising terms. Specifically, the denoising component consists of an I-divergence term and an adaptive total-variation (TV) regularizer, making the model well suited to images contaminated by Gamma--distributed multiplicative noise and Poisson noise. A spatially adaptive weight derived from a gray-level indicator guides diffusion differently across regions of varying intensity. To further address intensity inhomogeneity, we estimate a smoothly varying bias field, which improves segmentation accuracy. Regions are represented by characteristic functions, with contour length encoded accordingly. For efficient optimization, we couple ICTM with a relaxed modified scalar auxiliary variable (RMSAV) scheme. Extensive experiments on synthetic and real-world images with intensity inhomogeneity and diverse noise types show that the proposed model achieves superior accuracy and robustness compared with competing approaches.

</details>


### [26] [T-Rex-Omni: Integrating Negative Visual Prompt in Generic Object Detection](https://arxiv.org/abs/2511.08997)
*Jiazhou Zhou,Qing Jiang,Kanghao Chen,Lutao Jiang,Yuanhuiyi Lyu,Ying-Cong Chen,Lei Zhang*

Main category: cs.CV

TL;DR: T-Rex-Omni是一个新颖的开放集目标检测框架，通过引入负视觉提示来抑制视觉相似但语义不同的干扰物，解决了现有方法仅依赖正提示的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前开放集目标检测器仅依赖基于文本描述或视觉示例的正提示，在面对视觉相似但语义不同的干扰物时存在脆弱性。

Method: 提出统一视觉提示编码器处理正负视觉提示，设计无需训练的负向否定计算模块动态抑制负响应，并使用负向否定铰链损失增强正负嵌入的判别性边界。

Result: 在零样本检测中表现出色，显著缩小了视觉提示与文本提示方法之间的性能差距，在长尾场景中表现尤为突出（LVIS-minival上达到51.2 AP_r）。

Conclusion: 负提示是推进开放集视觉识别系统的关键新维度，T-Rex-Omni支持灵活部署，可适应用户指定或自动生成的负示例。

Abstract: Object detection methods have evolved from closed-set to open-set paradigms over the years. Current open-set object detectors, however, remain constrained by their exclusive reliance on positive indicators based on given prompts like text descriptions or visual exemplars. This positive-only paradigm experiences consistent vulnerability to visually similar but semantically different distractors. We propose T-Rex-Omni, a novel framework that addresses this limitation by incorporating negative visual prompts to negate hard negative distractors. Specifically, we first introduce a unified visual prompt encoder that jointly processes positive and negative visual prompts. Next, a training-free Negating Negative Computing (NNC) module is proposed to dynamically suppress negative responses during the probability computing stage. To further boost performance through fine-tuning, our Negating Negative Hinge (NNH) loss enforces discriminative margins between positive and negative embeddings. T-Rex-Omni supports flexible deployment in both positive-only and joint positive-negative inference modes, accommodating either user-specified or automatically generated negative examples. Extensive experiments demonstrate remarkable zero-shot detection performance, significantly narrowing the performance gap between visual-prompted and text-prompted methods while showing particular strength in long-tailed scenarios (51.2 AP_r on LVIS-minival). This work establishes negative prompts as a crucial new dimension for advancing open-set visual recognition systems.

</details>


### [27] [Causally-Grounded Dual-Path Attention Intervention for Object Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2511.09018)
*Liu Yu,Zhonghao Chen,Ping Kuang,Zhikun Feng,Fan Zhou,Lan Wang,Gillian Dobbie*

Main category: cs.CV

TL;DR: Owl是一个基于因果推理的双模态注意力重加权框架，通过建模幻觉的结构因果图，将分解的视觉和文本注意力作为中介变量，提出VTACR指标量化模态贡献不平衡，并设计细粒度注意力干预机制和双路径对比解码策略来显著减少大视觉语言模型中的物体幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言解码器的缓解方法通常独立调节视觉或文本注意力，忽视了它们作为两个关键因果因素的交互作用。物体幻觉是大视觉语言模型中的关键挑战，模型生成与视觉输入不一致的内容。

Method: 提出Owl框架：1）通过结构因果图建模幻觉过程；2）引入VTACR指标量化解码过程中的模态贡献不平衡；3）设计细粒度注意力干预机制，基于VTACR信号动态调整token级和layer级注意力；4）提出双路径对比解码策略，一条路径强调视觉接地预测，另一条路径放大幻觉预测。

Result: 在POPE和CHAIR基准测试上的实验结果表明，Owl实现了显著的幻觉减少，在保持视觉语言理解能力的同时，在忠实度方面达到了新的最先进水平。

Conclusion: Owl通过因果建模和注意力重加权有效缓解了大视觉语言模型中的物体幻觉问题，证明了模态贡献平衡对于减少幻觉的重要性，同时保持了模型的视觉语言理解能力。

Abstract: Object hallucination remains a critical challenge in Large Vision-Language Models (LVLMs), where models generate content inconsistent with visual inputs. Existing language-decoder based mitigation approaches often regulate visual or textual attention independently, overlooking their interaction as two key causal factors. To address this, we propose Owl (Bi-mOdal attention reWeighting for Layer-wise hallucination mitigation), a causally-grounded framework that models hallucination process via a structural causal graph, treating decomposed visual and textual attentions as mediators. We introduce VTACR (Visual-to-Textual Attention Contribution Ratio), a novel metric that quantifies the modality contribution imbalance during decoding. Our analysis reveals that hallucinations frequently occur in low-VTACR scenarios, where textual priors dominate and visual grounding is weakened. To mitigate this, we design a fine-grained attention intervention mechanism that dynamically adjusts token- and layer-wise attention guided by VTACR signals. Finally, we propose a dual-path contrastive decoding strategy: one path emphasizes visually grounded predictions, while the other amplifies hallucinated ones -- letting visual truth shine and hallucination collapse. Experimental results on the POPE and CHAIR benchmarks show that Owl achieves significant hallucination reduction, setting a new SOTA in faithfulness while preserving vision-language understanding capability. Our code is available at https://github.com/CikZ2023/OWL

</details>


### [28] [Dense Cross-Scale Image Alignment With Fully Spatial Correlation and Just Noticeable Difference Guidance](https://arxiv.org/abs/2511.09028)
*Jinkun You,Jiaxue Li,Jie Zhang,Yicong Zhou*

Main category: cs.CV

TL;DR: 提出了一种密集跨尺度图像对齐模型，通过考虑跨尺度特征相关性来降低对齐难度，支持在精度和效率之间灵活权衡，并引入全空间相关模块和恰可察觉差异来提升精度。


<details>
  <summary>Details</summary>
Motivation: 现有无监督图像对齐方法存在精度有限和计算复杂度高的问题，需要开发更准确高效的对齐方法。

Method: 提出密集跨尺度图像对齐模型，考虑跨尺度特征相关性；引入全空间相关模块提升精度；利用恰可察觉差异使模型关注对失真更敏感的图像区域。

Result: 大量定量和定性实验表明，该方法在精度和效率方面均优于现有最先进方法。

Conclusion: 该方法通过跨尺度特征建模和空间相关性优化，成功解决了无监督图像对齐中的精度和效率问题，具有实际应用价值。

Abstract: Existing unsupervised image alignment methods exhibit limited accuracy and high computational complexity. To address these challenges, we propose a dense cross-scale image alignment model. It takes into account the correlations between cross-scale features to decrease the alignment difficulty. Our model supports flexible trade-offs between accuracy and efficiency by adjusting the number of scales utilized. Additionally, we introduce a fully spatial correlation module to further improve accuracy while maintaining low computational costs. We incorporate the just noticeable difference to encourage our model to focus on image regions more sensitive to distortions, eliminating noticeable alignment errors. Extensive quantitative and qualitative experiments demonstrate that our method surpasses state-of-the-art approaches.

</details>


### [29] [PAN: A World Model for General, Interactable, and Long-Horizon World Simulation](https://arxiv.org/abs/2511.09057)
*PAN Team,Jiannan Xiang,Yi Gu,Zihan Liu,Zeyu Feng,Qiyue Gao,Yiyan Hu,Benhao Huang,Guangyi Liu,Yichi Yang,Kun Zhou,Davit Abrahamyan,Arif Ahmad,Ganesh Bannur,Junrong Chen,Kimi Chen,Mingkai Deng,Ruobing Han,Xinqi Huang,Haoqiang Kang,Zheqi Li,Enze Ma,Hector Ren,Yashowardhan Shinde,Rohan Shingre,Ramsundar Tanikella,Kaiming Tao,Dequan Yang,Xinle Yu,Cong Zeng,Binglin Zhou,Hector Liu,Zhiting Hu,Eric P. Xing*

Main category: cs.CV

TL;DR: PAN是一个通用、可交互、长视野的世界模型，通过高质量视频模拟预测未来世界状态，结合语言模型推理和视频扩散解码器，实现潜在空间推理与可实现的动态世界的统一。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型缺乏因果控制、交互性和长视野一致性，而现有世界模型往往局限于特定领域且深度和可控性有限，难以泛化到多样化环境和交互形式。

Method: 采用生成潜在预测（GLP）架构，结合基于大语言模型的自回归潜在动态骨干（支持语言指定动作）和视频扩散解码器（重建感知细节和时间连贯的视觉观察）。

Result: 在大规模视频-动作对数据集上训练，PAN在动作条件世界模拟、长视野预测和模拟推理方面表现优异，优于其他视频生成器和世界模型。

Conclusion: PAN朝着通用世界模型迈进一步，能够为推理和行动提供未来世界状态的预测性模拟。

Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.

</details>


### [30] [Diversifying Counterattacks: Orthogonal Exploration for Robust CLIP Inference](https://arxiv.org/abs/2511.09064)
*Chengze Jiang,Minjing Dong,Xinli Shi,Jie Gui*

Main category: cs.CV

TL;DR: 本文提出DOC方法，通过引入正交梯度方向和动量更新来增强对抗性攻击的多样性，提高视觉语言预训练模型在测试时的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时对抗防御方法TTC仅基于对抗输入的梯度生成对抗扰动，搜索空间有限，容易过拟合特定对抗模式，缺乏多样性来完全中和广泛的扰动。

Method: 提出方向正交对抗攻击(DOC)，结合正交梯度方向和动量更新来扩展对抗攻击空间的探索，增加扰动多样性，同时引入基于平均余弦相似度的方向敏感度评分来提升示例区分度和自适应调节对抗攻击强度。

Result: 在16个数据集上的广泛实验表明，DOC在各种攻击下提高了对抗鲁棒性，同时保持了竞争力的干净准确率。

Conclusion: 增强对抗攻击的多样性和覆盖范围对于提高测试时防御的对抗鲁棒性至关重要，DOC方法通过正交梯度方向和动量更新有效实现了这一目标。

Abstract: Vision-language pre-training models (VLPs) demonstrate strong multimodal understanding and zero-shot generalization, yet remain vulnerable to adversarial examples, raising concerns about their reliability. Recent work, Test-Time Counterattack (TTC), improves robustness by generating perturbations that maximize the embedding deviation of adversarial inputs using PGD, pushing them away from their adversarial representations. However, due to the fundamental difference in optimization objectives between adversarial attacks and counterattacks, generating counterattacks solely based on gradients with respect to the adversarial input confines the search to a narrow space. As a result, the counterattacks could overfit limited adversarial patterns and lack the diversity to fully neutralize a broad range of perturbations. In this work, we argue that enhancing the diversity and coverage of counterattacks is crucial to improving adversarial robustness in test-time defense. Accordingly, we propose Directional Orthogonal Counterattack (DOC), which augments counterattack optimization by incorporating orthogonal gradient directions and momentum-based updates. This design expands the exploration of the counterattack space and increases the diversity of perturbations, which facilitates the discovery of more generalizable counterattacks and ultimately improves the ability to neutralize adversarial perturbations. Meanwhile, we present a directional sensitivity score based on averaged cosine similarity to boost DOC by improving example discrimination and adaptively modulating the counterattack strength. Extensive experiments on 16 datasets demonstrate that DOC improves adversarial robustness under various attacks while maintaining competitive clean accuracy. Code is available at https://github.com/bookman233/DOC.

</details>


### [31] [Composition-Incremental Learning for Compositional Generalization](https://arxiv.org/abs/2511.09082)
*Zhen Li,Yuwei Wu,Chenchen Jing,Che Sun,Chuanhao Li,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出了组合增量学习（CompIL）框架，用于在组合零样本学习任务中持续学习新组合，逐步提升组合泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据不断涌现，组合方式近乎无限、长尾分布且不完全可见，需要模型以增量方式逐步提升组合泛化能力。

Method: 提出了基于伪回放的框架，使用视觉合成器合成已学习组合的视觉表示，并通过语言基元蒸馏机制在学习过程中保持对齐的基元表示。

Result: 构建了MIT-States-CompIL和C-GQA-CompIL基准数据集，并通过大量实验验证了所提框架的有效性。

Conclusion: 该框架能够有效支持组合增量学习，提升模型在组合零样本学习任务中的持续学习能力。

Abstract: Compositional generalization has achieved substantial progress in computer vision on pre-collected training data. Nonetheless, real-world data continually emerges, with possible compositions being nearly infinite, long-tailed, and not entirely visible. Thus, an ideal model is supposed to gradually improve the capability of compositional generalization in an incremental manner. In this paper, we explore Composition-Incremental Learning for Compositional Generalization (CompIL) in the context of the compositional zero-shot learning (CZSL) task, where models need to continually learn new compositions, intending to improve their compositional generalization capability progressively. To quantitatively evaluate CompIL, we develop a benchmark construction pipeline leveraging existing datasets, yielding MIT-States-CompIL and C-GQA-CompIL. Furthermore, we propose a pseudo-replay framework utilizing a visual synthesizer to synthesize visual representations of learned compositions and a linguistic primitive distillation mechanism to maintain aligned primitive representations across the learning process. Extensive experiments demonstrate the effectiveness of the proposed framework.

</details>


### [32] [DKDS: A Benchmark Dataset of Degraded Kuzushiji Documents with Seals for Detection and Binarization](https://arxiv.org/abs/2511.09117)
*Rui-Yang Ju,Kohei Yamashita,Hirotaka Kameko,Shinsuke Mori*

Main category: cs.CV

TL;DR: 本文提出了DKDS数据集，专门针对古日文草书体Kuzushiji文档中的噪声问题（如文档退化和印章），并建立了文本印章检测和文档二值化两个基准任务。


<details>
  <summary>Details</summary>
Motivation: 现有OCR方法在干净的Kuzushiji文档上表现良好，但无法有效处理文档退化和印章等噪声问题，且缺乏专门针对这些挑战的数据集。

Method: 构建了DKDS数据集，包含两个基准任务：1）使用YOLO模型进行文本和印章检测；2）使用传统算法、K-means聚类和GAN方法进行文档二值化。

Result: 提供了两个基准任务的基线结果，包括YOLO模型在检测任务上的表现，以及多种方法在二值化任务上的性能。

Conclusion: DKDS数据集填补了Kuzushiji识别领域在噪声处理方面的空白，为相关研究提供了新的基准和基线方法。

Abstract: Kuzushiji, a pre-modern Japanese cursive script, can currently be read and understood by only a few thousand trained experts in Japan. With the rapid development of deep learning, researchers have begun applying Optical Character Recognition (OCR) techniques to transcribe Kuzushiji into modern Japanese. Although existing OCR methods perform well on clean pre-modern Japanese documents written in Kuzushiji, they often fail to consider various types of noise, such as document degradation and seals, which significantly affect recognition accuracy. To the best of our knowledge, no existing dataset specifically addresses these challenges. To address this gap, we introduce the Degraded Kuzushiji Documents with Seals (DKDS) dataset as a new benchmark for related tasks. We describe the dataset construction process, which required the assistance of a trained Kuzushiji expert, and define two benchmark tracks: (1) text and seal detection and (2) document binarization. For the text and seal detection track, we provide baseline results using multiple versions of the You Only Look Once (YOLO) models for detecting Kuzushiji characters and seals. For the document binarization track, we present baseline results from traditional binarization algorithms, traditional algorithms combined with K-means clustering, and Generative Adversarial Network (GAN)-based methods. The DKDS dataset and the implementation code for baseline methods are available at https://ruiyangju.github.io/DKDS.

</details>


### [33] [PressTrack-HMR: Pressure-Based Top-Down Multi-Person Global Human Mesh Recovery](https://arxiv.org/abs/2511.09147)
*Jiayue Yuan,Fangting Xie,Guangwen Ouyang,Changhai Ma,Ziyu Wu,Heyu Ding,Quan Wan,Yi Ke,Yuchen Wu,Xiaohui Cai*

Main category: cs.CV

TL;DR: PressTrack-HMR是一个基于压力信号的多人体网格恢复方法，通过检测跟踪策略从原始压力数据中分离个体信号，并在多人交互压力数据集MIP上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统视觉方法在多人场景中存在遮挡、光照不足和隐私问题，而基于压力信号的方法提供了一种无遮挡且保护隐私的替代方案，但多人同时行走时如何区分混合压力信号仍是一个挑战。

Method: 采用自上而下的检测跟踪策略，首先从原始压力数据中识别和分割每个个体的压力信号，然后对每个提取的个体信号执行人体网格恢复。

Result: 在多人HMR任务中取得了89.2mm MPJPE和112.6mm WA-MPJPE100的优异性能，展示了触觉垫在多人动作识别中的潜力。

Conclusion: 该方法证明了仅使用压力信号进行多人体网格恢复的可行性，为无处不在且保护隐私的多人动作识别提供了新途径。

Abstract: Multi-person global human mesh recovery (HMR) is crucial for understanding crowd dynamics and interactions. Traditional vision-based HMR methods sometimes face limitations in real-world scenarios due to mutual occlusions, insufficient lighting, and privacy concerns. Human-floor tactile interactions offer an occlusion-free and privacy-friendly alternative for capturing human motion. Existing research indicates that pressure signals acquired from tactile mats can effectively estimate human pose in single-person scenarios. However, when multiple individuals walk randomly on the mat simultaneously, how to distinguish intermingled pressure signals generated by different persons and subsequently acquire individual temporal pressure data remains a pending challenge for extending pressure-based HMR to the multi-person situation. In this paper, we present \textbf{PressTrack-HMR}, a top-down pipeline that recovers multi-person global human meshes solely from pressure signals. This pipeline leverages a tracking-by-detection strategy to first identify and segment each individual's pressure signal from the raw pressure data, and subsequently performs HMR for each extracted individual signal. Furthermore, we build a multi-person interaction pressure dataset \textbf{MIP}, which facilitates further research into pressure-based human motion analysis in multi-person scenarios. Experimental results demonstrate that our method excels in multi-person HMR using pressure data, with 89.2~$mm$ MPJPE and 112.6~$mm$ WA-MPJPE$_{100}$, and these showcase the potential of tactile mats for ubiquitous, privacy-preserving multi-person action recognition. Our dataset \& code are available at https://github.com/Jiayue-Yuan/PressTrack-HMR.

</details>


### [34] [HOTFLoc++: End-to-End Hierarchical LiDAR Place Recognition, Re-Ranking, and 6-DoF Metric Localisation in Forests](https://arxiv.org/abs/2511.09170)
*Ethan Griffiths,Maryam Haghighat,Simon Denman,Clinton Fookes,Milad Ramezani*

Main category: cs.CV

TL;DR: HOTFLoc++是一个用于森林环境中LiDAR地点识别、重排序和6自由度度量定位的端到端框架，采用八叉树变换器提取多粒度层次局部描述符，通过可学习多尺度几何验证模块减少重排序失败，在公共数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决森林环境中LiDAR地点识别面临的挑战，包括杂波、自相似性和视角变化等问题，特别是在地面到地面和地面到空中的跨模态场景中。

Method: 使用基于八叉树的变换器提取层次局部描述符，提出可学习多尺度几何验证模块，采用从粗到精的配准方法。

Result: 在CS-Wild-Places数据集上平均Recall@1达到90.7%，比基线提升29.6个百分点；在Wild-Places和MulRan数据集上分别达到91.7%和96.0%；97.2%的6自由度配准尝试误差小于2米和5度；多尺度重排序模块平均减少约2倍定位误差；运行时间比RANSAC快两个数量级。

Conclusion: HOTFLoc++在具有挑战性的森林和城市环境中表现出优越性能，显著提高了地点识别精度和定位准确性，同时大幅提升了计算效率。

Abstract: This article presents HOTFLoc++, an end-to-end framework for LiDAR place recognition, re-ranking, and 6-DoF metric localisation in forests. Leveraging an octree-based transformer, our approach extracts hierarchical local descriptors at multiple granularities to increase robustness to clutter, self-similarity, and viewpoint changes in challenging scenarios, including ground-to-ground and ground-to-aerial in forest and urban environments. We propose a learnable multi-scale geometric verification module to reduce re-ranking failures in the presence of degraded single-scale correspondences. Our coarse-to-fine registration approach achieves comparable or lower localisation errors to baselines, with runtime improvements of two orders of magnitude over RANSAC for dense point clouds. Experimental results on public datasets show the superiority of our approach compared to state-of-the-art methods, achieving an average Recall@1 of 90.7% on CS-Wild-Places: an improvement of 29.6 percentage points over baselines, while maintaining high performance on single-source benchmarks with an average Recall@1 of 91.7% and 96.0% on Wild-Places and MulRan, respectively. Our method achieves under 2 m and 5 degrees error for 97.2% of 6-DoF registration attempts, with our multi-scale re-ranking module reducing localisation errors by ~2$\times$ on average. The code will be available upon acceptance.

</details>


### [35] [DBINDS - Can Initial Noise from Diffusion Model Inversion Help Reveal AI-Generated Videos?](https://arxiv.org/abs/2511.09184)
*Yanlin Wu,Xiaogang Yuan,Dezhi An*

Main category: cs.CV

TL;DR: DBINDS是一种基于扩散模型反演的AI生成视频检测器，通过分析潜在空间动态而非像素特征，在单一生成器训练下实现跨生成器的强泛化性能。


<details>
  <summary>Details</summary>
Motivation: AI生成视频技术快速发展，对内容安全和取证分析构成严重挑战。现有检测器主要依赖像素级视觉线索，对未见过的生成器泛化能力差。

Method: 提出DBINDS检测器，基于扩散模型反演分析潜在空间动态，发现真实和生成视频的初始噪声序列存在系统性差异，构建初始噪声差异序列(INDS)并提取多领域多尺度特征，结合特征优化和贝叶斯搜索调优的LightGBM分类器。

Result: 在GenVidBench基准测试中，DBINDS（仅使用单一生成器训练）实现了强大的跨生成器性能，在有限数据设置下表现出良好的泛化能力和鲁棒性。

Conclusion: DBINDS通过分析潜在空间动态而非像素特征，有效解决了AI生成视频检测中的泛化问题，为内容安全提供了新的解决方案。

Abstract: AI-generated video has advanced rapidly and poses serious challenges to content security and forensic analysis. Existing detectors rely mainly on pixel-level visual cues and generalize poorly to unseen generators. We propose DBINDS, a diffusion-model-inversion based detector that analyzes latent-space dynamics rather than pixels. We find that initial noise sequences recovered by diffusion inversion differ systematically between real and generated videos. Building on this, DBINDS forms an Initial Noise Difference Sequence (INDS) and extracts multi-domain, multi-scale features. With feature optimization and a LightGBM classifier tuned by Bayesian search, DBINDS (trained on a single generator) achieves strong cross-generator performance on GenVidBench, demonstrating good generalization and robustness in limited-data settings.

</details>


### [36] [Towards Trustworthy Dermatology MLLMs: A Benchmark and Multimodal Evaluator for Diagnostic Narratives](https://arxiv.org/abs/2511.09195)
*Yuhao Shen,Jiahe Qian,Shuping Zhang,Zhangtianyi Chen,Tao Lu,Juexiao Zhou*

Main category: cs.CV

TL;DR: 提出了一个用于评估皮肤病学多模态大语言模型的新框架，包括精心策划的基准DermBench和自动评估器DermEval，实现了临床意义、可重复和可扩展的评估。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型越来越多地用于直接从图像生成皮肤病诊断叙述，但可靠的评估仍然是负责任临床部署的主要瓶颈。

Method: 构建DermBench基准，包含4000张真实皮肤病图像和专家认证的诊断叙述，使用基于LLM的评判员在临床维度上评分；训练DermEval无参考多模态评估器，对图像和生成叙述进行结构化批判和评分。

Result: 在4500个病例的多样化数据集上，DermBench和DermEval与专家评分高度一致，平均偏差分别为0.251和0.117（满分5分），可靠地测量了不同多模态LLM的诊断能力和可信度。

Conclusion: 该框架为皮肤病学多模态大语言模型提供了可靠、可扩展的评估方法，有助于识别模型局限性和偏见，促进负责任临床部署。

Abstract: Multimodal large language models (LLMs) are increasingly used to generate dermatology diagnostic narratives directly from images. However, reliable evaluation remains the primary bottleneck for responsible clinical deployment. We introduce a novel evaluation framework that combines DermBench, a meticulously curated benchmark, with DermEval, a robust automatic evaluator, to enable clinically meaningful, reproducible, and scalable assessment. We build DermBench, which pairs 4,000 real-world dermatology images with expert-certified diagnostic narratives and uses an LLM-based judge to score candidate narratives across clinically grounded dimensions, enabling consistent and comprehensive evaluation of multimodal models. For individual case assessment, we train DermEval, a reference-free multimodal evaluator. Given an image and a generated narrative, DermEval produces a structured critique along with an overall score and per-dimension ratings. This capability enables fine-grained, per-case analysis, which is critical for identifying model limitations and biases. Experiments on a diverse dataset of 4,500 cases demonstrate that DermBench and DermEval achieve close alignment with expert ratings, with mean deviations of 0.251 and 0.117 (out of 5), respectively, providing reliable measurement of diagnostic ability and trustworthiness across different multimodal LLMs.

</details>


### [37] [Taming Object Hallucinations with Verified Atomic Confidence Estimation](https://arxiv.org/abs/2511.09228)
*Jiarui Liu,Weihao Xuan,Zhijing Jin,Mona Diab*

Main category: cs.CV

TL;DR: TACO是一个通过自验证和置信度校准来减轻多模态大语言模型幻觉的框架，无需依赖外部视觉专家，在多个基准测试中表现优于直接提示和视觉对比解码方法。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型经常出现幻觉问题，特别是在对象存在性、属性和关系方面的错误，这影响了模型的可靠性。

Method: TACO将响应分解为原子查询，通过改写减少对措辞的敏感性，使用自一致性（黑盒）或自置信度（灰盒）聚合来估计置信度，最后用语言模型精炼答案。

Result: 在五个基准测试（POPE、MME、HallusionBench、AMBER和MM-Hal Bench）和两个MLLM模型上的实验表明，TACO持续优于直接提示和视觉对比解码，减少了系统性偏差并改善了置信度校准。

Conclusion: TACO框架在增强多模态大语言模型的忠实性方面表现出有效性。

Abstract: Multimodal Large Language Models (MLLMs) often suffer from hallucinations, particularly errors in object existence, attributes, or relations, which undermine their reliability. We introduce TACO (Verified Atomic Confidence Estimation), a simple framework that mitigates hallucinations through self-verification and confidence calibration without relying on external vision experts. TACO decomposes responses into atomic queries, paraphrases them to reduce sensitivity to wording, and estimates confidence using self-consistency (black-box) or self-confidence (gray-box) aggregation, before refining answers with a language model. Experiments on five benchmarks (POPE, MME, HallusionBench, AMBER, and MM-Hal Bench) with two MLLMs (\texttt{LLaVA-1.5-7B} and \texttt{CogVLM2}) show that TACO consistently outperforms direct prompting and Visual Contrastive Decoding, reduces systematic biases, and improves confidence calibration, demonstrating its effectiveness in enhancing the faithfulness of MLLMs.

</details>


### [38] [Enriching Knowledge Distillation with Cross-Modal Teacher Fusion](https://arxiv.org/abs/2511.09286)
*Amir M. Mansourian,Amir Mohammad Babaei,Shohreh Kasaei*

Main category: cs.CV

TL;DR: 本文提出RichKD方法，通过融合传统教师模型和CLIP的视觉-语言知识来增强多教师知识蒸馏，利用CLIP的多提示文本指导来丰富监督信号。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要依赖单模态视觉信息，缺乏知识多样性，忽略了跨模态表示的潜力。CLIP的视觉-语言知识作为补充监督源在知识蒸馏中尚未得到充分探索。

Method: 提出简单有效的框架，将传统教师的logits和特征与CLIP的logits和特征进行融合。通过CLIP的多提示文本指导，融合监督既捕获数据集特定信息又获得语义丰富的视觉线索。

Result: 融合教师产生更自信可靠的预测，显著增加自信正确案例，减少自信错误案例。与CLIP融合还能优化整个logit分布，为非目标类产生语义有意义的概率，提高类间一致性和蒸馏质量。

Conclusion: 尽管方法简单，但RichKD在多个基准测试中持续优于现有基线，并在分布偏移和输入损坏下表现出更强的鲁棒性。

Abstract: Multi-teacher knowledge distillation (KD), a more effective technique than traditional single-teacher methods, transfers knowledge from expert teachers to a compact student model using logit or feature matching. However, most existing approaches lack knowledge diversity, as they rely solely on unimodal visual information, overlooking the potential of cross-modal representations. In this work, we explore the use of CLIP's vision-language knowledge as a complementary source of supervision for KD, an area that remains largely underexplored. We propose a simple yet effective framework that fuses the logits and features of a conventional teacher with those from CLIP. By incorporating CLIP's multi-prompt textual guidance, the fused supervision captures both dataset-specific and semantically enriched visual cues. Beyond accuracy, analysis shows that the fused teacher yields more confident and reliable predictions, significantly increasing confident-correct cases while reducing confidently wrong ones. Moreover, fusion with CLIP refines the entire logit distribution, producing semantically meaningful probabilities for non-target classes, thereby improving inter-class consistency and distillation quality. Despite its simplicity, the proposed method, Enriching Knowledge Distillation (RichKD), consistently outperforms most existing baselines across multiple benchmarks and exhibits stronger robustness under distribution shifts and input corruptions.

</details>


### [39] [DensiCrafter: Physically-Constrained Generation and Fabrication of Self-Supporting Hollow Structures](https://arxiv.org/abs/2511.09298)
*Shengqi Dang,Fu Chai,Jiaxin Li,Chao Yuan,Wei Ye,Nan Cao*

Main category: cs.CV

TL;DR: DensiCrafter是一个生成轻量、自支撑3D空心结构的框架，通过优化密度场实现材料质量减少43%，同时保持几何保真度和可制造性。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型往往忽略物理约束和可制造性考虑，需要解决生成既轻量又自支撑的3D设计挑战。

Method: 从Trellis生成的粗体素网格出发，将其解释为连续密度场进行优化，引入三个可微分、物理约束且无需模拟的损失项，结合质量正则化和受限优化域。

Result: 在文本到3D任务中实现材料质量减少高达43%，相比最先进基线方法提高了稳定性并保持高几何保真度。

Conclusion: 该方法可与预训练的Trellis模型无缝集成，无需架构更改，真实世界3D打印实验证实空心设计可可靠制造且具有自支撑能力。

Abstract: The rise of 3D generative models has enabled automatic 3D geometry and texture synthesis from multimodal inputs (e.g., text or images). However, these methods often ignore physical constraints and manufacturability considerations. In this work, we address the challenge of producing 3D designs that are both lightweight and self-supporting. We present DensiCrafter, a framework for generating lightweight, self-supporting 3D hollow structures by optimizing the density field. Starting from coarse voxel grids produced by Trellis, we interpret these as continuous density fields to optimize and introduce three differentiable, physically constrained, and simulation-free loss terms. Additionally, a mass regularization penalizes unnecessary material, while a restricted optimization domain preserves the outer surface. Our method seamlessly integrates with pretrained Trellis-based models (e.g., Trellis, DSO) without any architectural changes. In extensive evaluations, we achieve up to 43% reduction in material mass on the text-to-3D task. Compared to state-of-the-art baselines, our method could improve the stability and maintain high geometric fidelity. Real-world 3D-printing experiments confirm that our hollow designs can be reliably fabricated and could be self-supporting.

</details>


### [40] [DualFete: Revisiting Teacher-Student Interactions from a Feedback Perspective for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2511.09319)
*Le Yi,Wei Huang,Lei Zhang,Kefu Zhao,Yan Wang,Zizhou Wang*

Main category: cs.CV

TL;DR: 该论文针对半监督医学图像分割中教师-学生框架存在的错误传播问题，提出了一种反馈机制，通过学生反馈教师伪标签引起的变化，使教师能够相应优化伪标签，并进一步设计了双教师反馈模型来增强反馈动态性。


<details>
  <summary>Details</summary>
Motivation: 传统教师-学生框架在医学图像分割中面临图像固有模糊性带来的挑战，容易产生错误监督，且学生对这些错误的迭代确认会导致自我强化的偏见。现有方法多依赖外部修改，忽视了框架内在的错误纠正潜力。

Method: 在教师-学生框架中引入反馈机制，包含两个关键组件：反馈归因器（指定触发学生更新的伪标签）和反馈接收器（确定反馈应用位置）。进一步提出双教师反馈模型，通过跨教师监督解决分歧，避免一致错误。

Result: 在三个医学图像基准数据集上的综合评估表明，该方法能有效解决半监督医学图像分割中的错误传播问题。

Conclusion: 所提出的反馈机制和双教师反馈模型能够有效对抗错误确认，提升半监督医学图像分割的性能。

Abstract: The teacher-student paradigm has emerged as a canonical framework in semi-supervised learning. When applied to medical image segmentation, the paradigm faces challenges due to inherent image ambiguities, making it particularly vulnerable to erroneous supervision. Crucially, the student's iterative reconfirmation of these errors leads to self-reinforcing bias. While some studies attempt to mitigate this bias, they often rely on external modifications to the conventional teacher-student framework, overlooking its intrinsic potential for error correction. In response, this work introduces a feedback mechanism into the teacher-student framework to counteract error reconfirmations. Here, the student provides feedback on the changes induced by the teacher's pseudo-labels, enabling the teacher to refine these labels accordingly. We specify that this interaction hinges on two key components: the feedback attributor, which designates pseudo-labels triggering the student's update, and the feedback receiver, which determines where to apply this feedback. Building on this, a dual-teacher feedback model is further proposed, which allows more dynamics in the feedback loop and fosters more gains by resolving disagreements through cross-teacher supervision while avoiding consistent errors. Comprehensive evaluations on three medical image benchmarks demonstrate the method's effectiveness in addressing error propagation in semi-supervised medical image segmentation.

</details>


### [41] [FQ-PETR: Fully Quantized Position Embedding Transformation for Multi-View 3D Object Detection](https://arxiv.org/abs/2511.09347)
*Jiangyong Yu,Changyong Shu,Sifan Zhou,Zichen Yu,Xing Hu,Yan Chen,Dawei Yang*

Main category: cs.CV

TL;DR: FQ-PETR是一个针对PETR系列3D检测模型的全量化框架，通过量化友好的位置嵌入、双查找表和非线性算子优化，在W8A8量化下实现接近浮点精度（仅1%性能下降），同时减少75%延迟。


<details>
  <summary>Details</summary>
Motivation: PETR系列模型在3D检测基准上表现出色，但面临高计算成本和内存占用的部署挑战。现有量化方法直接应用于PETR会导致严重精度下降，主要由于多模态特征尺度差异和非线性算子量化困难。

Method: 提出三个关键创新：1）量化友好的LiDAR射线位置嵌入（QFPE），用单点采样替代多点采样；2）双查找表（DULUT）近似复杂非线性函数；3）数值稳定后量化（QANS）减少注意力失真。

Result: 在PETR、StreamPETR、PETRv2、MV2d等模型上，FQ-PETR在W8A8量化下实现接近浮点精度（仅1%性能下降），同时减少高达75%的延迟，显著优于现有PTQ和QAT基线方法。

Conclusion: FQ-PETR成功解决了PETR系列模型量化部署的挑战，在保持高精度的同时大幅提升推理效率，为自动驾驶3D检测的实际部署提供了有效解决方案。

Abstract: Camera-based multi-view 3D detection is crucial for autonomous driving. PETR and its variants (PETRs) excel in benchmarks but face deployment challenges due to high computational cost and memory footprint. Quantization is an effective technique for compressing deep neural networks by reducing the bit width of weights and activations. However, directly applying existing quantization methods to PETRs leads to severe accuracy degradation. This issue primarily arises from two key challenges: (1) significant magnitude disparity between multi-modal features-specifically, image features and camera-ray positional embeddings (PE), and (2) the inefficiency and approximation error of quantizing non-linear operators, which commonly rely on hardware-unfriendly computations. In this paper, we propose FQ-PETR, a fully quantized framework for PETRs, featuring three key innovations: (1) Quantization-Friendly LiDAR-ray Position Embedding (QFPE): Replacing multi-point sampling with LiDAR-prior-guided single-point sampling and anchor-based embedding eliminates problematic non-linearities (e.g., inverse-sigmoid) and aligns PE scale with image features, preserving accuracy. (2) Dual-Lookup Table (DULUT): This algorithm approximates complex non-linear functions using two cascaded linear LUTs, achieving high fidelity with minimal entries and no specialized hardware. (3) Quantization After Numerical Stabilization (QANS): Performing quantization after softmax numerical stabilization mitigates attention distortion from large inputs. On PETRs (e.g. PETR, StreamPETR, PETRv2, MV2d), FQ-PETR under W8A8 achieves near-floating-point accuracy (1% degradation) while reducing latency by up to 75%, significantly outperforming existing PTQ and QAT baselines.

</details>


### [42] [Learning by Neighbor-Aware Semantics, Deciding by Open-form Flows: Towards Robust Zero-Shot Skeleton Action Recognition](https://arxiv.org/abs/2511.09388)
*Yang Chen,Miaoge Li,Zhijie Rao,Deze Zeng,Song Guo,Jingcai Guo*

Main category: cs.CV

TL;DR: 提出Flora方法解决零样本骨架动作识别中的语义对齐和分类器限制问题，通过灵活的邻居感知语义调整和分布感知流分类器实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本骨架动作识别方法存在两个基本问题：(1) 由于不完美的语义导致的脆弱点对点对齐，(2) 受限于静态决策边界和粗粒度锚点的刚性分类器。

Method: Flora方法包含两个核心组件：1) 灵活的邻居感知语义调整，通过融入相邻类间上下文线索形成方向感知的区域语义；2) 开放形式的分布感知流分类器，使用无噪声流匹配来弥合语义和骨架潜在嵌入之间的模态分布差距。

Result: 在三个基准数据集上的广泛实验验证了该方法的有效性，即使在仅使用10%可见数据训练时也表现出特别令人印象深刻的性能。

Conclusion: Flora方法通过创新的语义调整和分布感知分类器设计，有效解决了零样本骨架动作识别中的关键挑战，展现了优越的性能表现。

Abstract: Recognizing unseen skeleton action categories remains highly challenging due to the absence of corresponding skeletal priors. Existing approaches generally follow an "align-then-classify" paradigm but face two fundamental issues, i.e., (i) fragile point-to-point alignment arising from imperfect semantics, and (ii) rigid classifiers restricted by static decision boundaries and coarse-grained anchors. To address these issues, we propose a novel method for zero-shot skeleton action recognition, termed $\texttt{$\textbf{Flora}$}$, which builds upon $\textbf{F}$lexib$\textbf{L}$e neighb$\textbf{O}$r-aware semantic attunement and open-form dist$\textbf{R}$ibution-aware flow cl$\textbf{A}$ssifier. Specifically, we flexibly attune textual semantics by incorporating neighboring inter-class contextual cues to form direction-aware regional semantics, coupled with a cross-modal geometric consistency objective that ensures stable and robust point-to-region alignment. Furthermore, we employ noise-free flow matching to bridge the modality distribution gap between semantic and skeleton latent embeddings, while a condition-free contrastive regularization enhances discriminability, leading to a distribution-aware classifier with fine-grained decision boundaries achieved through token-level velocity predictions. Extensive experiments on three benchmark datasets validate the effectiveness of our method, showing particularly impressive performance even when trained with only 10\% of the seen data. Code is available at https://github.com/cseeyangchen/Flora.

</details>


### [43] [OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS](https://arxiv.org/abs/2511.09397)
*Haiyi Li,Qi Chen,Denis Kalkofen,Hsiang-Ting Chen*

Main category: cs.CV

TL;DR: 本文提出了OUGS框架，通过基于3D高斯原语物理参数的显式不确定性建模，结合语义分割实现对象感知的主动重建，显著提升了3D高斯泼溅在特定对象重建中的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在复杂场景中重建特定对象时效率不高，主要原因是场景级不确定性度量容易受到无关背景干扰，导致视图选择效率低下。

Method: 从3D高斯原语的物理参数（位置、尺度、旋转）推导不确定性，通过渲染雅可比矩阵传播协方差，建立可解释的不确定性模型，并集成语义分割掩码生成对象感知的不确定性评分。

Result: 在公开数据集上的实验表明，该方法显著提高了3DGS重建过程的效率，相比现有最先进方法在目标对象上实现了更高质量的重建，同时也可作为全局场景的鲁棒不确定性估计器。

Conclusion: OUGS框架通过物理基础的不确定性建模和对象感知的视图选择，有效解决了3D高斯泼溅在对象中心重建任务中的效率问题，为主动重建提供了更原则性的解决方案。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.

</details>


### [44] [BronchOpt : Vision-Based Pose Optimization with Fine-Tuned Foundation Models for Accurate Bronchoscopy Navigation](https://arxiv.org/abs/2511.09443)
*Hongchao Shu,Roger D. Soberanis-Mukul,Jiru Xu,Hao Ding,Morgan Ringel,Mali Shen,Saif Iftekar Sayed,Hedyeh Rafii-Tari,Mathias Unberath*

Main category: cs.CV

TL;DR: 本文提出了一个基于视觉的支气管镜导航框架，通过2D-3D配准实现术中内窥镜视图与术前CT解剖的帧级配准，并创建了首个公开的合成基准数据集用于标准化评估。


<details>
  <summary>Details</summary>
Motivation: 解决支气管镜术中定位的挑战，包括呼吸运动、解剖变异和CT到身体的差异导致的变形和错位问题，现有视觉方法在跨域和跨患者泛化方面存在局限。

Method: 使用微调的模态和域不变编码器直接计算真实内窥镜RGB帧与CT渲染深度图之间的相似性，通过可微分渲染模块迭代优化相机位姿，实现深度一致性配准。

Result: 仅使用与基准不同的合成数据训练，模型达到平均平移误差2.65毫米和旋转误差0.19弧度，在真实患者数据上表现出强大的跨域泛化能力。

Conclusion: 该框架通过迭代视觉优化实现了稳健、域不变的定位，新基准为基于视觉的支气管镜导航的标准化进展奠定了基础。

Abstract: Accurate intra-operative localization of the bronchoscope tip relative to patient anatomy remains challenging due to respiratory motion, anatomical variability, and CT-to-body divergence that cause deformation and misalignment between intra-operative views and pre-operative CT. Existing vision-based methods often fail to generalize across domains and patients, leading to residual alignment errors. This work establishes a generalizable foundation for bronchoscopy navigation through a robust vision-based framework and a new synthetic benchmark dataset that enables standardized and reproducible evaluation. We propose a vision-based pose optimization framework for frame-wise 2D-3D registration between intra-operative endoscopic views and pre-operative CT anatomy. A fine-tuned modality- and domain-invariant encoder enables direct similarity computation between real endoscopic RGB frames and CT-rendered depth maps, while a differentiable rendering module iteratively refines camera poses through depth consistency. To enhance reproducibility, we introduce the first public synthetic benchmark dataset for bronchoscopy navigation, addressing the lack of paired CT-endoscopy data. Trained exclusively on synthetic data distinct from the benchmark, our model achieves an average translational error of 2.65 mm and a rotational error of 0.19 rad, demonstrating accurate and stable localization. Qualitative results on real patient data further confirm strong cross-domain generalization, achieving consistent frame-wise 2D-3D alignment without domain-specific adaptation. Overall, the proposed framework achieves robust, domain-invariant localization through iterative vision-based optimization, while the new benchmark provides a foundation for standardized progress in vision-based bronchoscopy navigation.

</details>


### [45] [Hand Held Multi-Object Tracking Dataset in American Football](https://arxiv.org/abs/2511.09455)
*Rintaro Otsubo,Kanta Sawafuji,Hideo Saito*

Main category: cs.CV

TL;DR: 本文构建了首个美式足球运动员专用检测与跟踪数据集，解决了该领域缺乏标准化数据集的问题，并在拥挤场景下实现了准确的检测与跟踪。


<details>
  <summary>Details</summary>
Motivation: 当前多目标跟踪方法主要针对日常场景或特定运动（如足球、篮球），而美式足球由于频繁遮挡和身体接触等固有挑战，缺乏标准化数据集，难以公平比较不同方法。

Method: 构建首个美式足球运动员专用检测与跟踪数据集，比较评估各种检测与跟踪方法，包括微调检测模型和集成重识别模型到跟踪系统中。

Result: 微调检测模型相比预训练模型性能提升；将微调检测器和重识别模型集成到跟踪系统中，相比现有方法显著提高了跟踪精度；在拥挤场景下也能实现准确检测与跟踪。

Conclusion: 本研究实现了在具有挑战性的高密度美式足球场景中鲁棒的检测与跟踪，填补了传统方法在该领域的空白。

Abstract: Multi-Object Tracking (MOT) plays a critical role in analyzing player behavior from videos, enabling performance evaluation. Current MOT methods are often evaluated using publicly available datasets. However, most of these focus on everyday scenarios such as pedestrian tracking or are tailored to specific sports, including soccer and basketball. Despite the inherent challenges of tracking players in American football, such as frequent occlusion and physical contact, no standardized dataset has been publicly available, making fair comparisons between methods difficult. To address this gap, we constructed the first dedicated detection and tracking dataset for the American football players and conducted a comparative evaluation of various detection and tracking methods. Our results demonstrate that accurate detection and tracking can be achieved even in crowded scenarios. Fine-tuning detection models improved performance over pre-trained models. Furthermore, when these fine-tuned detectors and re-identification models were integrated into tracking systems, we observed notable improvements in tracking accuracy compared to existing approaches. This work thus enables robust detection and tracking of American football players in challenging, high-density scenarios previously underserved by conventional methods.

</details>


### [46] [Revisiting Cross-Architecture Distillation: Adaptive Dual-Teacher Transfer for Lightweight Video Models](https://arxiv.org/abs/2511.09469)
*Ying Peng,Hongsen Ye,Changxin Huang,Xiping Hu,Jian Chen,Runhao Zeng*

Main category: cs.CV

TL;DR: 提出了一种双教师知识蒸馏框架，利用异构ViT教师和同构CNN教师协同指导轻量级CNN学生，通过差异感知教师加权和结构差异感知蒸馏策略，显著提升了视频动作识别的性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在视频动作识别中表现优异但计算成本高，轻量级CNNs效率高但精度不足。现有跨架构知识蒸馏方法存在架构不匹配问题，且忽视了同构CNN教师的价值。

Method: 双教师知识蒸馏框架：1) 差异感知教师加权，基于教师置信度和与学生预测差异动态融合ViT和CNN教师的预测；2) 结构差异感知蒸馏，通过轻量级辅助分支学习ViT和CNN教师之间的残差特征。

Result: 在HMDB51、EPIC-KITCHENS-100和Kinetics-400基准测试中，该方法始终优于最先进的蒸馏方法，在HMDB51上最高获得5.95%的准确率提升。

Conclusion: 所提出的双教师知识蒸馏框架有效解决了架构不匹配问题，通过协同利用异构和同构教师，显著提升了轻量级CNN在视频动作识别中的性能。

Abstract: Vision Transformers (ViTs) have achieved strong performance in video action recognition, but their high computational cost limits their practicality. Lightweight CNNs are more efficient but suffer from accuracy gaps. Cross-Architecture Knowledge Distillation (CAKD) addresses this by transferring knowledge from ViTs to CNNs, yet existing methods often struggle with architectural mismatch and overlook the value of stronger homogeneous CNN teachers. To tackle these challenges, we propose a Dual-Teacher Knowledge Distillation framework that leverages both a heterogeneous ViT teacher and a homogeneous CNN teacher to collaboratively guide a lightweight CNN student. We introduce two key components: (1) Discrepancy-Aware Teacher Weighting, which dynamically fuses the predictions from ViT and CNN teachers by assigning adaptive weights based on teacher confidence and prediction discrepancy with the student, enabling more informative and effective supervision; and (2) a Structure Discrepancy-Aware Distillation strategy, where the student learns the residual features between ViT and CNN teachers via a lightweight auxiliary branch, focusing on transferable architectural differences without mimicking all of ViT's high-dimensional patterns. Extensive experiments on benchmarks including HMDB51, EPIC-KITCHENS-100, and Kinetics-400 demonstrate that our method consistently outperforms state-of-the-art distillation approaches, achieving notable performance improvements with a maximum accuracy gain of 5.95% on HMDB51.

</details>


### [47] [DreamPose3D: Hallucinative Diffusion with Prompt Learning for 3D Human Pose Estimation](https://arxiv.org/abs/2511.09502)
*Jerrin Bright,Yuhao Chen,John S. Zelek*

Main category: cs.CV

TL;DR: DreamPose3D是一个基于扩散模型的3D人体姿态估计框架，通过动作感知推理和时间想象力结合，解决了现有方法在时间一致性和关节关系建模方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态估计方法主要依赖几何线索并独立预测每帧姿态，无法有效解决模糊运动和泛化到真实场景的问题。受人类理解和预测运动方式的启发，需要结合高层意图和结构关系建模。

Method: 提出DreamPose3D框架：1）使用从2D姿态序列提取的任务相关动作提示动态调节去噪过程；2）引入表示编码器，在注意力机制中融入运动学关节亲和力；3）使用幻觉姿态解码器预测时间一致的3D姿态序列。

Result: 在Human3.6M和MPI-3DHP基准数据集上实现了最先进的性能。在广播棒球数据集上的测试表明，即使在模糊和噪声的2D输入下，也能有效处理时间一致性和意图驱动的运动变化。

Conclusion: DreamPose3D通过结合动作感知推理、结构关系建模和时间想象力，显著提升了3D人体姿态估计的准确性和鲁棒性，特别是在处理模糊运动和真实场景方面表现出色。

Abstract: Accurate 3D human pose estimation remains a critical yet unresolved challenge, requiring both temporal coherence across frames and fine-grained modeling of joint relationships. However, most existing methods rely solely on geometric cues and predict each 3D pose independently, which limits their ability to resolve ambiguous motions and generalize to real-world scenarios. Inspired by how humans understand and anticipate motion, we introduce DreamPose3D, a diffusion-based framework that combines action-aware reasoning with temporal imagination for 3D pose estimation. DreamPose3D dynamically conditions the denoising process using task-relevant action prompts extracted from 2D pose sequences, capturing high-level intent. To model the structural relationships between joints effectively, we introduce a representation encoder that incorporates kinematic joint affinity into the attention mechanism. Finally, a hallucinative pose decoder predicts temporally coherent 3D pose sequences during training, simulating how humans mentally reconstruct motion trajectories to resolve ambiguity in perception. Extensive experiments on benchmarked Human3.6M and MPI-3DHP datasets demonstrate state-of-the-art performance across all metrics. To further validate DreamPose3D's robustness, we tested it on a broadcast baseball dataset, where it demonstrated strong performance despite ambiguous and noisy 2D inputs, effectively handling temporal consistency and intent-driven motion variations.

</details>
