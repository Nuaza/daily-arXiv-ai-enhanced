{"id": "2602.22649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22649", "abs": "https://arxiv.org/abs/2602.22649", "authors": ["Woojae Hong", "Jong Ha Hwang", "Jiyong Chung", "Joongyeon Choi", "Hyunngun Kim", "Yong Hwy Kim"], "title": "Interactive Medical-SAM2 GUI: A Napari-based semi-automatic annotation tool for medical images", "comment": "6 pages, 2 figures, Planning to submit JOSS (Journal of Open Source Software)", "summary": "Interactive Medical-SAM2 GUI is an open-source desktop application for semi-automatic annotation of 2D and 3D medical images. Built on the Napari multi-dimensional viewer, box/point prompting is integrated with SAM2-style propagation by treating a 3D volume as a slice sequence, enabling mask propagation from sparse prompts using Medical-SAM2 on top of SAM2. Voxel-level annotation remains essential for developing and validating medical imaging algorithms, yet manual labeling is slow and expensive for 3D scans, and existing integrations frequently emphasize per-slice interaction without providing a unified, cohort-oriented workflow for navigation, propagation, interactive correction, and quantitative export in a single local pipeline. To address this practical limitation, a local-first Napari workflow is provided for efficient 3D annotation across multiple studies using standard DICOM series and/or NIfTI volumes. Users can annotate cases sequentially under a single root folder with explicit proceed/skip actions, initialize objects via box-first prompting (including first/last-slice initialization for single-object propagation), refine predictions with point prompts, and finalize labels through prompt-first correction prior to saving. During export, per-object volumetry and 3D volume rendering are supported, and image geometry is preserved via SimpleITK. The GUI is implemented in Python using Napari and PyTorch, with optional N4 bias-field correction, and is intended exclusively for research annotation workflows. The code is released on the project page: https://github.com/SKKU-IBE/Medical-SAM2GUI/.", "AI": {"tldr": "Medical-SAM2 GUI\u662f\u4e00\u4e2a\u57fa\u4e8eNapari\u7684\u5f00\u6e90\u684c\u9762\u5e94\u7528\uff0c\u7528\u4e8e2D/3D\u533b\u5b66\u56fe\u50cf\u7684\u534a\u81ea\u52a8\u6807\u6ce8\uff0c\u901a\u8fc7SAM2\u4f20\u64ad\u548c\u4ea4\u4e92\u5f0f\u4fee\u6b63\u63d0\u9ad83D\u6807\u6ce8\u6548\u7387\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u7b97\u6cd5\u5f00\u53d1\u9700\u8981\u4f53\u7d20\u7ea7\u6807\u6ce8\uff0c\u4f463D\u626b\u63cf\u7684\u624b\u52a8\u6807\u6ce8\u8017\u65f6\u6602\u8d35\uff0c\u73b0\u6709\u5de5\u5177\u7f3a\u4e4f\u7edf\u4e00\u7684\u961f\u5217\u5bfc\u5411\u5de5\u4f5c\u6d41\u7a0b\uff0c\u65e0\u6cd5\u5728\u4e00\u4e2a\u672c\u5730\u7ba1\u9053\u4e2d\u6574\u5408\u5bfc\u822a\u3001\u4f20\u64ad\u3001\u4ea4\u4e92\u4fee\u6b63\u548c\u5b9a\u91cf\u5bfc\u51fa\u3002", "method": "\u57fa\u4e8eNapari\u6784\u5efa\uff0c\u5c063D\u4f53\u79ef\u89c6\u4e3a\u5207\u7247\u5e8f\u5217\uff0c\u96c6\u6210SAM2\u5f0f\u4f20\u64ad\u548cMedical-SAM2\uff1b\u63d0\u4f9b\u672c\u5730\u4f18\u5148\u5de5\u4f5c\u6d41\uff0c\u652f\u6301DICOM/NIfTI\u683c\u5f0f\uff0c\u901a\u8fc7\u6846/\u70b9\u63d0\u793a\u521d\u59cb\u5316\u5bf9\u8c61\uff0c\u652f\u6301\u9996\u5c3e\u5207\u7247\u521d\u59cb\u5316\u3001\u4ea4\u4e92\u4fee\u6b63\uff0c\u6700\u540e\u4fdd\u5b58\u6807\u6ce8\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5f00\u6e90GUI\u5de5\u5177\uff0c\u652f\u6301\u591a\u75c5\u4f8b\u987a\u5e8f\u6807\u6ce8\u3001\u5355\u5bf9\u8c61\u4f20\u64ad\u3001\u4ea4\u4e92\u5f0f\u4fee\u6b63\uff0c\u5bfc\u51fa\u65f6\u652f\u6301\u6bcf\u5bf9\u8c61\u4f53\u79ef\u6d4b\u91cf\u548c3D\u4f53\u79ef\u6e32\u67d3\uff0c\u901a\u8fc7SimpleITK\u4fdd\u6301\u56fe\u50cf\u51e0\u4f55\u4fe1\u606f\u3002", "conclusion": "\u8be5\u5de5\u5177\u4e3a\u533b\u5b66\u5f71\u50cf\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u76843D\u534a\u81ea\u52a8\u6807\u6ce8\u89e3\u51b3\u65b9\u6848\uff0c\u6574\u5408\u4e86\u5bfc\u822a\u3001\u4f20\u64ad\u3001\u4fee\u6b63\u548c\u5bfc\u51fa\u529f\u80fd\uff0c\u4e13\u95e8\u7528\u4e8e\u7814\u7a76\u6807\u6ce8\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2602.22654", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22654", "abs": "https://arxiv.org/abs/2602.22654", "authors": ["Bowen Cui", "Yuanbin Wang", "Huajiang Xu", "Biaolong Chen", "Aixi Zhang", "Hao Jiang", "Zhengzheng Jin", "Xu Liu", "Pipei Huang"], "title": "Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache", "comment": "Accepted by CVPR 2026", "summary": "Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.", "AI": {"tldr": "DPCache\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u52a0\u901f\u6846\u67b6\uff0c\u5c06\u91c7\u6837\u52a0\u901f\u89c6\u4e3a\u5168\u5c40\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u9009\u62e9\u6700\u4f18\u5173\u952e\u65f6\u95f4\u6b65\u5e8f\u5217\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u53d7\u5230\u591a\u6b65\u8fed\u4ee3\u91c7\u6837\u7684\u8ba1\u7b97\u5f00\u9500\u9650\u5236\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u7f13\u5b58\u7684\u52a0\u901f\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u6216\u5c40\u90e8\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u672a\u8003\u8651\u53bb\u566a\u8f68\u8ff9\u7684\u5168\u5c40\u7ed3\u6784\uff0c\u5bb9\u6613\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u89c6\u89c9\u4f2a\u5f71\u3002", "method": "DPCache\u5c06\u6269\u6563\u91c7\u6837\u52a0\u901f\u6784\u5efa\u4e3a\u5168\u5c40\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002\u9996\u5148\u4ece\u5c0f\u578b\u6821\u51c6\u96c6\u6784\u5efa\u8def\u5f84\u611f\u77e5\u6210\u672c\u5f20\u91cf\uff0c\u91cf\u5316\u5728\u7ed9\u5b9a\u524d\u4e00\u4e2a\u5173\u952e\u65f6\u95f4\u6b65\u6761\u4ef6\u4e0b\u8df3\u8fc7\u65f6\u95f4\u6b65\u7684\u8def\u5f84\u76f8\u5173\u8bef\u5dee\u3002\u7136\u540e\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u9009\u62e9\u6700\u5c0f\u5316\u603b\u8def\u5f84\u6210\u672c\u7684\u5173\u952e\u65f6\u95f4\u6b65\u5e8f\u5217\uff0c\u5728\u63a8\u7406\u65f6\u53ea\u5728\u5173\u952e\u65f6\u95f4\u6b65\u8fdb\u884c\u5b8c\u6574\u8ba1\u7b97\uff0c\u4e2d\u95f4\u8f93\u51fa\u4f7f\u7528\u7f13\u5b58\u7279\u5f81\u9ad8\u6548\u9884\u6d4b\u3002", "result": "\u5728DiT\u3001FLUX\u548cHunyuanVideo\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDPCache\u5728\u4fdd\u6301\u6700\u5c0f\u8d28\u91cf\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002\u5728FLUX\u4e0a\uff0c4.87\u500d\u52a0\u901f\u65f6ImageReward\u63d0\u5347+0.031\uff0c3.54\u500d\u52a0\u901f\u65f6\u751a\u81f3\u8d85\u8fc7\u5168\u6b65\u57fa\u7ebf+0.028 ImageReward\uff0c\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u3002", "conclusion": "DPCache\u901a\u8fc7\u8def\u5f84\u611f\u77e5\u7684\u5168\u5c40\u8c03\u5ea6\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u5ffd\u7565\u5168\u5c40\u7ed3\u6784\u7684\u95ee\u9898\uff0c\u5728\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22666", "abs": "https://arxiv.org/abs/2602.22666", "authors": ["Xuelu Li", "Zhaonan Wang", "Xiaogang Wang", "Lei Wu", "Manyi Li", "Changhe Tu"], "title": "ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals", "comment": null, "summary": "Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part segmentation. Their reliance on heuristic clustering or pre-trained models often causes optimization to converge to local minima, especially for complex multi-part objects. To address these limitations, we propose ArtPro, a novel self-supervised framework that introduces adaptive integration of mobility proposals. Our approach begins with an over-segmentation initialization guided by geometry features and motion priors, generating part proposals with plausible motion hypotheses. During optimization, we dynamically merge these proposals by analyzing motion consistency among spatial neighbors, while a collision-aware motion pruning mechanism prevents erroneous kinematic estimation. Extensive experiments on both synthetic and real-world objects demonstrate that ArtPro achieves robust reconstruction of complex multi-part objects, significantly outperforming existing methods in accuracy and stability.", "AI": {"tldr": "ArtPro\uff1a\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6574\u5408\u8fd0\u52a8\u63d0\u8bae\u6765\u91cd\u5efa\u590d\u6742\u591a\u90e8\u4ef6\u94f0\u63a5\u7269\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u521d\u59cb\u5206\u5272\u654f\u611f\u4e14\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53ef\u5fae\u5206\u6e32\u67d3\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\u5bf9\u521d\u59cb\u90e8\u4ef6\u5206\u5272\u9ad8\u5ea6\u654f\u611f\uff0c\u4f9d\u8d56\u542f\u53d1\u5f0f\u805a\u7c7b\u6216\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5bfc\u81f4\u590d\u6742\u591a\u90e8\u4ef6\u7269\u4f53\u4f18\u5316\u65f6\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u3002", "method": "\u63d0\u51faArtPro\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u51e0\u4f55\u7279\u5f81\u548c\u8fd0\u52a8\u5148\u9a8c\u8fdb\u884c\u8fc7\u5206\u5272\u521d\u59cb\u5316\uff0c\u751f\u6210\u5e26\u5408\u7406\u8fd0\u52a8\u5047\u8bbe\u7684\u90e8\u4ef6\u63d0\u8bae\uff1b2\uff09\u4f18\u5316\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5206\u6790\u7a7a\u95f4\u90bb\u57df\u7684\u8fd0\u52a8\u4e00\u81f4\u6027\u52a8\u6001\u5408\u5e76\u63d0\u8bae\uff1b3\uff09\u78b0\u649e\u611f\u77e5\u7684\u8fd0\u52a8\u526a\u679d\u673a\u5236\u9632\u6b62\u9519\u8bef\u7684\u8fd0\u52a8\u5b66\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u7269\u4f53\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cArtPro\u80fd\u591f\u7a33\u5065\u5730\u91cd\u5efa\u590d\u6742\u591a\u90e8\u4ef6\u7269\u4f53\uff0c\u5728\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ArtPro\u901a\u8fc7\u81ea\u9002\u5e94\u6574\u5408\u8fd0\u52a8\u63d0\u8bae\u7684\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u94f0\u63a5\u7269\u4f53\u91cd\u5efa\u4e2d\u5bf9\u521d\u59cb\u5206\u5272\u654f\u611f\u7684\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u591a\u90e8\u4ef6\u7269\u4f53\u7684\u9ad8\u4fdd\u771f\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22667", "abs": "https://arxiv.org/abs/2602.22667", "authors": ["Changqing Zhou", "Yueru Luo", "Han Zhang", "Zeyu Jiang", "Changhao Chen"], "title": "Monocular Open Vocabulary Occupancy Prediction for Indoor Scenes", "comment": "Accepted by CVPR2026", "summary": "Open-vocabulary 3D occupancy is vital for embodied agents, which need to understand complex indoor environments where semantic categories are abundant and evolve beyond fixed taxonomies. While recent work has explored open-vocabulary occupancy in outdoor driving scenarios, such methods transfer poorly indoors, where geometry is denser, layouts are more intricate, and semantics are far more fine-grained. To address these challenges, we adopt a geometry-only supervision paradigm that uses only binary occupancy labels (occupied vs free). Our framework builds upon 3D Language-Embedded Gaussians, which serve as a unified intermediate representation coupling fine-grained 3D geometry with a language-aligned semantic embedding. On the geometry side, we find that existing Gaussian-to-Occupancy operators fail to converge under such weak supervision, and we introduce an opacity-aware, Poisson-based approach that stabilizes volumetric aggregation. On the semantic side, direct alignment between rendered features and open-vocabulary segmentation features suffers from feature mixing; we therefore propose a Progressive Temperature Decay schedule that gradually sharpens opacities during splatting, strengthening Gaussian-language alignment. On Occ-ScanNet, our framework achieves 59.50 IoU and 21.05 mIoU in the open-vocabulary setting, surpassing all existing occupancy methods in IoU and outperforming prior open-vocabulary approaches by a large margin in mIoU. Code will be released at https://github.com/JuIvyy/LegoOcc.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5ba4\u5185\u573a\u666f\u7684\u5f00\u653e\u8bcd\u6c473D\u5360\u7528\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u4ec5\u51e0\u4f55\u76d1\u7763\uff08\u4e8c\u503c\u5360\u7528\u6807\u7b7e\uff09\u548c3D\u8bed\u8a00\u5d4c\u5165\u9ad8\u65af\u8868\u793a\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6cca\u677e\u805a\u5408\u548c\u6e10\u8fdb\u6e29\u5ea6\u8870\u51cf\u5b9e\u73b0\u51e0\u4f55\u4e0e\u8bed\u4e49\u7684\u8054\u5408\u5b66\u4e60\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u8bcd\u6c473D\u5360\u7528\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5ba4\u5916\u9a7e\u9a76\u573a\u666f\uff0c\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002\u5ba4\u5185\u73af\u5883\u51e0\u4f55\u66f4\u5bc6\u96c6\u3001\u5e03\u5c40\u66f4\u590d\u6742\u3001\u8bed\u4e49\u66f4\u7ec6\u7c92\u5ea6\uff0c\u9700\u8981\u4e13\u95e8\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u4ec5\u51e0\u4f55\u76d1\u7763\u8303\u5f0f\uff08\u4ec5\u4f7f\u7528\u4e8c\u503c\u5360\u7528\u6807\u7b7e\uff09\uff0c\u57fa\u4e8e3D\u8bed\u8a00\u5d4c\u5165\u9ad8\u65af\u4f5c\u4e3a\u7edf\u4e00\u4e2d\u95f4\u8868\u793a\u3002\u51e0\u4f55\u65b9\u9762\uff1a\u63d0\u51fa\u4e0d\u900f\u660e\u5ea6\u611f\u77e5\u7684\u6cca\u677e\u57fa\u65b9\u6cd5\u7a33\u5b9a\u4f53\u7d20\u805a\u5408\uff1b\u8bed\u4e49\u65b9\u9762\uff1a\u63d0\u51fa\u6e10\u8fdb\u6e29\u5ea6\u8870\u51cf\u8c03\u5ea6\uff0c\u5728\u6e32\u67d3\u8fc7\u7a0b\u4e2d\u9010\u6e10\u9510\u5316\u4e0d\u900f\u660e\u5ea6\uff0c\u589e\u5f3a\u9ad8\u65af-\u8bed\u8a00\u5bf9\u9f50\u3002", "result": "\u5728Occ-ScanNet\u6570\u636e\u96c6\u4e0a\uff0c\u5f00\u653e\u8bcd\u6c47\u8bbe\u7f6e\u4e0b\u8fbe\u523059.50 IoU\u548c21.05 mIoU\uff0c\u5728IoU\u65b9\u9762\u8d85\u8d8a\u6240\u6709\u73b0\u6709\u5360\u7528\u65b9\u6cd5\uff0c\u5728mIoU\u65b9\u9762\u5927\u5e45\u9886\u5148\u5148\u524d\u5f00\u653e\u8bcd\u6c47\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5ba4\u5185\u5f00\u653e\u8bcd\u6c473D\u5360\u7528\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4ec5\u51e0\u4f55\u76d1\u7763\u548c\u521b\u65b0\u7684\u51e0\u4f55/\u8bed\u4e49\u5bf9\u9f50\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5728\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u9ad8\u6027\u80fd\u5360\u7528\u9884\u6d4b\u3002"}}
{"id": "2602.22678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22678", "abs": "https://arxiv.org/abs/2602.22678", "authors": ["Quoc-Khang Tran", "Minh-Thien Nguyen", "Nguyen-Khang Pham"], "title": "ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport", "comment": "Preprint submitted to Expert Systems with Applications", "summary": "Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval. The proposed framework integrates CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss to enhance global cross-modal consistency and mitigate modality gap issues. Extensive experiments on three Vietnamese benchmarks (UITOpenViIC, KTVIC, and Crossmodal-3600) demonstrate that ViCLIP-OT consistently outperforms CLIP and SigLIP baselines in both in-domain and zero-shot settings. On UIT-OpenViIC, the model achieves an average Recall@K of 67.34%, improving upon CLIP by 5.75 percentage points. In zero-shot evaluation on Crossmodal-3600, ViCLIPOT surpasses CLIP by 11.72 percentage points. Embedding-space analysis further confirms improved alignment and reduced modality gap. The results indicate that integrating SIGROT provides an effective and scalable strategy for cross-modal retrieval in low-resource languages, offering practical implications for intelligent multimedia retrieval systems in Vietnamese and other underrepresented linguistic contexts.", "AI": {"tldr": "ViCLIP-OT\uff1a\u9488\u5bf9\u8d8a\u5357\u8bed\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408CLIP\u5bf9\u6bd4\u5b66\u4e60\u548cSIGROT\u635f\u5931\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u4f18\u5316\uff0c\u5728\u8d8a\u5357\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u8bbe\u8ba1\u9002\u5408\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u6a21\u578b\u3002", "method": "\u63d0\u51faViCLIP-OT\u6846\u67b6\uff0c\u96c6\u6210CLIP\u98ce\u683c\u7684\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u76f8\u4f3c\u6027\u56fe\u6b63\u5219\u5316\u6700\u4f18\u4f20\u8f93\uff08SIGROT\uff09\u635f\u5931\uff0c\u589e\u5f3a\u5168\u5c40\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u5e76\u7f13\u89e3\u6a21\u6001\u5dee\u8ddd\u95ee\u9898\u3002", "result": "\u5728\u4e09\u4e2a\u8d8a\u5357\u8bed\u57fa\u51c6\u6d4b\u8bd5\uff08UITOpenViIC\u3001KTVIC\u3001Crossmodal-3600\uff09\u4e0a\u5747\u4f18\u4e8eCLIP\u548cSigLIP\u57fa\u7ebf\u3002\u5728UIT-OpenViIC\u4e0a\u5e73\u5747Recall@K\u8fbe67.34%\uff0c\u6bd4CLIP\u63d0\u53475.75\u4e2a\u767e\u5206\u70b9\uff1b\u5728Crossmodal-3600\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\u6bd4CLIP\u63d0\u534711.72\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "SIGROT\u96c6\u6210\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8de8\u6a21\u6001\u68c0\u7d22\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u7b56\u7565\uff0c\u5bf9\u8d8a\u5357\u8bed\u53ca\u5176\u4ed6\u4ee3\u8868\u6027\u4e0d\u8db3\u8bed\u8a00\u73af\u5883\u7684\u667a\u80fd\u591a\u5a92\u4f53\u68c0\u7d22\u7cfb\u7edf\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.22716", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22716", "abs": "https://arxiv.org/abs/2602.22716", "authors": ["Guanting Ye", "Qiyan Zhao", "Wenhao Yu", "Liangyu Yuan", "Mingkai Li", "Xiaofeng Zhang", "Jianmin Ji", "Yanyong Zhang", "Qing Jiang", "Ka-Veng Yuen"], "title": "SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs", "comment": "CVPR 2026", "summary": "3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSoPE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7403\u5750\u6807\u4f4d\u7f6e\u5d4c\u5165\u6539\u8fdb3D\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u89e3\u51b3\u4f20\u7edfRoPE\u57283D\u7a7a\u95f4\u5efa\u6a21\u4e2d\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u76843D\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ee7\u627f\u4e86\u4f20\u7edfRoPE\u4f4d\u7f6e\u7f16\u7801\u673a\u5236\uff0c\u4f46\u8fd9\u79cd\u4f4d\u7f6e\u4f9d\u8d56\u5efa\u6a21\u65b9\u5f0f\u57283D\u591a\u6a21\u6001\u7406\u89e3\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002\u4f20\u7edfRoPE\u5728\u7f16\u78013D token\u65f6\u65e0\u6cd5\u4fdd\u6301\u4e09\u7ef4\u7a7a\u95f4\u7ed3\u6784\uff0c\u5176\u76f8\u5bf9\u8ddd\u79bb\u8ba1\u7b97\u5ffd\u7565\u4e86\u89d2\u5ea6\u4f9d\u8d56\uff0c\u9650\u5236\u4e86\u6a21\u578b\u6355\u6349\u89c6\u89c9\u8868\u793a\u4e2d\u65b9\u5411\u53d8\u5316\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7403\u5750\u6807\u4f4d\u7f6e\u5d4c\u5165(SoPE)\u65b9\u6cd5\uff0c\u5c06\u70b9\u4e91token\u7d22\u5f15\u6620\u5c04\u52303D\u7403\u5750\u6807\u7a7a\u95f4\uff0c\u5b9e\u73b0\u7a7a\u95f4\u4f4d\u7f6e\u548c\u65b9\u5411\u89d2\u7684\u7edf\u4e00\u5efa\u6a21\u3002\u540c\u65f6\u5f15\u5165\u591a\u5c3a\u5ea6\u9891\u7387\u6df7\u5408\u7b56\u7565\uff0c\u878d\u5408\u4e0d\u540c\u9891\u7387\u57df\u7684\u7279\u5f81\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a3D\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u5176\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SoPE\u65b9\u6cd5\u80fd\u591f\u4fdd\u6301\u70b9\u4e91\u6570\u636e\u7684\u56fa\u6709\u51e0\u4f55\u7ed3\u6784\uff0c\u589e\u5f3a\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u66f4\u4e00\u81f4\u548c\u66f4\u5177\u8868\u73b0\u529b\u7684\u51e0\u4f55\u8868\u793a\u3002"}}
{"id": "2602.22717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22717", "abs": "https://arxiv.org/abs/2602.22717", "authors": ["Shuoqi Chen", "Yujia Wu", "Geoffrey P. Luke"], "title": "IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling", "comment": "12 pages main text + 6 pages appendix, 7 figures main + 3 figures appendix, 3 tables main + 1 table appendix. Preprint", "summary": "Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8d85\u58f0\u56fe\u50cf\u53bb\u6591\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u540c\u65f6\u6709\u6548\u6291\u5236\u6591\u70b9\u566a\u58f0\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u5b66\u4e60\u57fa\u7ebf\u3002", "motivation": "\u8d85\u58f0\u6210\u50cf\u867d\u7136\u5b9e\u65f6\u65e0\u521b\uff0c\u4f46\u6591\u70b9\u566a\u58f0\u548c\u76f8\u5173\u4f2a\u5f71\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\uff0c\u5f71\u54cd\u8bca\u65ad\u89e3\u8bfb\u3002\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u53bb\u6591\u65b9\u6cd5\u6765\u63d0\u9ad8\u8d85\u58f0\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u57fa\u4e8e\u56fe\u50cf\u6062\u590d\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\u6784\u5efa\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528Matlab\u8d85\u58f0\u5de5\u5177\u7bb1\u4ece\u65e0\u6591\u70b9\u7684\u78c1\u5171\u632f\u56fe\u50cf\u6a21\u62df\u8d85\u58f0\u56fe\u50cf\u521b\u5efa\u5927\u89c4\u6a21\u914d\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u5728\u6a21\u62df\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6291\u5236\u6591\u70b9\u566a\u58f0\u7684\u540c\u65f6\u4fdd\u7559\u4e86\u6709\u610f\u4e49\u7684\u89e3\u5256\u8fb9\u7f18\u548c\u5bf9\u6bd4\u5ea6\uff0c\u4f18\u4e8e\u7ecf\u5178\u6ee4\u6ce2\u5668\u548c\u8fd1\u671f\u5b66\u4e60\u57fa\u7ebf\u3002\u901a\u8fc7\u4ea4\u53c9\u6a21\u578b\u65b9\u5dee\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u53d1\u73b0\u9ad8\u4e0d\u786e\u5b9a\u6027\u533a\u57df\u4e0e\u9ad8\u91cd\u5efa\u8bef\u5dee\u76f8\u5173\u3002", "conclusion": "\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u80fd\u6709\u6548\u53bb\u6591\u5e76\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\uff0c\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u53ef\u4f5c\u4e3a\u56f0\u96be\u533a\u57df\u7684\u5b9e\u7528\u6307\u6807\u3002\u89c2\u5bdf\u5230\u5bf9\u6a21\u62df\u63a2\u5934\u8bbe\u7f6e\u7684\u654f\u611f\u6027\u5bfc\u81f4\u7684\u57df\u504f\u79fb\uff0c\u8868\u660e\u9700\u8981\u591a\u6837\u5316\u8bad\u7ec3\u548c\u9002\u5e94\u4ee5\u5b9e\u73b0\u7a33\u5065\u7684\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2602.22727", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22727", "abs": "https://arxiv.org/abs/2602.22727", "authors": ["Yangguang Lin", "Quan Fang", "Yufei Li", "Jiachen Sun", "Junyu Gao", "Jitao Sang"], "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models", "comment": "accepted at CVPR 2026", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.", "AI": {"tldr": "HulluEdit\uff1a\u4e00\u79cd\u5355\u6b21\u524d\u5411\u3001\u65e0\u9700\u53c2\u8003\u6a21\u578b\u7684\u5e72\u9884\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7f16\u8f91\u9009\u62e9\u6027\u6291\u5236\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u3002", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u4e25\u91cd\u963b\u788d\u4e86\u5176\u53ef\u9760\u90e8\u7f72\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff1a\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u53c2\u8003\u6a21\u578b\u548c\u591a\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u8981\u4e48\u91c7\u7528\u9759\u6001\u7f16\u8f91\u53ef\u80fd\u6291\u5236\u771f\u5b9e\u7684\u89c6\u89c9\u8bc1\u636e\u3002", "method": "\u63d0\u51fa\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7f16\u8f91\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u7684\u9690\u85cf\u72b6\u6001\u5206\u89e3\u4e3a\u4e09\u4e2a\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff1a\u89c6\u89c9\u8bc1\u636e\u3001\u51b2\u7a81\u5148\u9a8c\u548c\u6b8b\u5dee\u4e0d\u786e\u5b9a\u6027\uff0c\u4ece\u800c\u80fd\u591f\u9009\u62e9\u6027\u6291\u5236\u5e7b\u89c9\u6a21\u5f0f\u800c\u4e0d\u5e72\u6270\u89c6\u89c9\u57fa\u7840\u3002\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u4e0a\u4fdd\u8bc1\u5bf9\u5148\u9a8c\u5b50\u7a7a\u95f4\u7684\u7f16\u8f91\u5b8c\u5168\u4e0d\u5f71\u54cd\u89c6\u89c9\u6210\u5206\u3002", "result": "\u5728POPE\u548cCHAIR\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e7b\u89c9\u51cf\u5c11\u6548\u679c\uff0c\u540c\u65f6\u5728MME\u4e0a\u4fdd\u6301\u901a\u7528\u80fd\u529b\uff0c\u63a8\u7406\u6548\u7387\u9ad8\u3002\u5728\u5404\u79cd\u67b6\u6784\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5bf9\u6bd4\u89e3\u7801\u548c\u9759\u6001\u5b50\u7a7a\u95f4\u7f16\u8f91\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HulluEdit\u4e3a\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u4fe1\u8d56\u8def\u5f84\uff0c\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7f16\u8f91\u6709\u6548\u51cf\u5c11\u7269\u4f53\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.22740", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22740", "abs": "https://arxiv.org/abs/2602.22740", "authors": ["Tongfei Chen", "Shuo Yang", "Yuguang Yang", "Linlin Yang", "Runtang Guo", "Changbai Li", "He Long", "Chunyu Xie", "Dawei Leng", "Baochang Zhang"], "title": "AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation", "comment": "ICLR 2026 conference paper", "summary": "Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues. This approach results in state-of-the-art performance on RefCOCO datasets and also enhances robustness to diverse descriptions and scenarios", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAlignment-Aware Masked Learning (AML)\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u663e\u5f0f\u4f30\u8ba1\u50cf\u7d20\u7ea7\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u3001\u8fc7\u6ee4\u5bf9\u9f50\u4e0d\u4f73\u533a\u57df\u3001\u805a\u7126\u53ef\u4fe1\u7ebf\u7d22\u6765\u63d0\u5347Referring Image Segmentation\u6027\u80fd\uff0c\u5728RefCOCO\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u591a\u6837\u5316\u63cf\u8ff0\u548c\u573a\u666f\u7684\u9c81\u68d2\u6027\u3002", "motivation": "Referring Image Segmentation (RIS)\u4efb\u52a1\u9700\u8981\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5206\u5272\u56fe\u50cf\u4e2d\u7684\u76ee\u6807\u5bf9\u8c61\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u50cf\u7d20\u7ea7\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5bfc\u81f4\u5bf9\u591a\u6837\u5316\u63cf\u8ff0\u548c\u590d\u6742\u573a\u666f\u7684\u9c81\u68d2\u6027\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u80fd\u663e\u5f0f\u5efa\u6a21\u5bf9\u9f50\u5173\u7cfb\u3001\u8fc7\u6ee4\u4e0d\u53ef\u9760\u533a\u57df\u3001\u805a\u7126\u53ef\u4fe1\u7ebf\u7d22\u7684\u8bad\u7ec3\u7b56\u7565\u3002", "method": "\u63d0\u51faAlignment-Aware Masked Learning (AML)\u8bad\u7ec3\u7b56\u7565\uff1a1) \u663e\u5f0f\u4f30\u8ba1\u50cf\u7d20\u7ea7\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u5206\u6570\uff1b2) \u57fa\u4e8e\u5bf9\u9f50\u5206\u6570\u8fc7\u6ee4\u5bf9\u9f50\u4e0d\u4f73\u7684\u533a\u57df\uff0c\u907f\u514d\u8fd9\u4e9b\u533a\u57df\u5bf9\u4f18\u5316\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\uff1b3) \u805a\u7126\u4e8e\u5bf9\u9f50\u826f\u597d\u7684\u53ef\u4fe1\u7ebf\u7d22\u8fdb\u884c\u8bad\u7ec3\u4f18\u5316\u3002", "result": "\u5728RefCOCO\u7cfb\u5217\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86state-of-the-art\u6027\u80fd\u3002\u540c\u65f6\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u589e\u5f3a\u4e86\u5bf9\u591a\u6837\u5316\u8bed\u8a00\u63cf\u8ff0\u548c\u590d\u6742\u573a\u666f\u7684\u9c81\u68d2\u6027\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AML\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u50cf\u7d20\u7ea7\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u3001\u8fc7\u6ee4\u4e0d\u53ef\u9760\u533a\u57df\u3001\u805a\u7126\u53ef\u4fe1\u7ebf\u7d22\uff0c\u6709\u6548\u63d0\u5347\u4e86RIS\u4efb\u52a1\u7684\u6027\u80fd\u548c\u5bf9\u591a\u6837\u5316\u63cf\u8ff0\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u601d\u8def\u3002"}}
{"id": "2602.22742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22742", "abs": "https://arxiv.org/abs/2602.22742", "authors": ["Akihisa Watanabe", "Qing Yu", "Edgar Simo-Serra", "Kent Fujiwara"], "title": "ProjFlow: Projection Sampling with Flow Matching for Zero-Shot Exact Spatial Motion Control", "comment": null, "summary": "Generating human motion with precise spatial control is a challenging problem. Existing approaches often require task-specific training or slow optimization, and enforcing hard constraints frequently disrupts motion naturalness. Building on the observation that many animation tasks can be formulated as a linear inverse problem, we introduce ProjFlow, a training-free sampler that achieves zero-shot, exact satisfaction of linear spatial constraints while preserving motion realism. Our key advance is a novel kinematics-aware metric that encodes skeletal topology. This metric allows the sampler to enforce hard constraints by distributing corrections coherently across the entire skeleton, avoiding the unnatural artifacts of naive projection. Furthermore, for sparse inputs, such as filling in long gaps between a few keyframes, we introduce a time-varying formulation using pseudo-observations that fade during sampling. Extensive experiments on representative applications, motion inpainting, and 2D-to-3D lifting, demonstrate that ProjFlow achieves exact constraint satisfaction and matches or improves realism over zero-shot baselines, while remaining competitive with training-based controllers.", "AI": {"tldr": "ProjFlow\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u96f6\u6837\u672c\u7684\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u7ebf\u6027\u9006\u95ee\u9898\u65b9\u6cd5\u5b9e\u73b0\u7cbe\u786e\u7684\u7a7a\u95f4\u7ea6\u675f\u6ee1\u8db3\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u81ea\u7136\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u4f18\u5316\u6162\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u5177\u6709\u7cbe\u786e\u7a7a\u95f4\u63a7\u5236\u7684\u4eba\u7c7b\u8fd0\u52a8\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u901a\u5e38\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u7f13\u6162\u4f18\u5316\uff0c\u4e14\u786c\u7ea6\u675f\u7ecf\u5e38\u7834\u574f\u8fd0\u52a8\u7684\u81ea\u7136\u6027\u3002\u8bb8\u591a\u52a8\u753b\u4efb\u52a1\u53ef\u4ee5\u8868\u8ff0\u4e3a\u7ebf\u6027\u9006\u95ee\u9898\uff0c\u8fd9\u4e3a\u5f00\u53d1\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "ProjFlow\u57fa\u4e8e\u7ebf\u6027\u9006\u95ee\u9898\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9aa8\u9abc\u611f\u77e5\u5ea6\u91cf\u6765\u7f16\u7801\u9aa8\u9abc\u62d3\u6251\u7ed3\u6784\u3002\u8be5\u5ea6\u91cf\u4f7f\u91c7\u6837\u5668\u80fd\u591f\u901a\u8fc7\u5728\u6574\u4e2a\u9aa8\u9abc\u4e0a\u534f\u8c03\u5206\u5e03\u4fee\u6b63\u6765\u5f3a\u5236\u6267\u884c\u786c\u7ea6\u675f\uff0c\u907f\u514d\u6734\u7d20\u6295\u5f71\u65b9\u6cd5\u4ea7\u751f\u7684\u4e0d\u81ea\u7136\u4f2a\u5f71\u3002\u5bf9\u4e8e\u7a00\u758f\u8f93\u5165\uff08\u5982\u586b\u5145\u5173\u952e\u5e27\u4e4b\u95f4\u7684\u957f\u95f4\u9699\uff09\uff0c\u8fd8\u5f15\u5165\u4e86\u4f7f\u7528\u4f2a\u89c2\u6d4b\u7684\u65f6\u95f4\u53d8\u5316\u516c\u5f0f\uff0c\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u9010\u6e10\u6de1\u5316\u3002", "result": "\u5728\u4ee3\u8868\u6027\u5e94\u7528\uff08\u8fd0\u52a8\u4fee\u590d\u548c2D\u52303D\u63d0\u5347\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cProjFlow\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7ea6\u675f\u6ee1\u8db3\uff0c\u5728\u96f6\u6837\u672c\u57fa\u7ebf\u4e0a\u5339\u914d\u6216\u63d0\u9ad8\u4e86\u771f\u5b9e\u611f\uff0c\u540c\u65f6\u4e0e\u57fa\u4e8e\u8bad\u7ec3\u7684\u63a7\u5236\u5668\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "ProjFlow\u901a\u8fc7\u7ebf\u6027\u9006\u95ee\u9898\u65b9\u6cd5\u548c\u9aa8\u9abc\u611f\u77e5\u5ea6\u91cf\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u3001\u96f6\u6837\u672c\u7684\u7cbe\u786e\u7a7a\u95f4\u7ea6\u675f\u6ee1\u8db3\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u81ea\u7136\u6027\uff0c\u4e3a\u4eba\u7c7b\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22759", "abs": "https://arxiv.org/abs/2602.22759", "authors": ["Yuan-Chih Chen", "Chun-Shien Lu"], "title": "Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval", "comment": null, "summary": "Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u9690\u85cf\u7801\u6062\u590d\u6846\u67b6\uff0c\u5b9e\u73b0\u7be1\u6539\u56fe\u50cf\u7684\u68c0\u7d22\u4e0e\u6062\u590d\uff0c\u517c\u5bb9\u591a\u79cd\u6c34\u5370\u65b9\u6848\uff0c\u5e76\u5728ImageNet-S\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u6027\u80fd", "motivation": "\u5f53\u524d\u56fe\u50cf\u771f\u5b9e\u6027\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u800c\u7be1\u6539\u5185\u5bb9\u7684\u6062\u590d\u548c\u4e8b\u5b9e\u68c0\u7d22\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u7684\u6062\u590d\u6846\u67b6", "method": "\u901a\u8fc7\u591a\u5c3a\u5ea6\u5411\u91cf\u91cf\u5316\u5c06\u8bed\u4e49\u548c\u611f\u77e5\u4fe1\u606f\u7f16\u7801\u4e3a\u7d27\u51d1\u9690\u85cf\u7801\u8868\u793a\uff0c\u5229\u7528\u6761\u4ef6Transformer\u6a21\u5757\u589e\u5f3a\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u652f\u6301\u4e8b\u540e\u548c\u751f\u6210\u65f6\u6c34\u5370\u8303\u5f0f", "result": "\u5728ImageNet-S\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u68c0\u7d22\u548c\u91cd\u5efa\u6027\u80fd\uff0c\u5b8c\u5168\u517c\u5bb9\u591a\u79cd\u6c34\u5370\u6d41\u7a0b\uff0c\u4e3a\u901a\u7528\u56fe\u50cf\u6062\u590d\u5960\u5b9a\u57fa\u7840", "conclusion": "\u8be5\u6846\u67b6\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4e3a\u901a\u7528\u56fe\u50cf\u6062\u590d\u5efa\u7acb\u4e86\u57fa\u7840\uff0c\u652f\u6301\u7be1\u6539\u5185\u5bb9\u7684\u68c0\u7d22\u548c\u6062\u590d"}}
{"id": "2602.22779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22779", "abs": "https://arxiv.org/abs/2602.22779", "authors": ["Chenhao Zheng", "Jieyu Zhang", "Jianing Zhang", "Weikai Huang", "Ashutosh Kumar", "Quan Kong", "Oncel Tuzel", "Chun-Liang Li", "Ranjay Krishna"], "title": "TrajTok: Learning Trajectory Tokens enables better Video Understanding", "comment": "CVPR 2026", "summary": "Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.", "AI": {"tldr": "TrajTok\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u89c6\u9891\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u65f6\u7a7a\u50cf\u7d20\u805a\u7c7b\u76f4\u63a5\u751f\u6210\u7269\u4f53\u8f68\u8ff9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u89c6\u9891\u6a21\u578b\u5206\u8bcd\u5197\u4f59\u95ee\u9898\uff0c\u53ef\u52a8\u6001\u9002\u5e94\u8bed\u4e49\u590d\u6742\u5ea6\uff0c\u63d0\u9ad8\u89c6\u9891\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u6a21\u578b\u901a\u8fc7\u5206\u5757\u5316\u8fdb\u884c\u5206\u8bcd\u4f1a\u4ea7\u751f\u5927\u91cf\u5197\u4f59token\uff0c\u4e25\u91cd\u9650\u5236\u89c6\u9891\u5904\u7406\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u7684\u8f68\u8ff9\u5206\u8bcd\u5668\u867d\u7136\u80fd\u89e3\u8026\u89c6\u9891\u65f6\u957f\u4e0etoken\u6570\u91cf\uff0c\u4f46\u4f9d\u8d56\u590d\u6742\u7684\u5916\u90e8\u5206\u5272\u548c\u8ddf\u8e2a\u6d41\u7a0b\uff0c\u901f\u5ea6\u6162\u4e14\u4e0e\u4efb\u52a1\u65e0\u5173\u3002", "method": "\u63d0\u51faTrajTok\u7aef\u5230\u7aef\u89c6\u9891\u5206\u8bcd\u5668\u6a21\u5757\uff0c\u5b8c\u5168\u96c6\u6210\u5e76\u4e0e\u89c6\u9891\u6a21\u578b\u534f\u540c\u8bad\u7ec3\uff0c\u5305\u542b\u7edf\u4e00\u7684segmenter\uff0c\u5728\u65f6\u7a7a\u7ef4\u5ea6\u5bf9\u50cf\u7d20\u8fdb\u884c\u9690\u5f0f\u805a\u7c7b\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u751f\u6210\u7269\u4f53\u8f68\u8ff9\uff0c\u52a8\u6001\u8c03\u6574token\u7c92\u5ea6\u4ee5\u9002\u5e94\u8bed\u4e49\u590d\u6742\u5ea6\u3002", "result": "\u57fa\u4e8eTrajTok\u5b9e\u73b0\u7684TrajViT2\u89c6\u9891CLIP\u6a21\u578b\u5728\u5206\u7c7b\u548c\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u4f73token\u5408\u5e76\u65b9\u6cd5\u76f8\u5f53\u7684\u6548\u7387\u3002TrajTok\u8fd8\u53ef\u4f5c\u4e3a\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u7684\u63a2\u6d4b\u5934\uff08TrajAdapter\uff09\u6216\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u8fde\u63a5\u5668\uff08TrajVLM\uff09\uff0c\u5728\u957f\u89c6\u9891\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TrajTok\u901a\u8fc7\u7aef\u5230\u7aef\u5b66\u4e60\u751f\u6210\u8bed\u4e49\u611f\u77e5\u7684\u8f68\u8ff9token\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u5206\u8bcd\u5197\u4f59\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u89c6\u9891\u7406\u89e3\u6027\u80fd\u548c\u6548\u7387\uff0c\u540c\u65f6\u5c55\u73b0\u51fa\u4f5c\u4e3a\u591a\u529f\u80fd\u7ec4\u4ef6\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.22785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22785", "abs": "https://arxiv.org/abs/2602.22785", "authors": ["Ling Wang", "Hao-Xiang Guo", "Xinzhou Wang", "Fuchun Sun", "Kai Sun", "Pengkun Liu", "Hang Xiao", "Zhong Wang", "Guangyuan Fu", "Eric Li", "Yang Liu", "Yikai Wang"], "title": "SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation", "comment": "published at iclr 2026", "summary": "We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.", "AI": {"tldr": "SceneTransporter\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u7ed3\u6784\u53163D\u573a\u666f\u3002\u5b83\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7ea6\u675f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u65e0\u6cd5\u5c06\u90e8\u4ef6\u7ea73D\u5bf9\u8c61\u7ec4\u7ec7\u6210\u4e0d\u540c\u5b9e\u4f8b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u80fd\u751f\u6210\u90e8\u4ef6\u7ea73D\u5bf9\u8c61\uff0c\u4f46\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u5f80\u5f80\u65e0\u6cd5\u5c06\u8fd9\u4e9b\u90e8\u4ef6\u7ec4\u7ec7\u6210\u4e0d\u540c\u7684\u5b9e\u4f8b\u3002\u901a\u8fc7\u53bb\u504f\u805a\u7c7b\u63a2\u9488\u5206\u6790\u53d1\u73b0\uff0c\u8fd9\u79cd\u5931\u8d25\u6e90\u4e8e\u6a21\u578b\u5185\u90e8\u5206\u914d\u673a\u5236\u7f3a\u4e4f\u7ed3\u6784\u7ea6\u675f\u3002", "method": "\u5c06\u7ed3\u6784\u53163D\u573a\u666f\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5168\u5c40\u76f8\u5173\u5206\u914d\u95ee\u9898\u3002\u5728\u7ec4\u5408DiT\u6a21\u578b\u7684\u53bb\u566a\u5faa\u73af\u4e2d\uff0c\u5236\u5b9a\u5e76\u6c42\u89e3\u71b5\u6700\u4f18\u4f20\u8f93\u76ee\u6807\u3002\u8be5\u516c\u5f0f\u65bd\u52a0\u4e24\u79cd\u7ed3\u6784\u7ea6\u675f\uff1a1\uff09\u4f20\u8f93\u8ba1\u5212\u901a\u8fc7\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u5f3a\u5236\u56fe\u50cf\u5757\u5230\u90e8\u4ef6\u7ea73D\u6f5c\u5728\u8868\u793a\u7684\u4e00\u5bf9\u4e00\u72ec\u5360\u8def\u7531\uff1b2\uff09\u4f20\u8f93\u7684\u7ade\u4e89\u6027\u9f13\u52b1\u76f8\u4f3c\u5757\u7684\u5206\u7ec4\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u8fb9\u7f18\u7684\u6210\u672c\u8fdb\u884c\u6b63\u5219\u5316\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSceneTransporter\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9e\u4f8b\u7ea7\u8fde\u8d2f\u6027\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "SceneTransporter\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7ea6\u675f\u6210\u529f\u89e3\u51b3\u4e86\u7ed3\u6784\u53163D\u573a\u666f\u751f\u6210\u4e2d\u7684\u5b9e\u4f8b\u7ec4\u7ec7\u95ee\u9898\uff0c\u4e3a\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf3D\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22791", "abs": "https://arxiv.org/abs/2602.22791", "authors": ["Taishu Arashima", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Robust Human Trajectory Prediction via Self-Supervised Skeleton Representation Learning", "comment": "11 pages main, 5 pages supplementary material", "summary": "Human trajectory prediction plays a crucial role in applications such as autonomous navigation and video surveillance. While recent works have explored the integration of human skeleton sequences to complement trajectory information, skeleton data in real-world environments often suffer from missing joints caused by occlusions. These disturbances significantly degrade prediction accuracy, indicating the need for more robust skeleton representations. We propose a robust trajectory prediction method that incorporates a self-supervised skeleton representation model pretrained with masked autoencoding. Experimental results in occlusion-prone scenarios show that our method improves robustness to missing skeletal data without sacrificing prediction accuracy, and consistently outperforms baseline models in clean-to-moderate missingness regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u9aa8\u67b6\u8868\u793a\u6a21\u578b\u7684\u9c81\u68d2\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u81ea\u7f16\u7801\u9884\u8bad\u7ec3\u5904\u7406\u906e\u6321\u5bfc\u81f4\u7684\u5173\u8282\u7f3a\u5931\u95ee\u9898\uff0c\u5728\u906e\u6321\u573a\u666f\u4e0b\u63d0\u5347\u9884\u6d4b\u9c81\u68d2\u6027", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u4eba\u4f53\u9aa8\u67b6\u6570\u636e\u5e38\u56e0\u906e\u6321\u5bfc\u81f4\u5173\u8282\u7f3a\u5931\uff0c\u8fd9\u4e9b\u5e72\u6270\u663e\u8457\u964d\u4f4e\u8f68\u8ff9\u9884\u6d4b\u7cbe\u5ea6\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u9aa8\u67b6\u8868\u793a\u65b9\u6cd5", "method": "\u63d0\u51fa\u9c81\u68d2\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u6574\u5408\u57fa\u4e8e\u63a9\u7801\u81ea\u7f16\u7801\u9884\u8bad\u7ec3\u7684\u81ea\u76d1\u7763\u9aa8\u67b6\u8868\u793a\u6a21\u578b\uff0c\u589e\u5f3a\u5bf9\u7f3a\u5931\u9aa8\u67b6\u6570\u636e\u7684\u5904\u7406\u80fd\u529b", "result": "\u5728\u906e\u6321\u573a\u666f\u4e0b\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u727a\u7272\u9884\u6d4b\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u4e86\u5bf9\u7f3a\u5931\u9aa8\u67b6\u6570\u636e\u7684\u9c81\u68d2\u6027\uff0c\u5728\u6e05\u6d01\u5230\u4e2d\u5ea6\u7f3a\u5931\u60c5\u51b5\u4e0b\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u901a\u8fc7\u81ea\u76d1\u7763\u9aa8\u67b6\u8868\u793a\u6a21\u578b\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u73b0\u5b9e\u73af\u5883\u4e2d\u56e0\u906e\u6321\u5bfc\u81f4\u7684\u9aa8\u67b6\u6570\u636e\u7f3a\u5931\u95ee\u9898\uff0c\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027"}}
{"id": "2602.22800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22800", "abs": "https://arxiv.org/abs/2602.22800", "authors": ["Hanliang Du", "Zhangji Lu", "Zewei Cai", "Qijian Tang", "Qifeng Yu", "Xiaoli Liu"], "title": "GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation", "comment": null, "summary": "Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.", "AI": {"tldr": "\u63d0\u51faGSTurb\u6846\u67b6\uff0c\u7ed3\u5408\u5149\u6d41\u5f15\u5bfc\u7684\u503e\u659c\u6821\u6b63\u548c\u9ad8\u65af\u6cfc\u6e85\u5efa\u6a21\u975e\u7b49\u6655\u6a21\u7cca\uff0c\u6709\u6548\u7f13\u89e3\u5927\u6c14\u6e4d\u6d41\u5f15\u8d77\u7684\u56fe\u50cf\u9000\u5316", "motivation": "\u5927\u6c14\u6e4d\u6d41\u5bfc\u81f4\u957f\u8ddd\u79bb\u6210\u50cf\u4e2d\u7684\u50cf\u7d20\u4f4d\u79fb\uff08\u503e\u659c\uff09\u548c\u6a21\u7cca\uff0c\u4e25\u91cd\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u9700\u8981\u6709\u6548\u6062\u590d\u65b9\u6cd5", "method": "\u63d0\u51faGSTurb\u6846\u67b6\uff0c\u4f7f\u7528\u9ad8\u65af\u53c2\u6570\u8868\u793a\u503e\u659c\u548c\u6a21\u7cca\uff0c\u901a\u8fc7\u5149\u6d41\u5f15\u5bfc\u503e\u659c\u6821\u6b63\u548c\u9ad8\u65af\u6cfc\u6e85\u5efa\u6a21\u975e\u7b49\u6655\u6a21\u7cca\uff0c\u5728\u591a\u5e27\u4e0a\u4f18\u5316\u53c2\u6570", "result": "\u5728ATSyn-static\u6570\u636e\u96c6\u4e0a\u8fbe\u5230PSNR 27.67 dB\u548cSSIM 0.8735\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5\u63d0\u5347PSNR 1.3 dB\uff084.5%\uff09\u548cSSIM 0.048\uff085.8%\uff09\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02", "conclusion": "\u5149\u6d41\u5f15\u5bfc\u7684\u503e\u659c\u6821\u6b63\u4e0e\u9ad8\u65af\u6cfc\u6e85\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u5408\u6210\u548c\u771f\u5b9e\u6e4d\u6d41\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u6062\u590d\u6548\u679c"}}
{"id": "2602.22809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22809", "abs": "https://arxiv.org/abs/2602.22809", "authors": ["Mingde Yao", "Zhiyuan You", "Tam-King Man", "Menglu Wang", "Tianfan Xue"], "title": "PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning", "comment": "A fully automated, intelligent photo-editing agent that autonomously plans multi-step aesthetic enhancements, smartly chooses diverse editing tools, and enables everyday users to achieve professional-looking results without crafting complex prompts. Project page: https://github.com/mdyao/PhotoAgent", "summary": "With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.", "AI": {"tldr": "PhotoAgent\u662f\u4e00\u4e2a\u81ea\u4e3b\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf\uff0c\u901a\u8fc7\u7f8e\u5b66\u89c4\u5212\u5c06\u56fe\u50cf\u7f16\u8f91\u5efa\u6a21\u4e3a\u957f\u671f\u51b3\u7b56\u95ee\u9898\uff0c\u65e0\u9700\u7528\u6237\u9010\u6b65\u6307\u4ee4\u5373\u53ef\u81ea\u52a8\u6267\u884c\u591a\u6b65\u7f16\u8f91\u64cd\u4f5c", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u8d28\u91cf\u9ad8\u5ea6\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u548c\u987a\u5e8f\u89c4\u5212\u7684\u8d1f\u62c5\u5b8c\u5168\u653e\u5728\u7528\u6237\u8eab\u4e0a\uff0c\u9700\u8981\u5b9e\u73b0\u66f4\u81ea\u4e3b\u7684\u56fe\u50cf\u7f16\u8f91\u7cfb\u7edf", "method": "\u5c06\u81ea\u4e3b\u56fe\u50cf\u7f16\u8f91\u5efa\u6a21\u4e3a\u957f\u671f\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u7406\u89e3\u7528\u6237\u7f8e\u5b66\u610f\u56fe\u3001\u57fa\u4e8e\u6811\u641c\u7d22\u89c4\u5212\u591a\u6b65\u7f16\u8f91\u52a8\u4f5c\u3001\u5229\u7528\u8bb0\u5fc6\u548c\u89c6\u89c9\u53cd\u9988\u8fdb\u884c\u95ed\u73af\u8fed\u4ee3\u6267\u884c", "result": "\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0cPhotoAgent\u5728\u6307\u4ee4\u9075\u5faa\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5f15\u5165\u4e86\u5305\u542b7,000\u5f20\u7167\u7247\u7684\u7f8e\u5b66\u8bc4\u4f30\u57fa\u51c6UGC-Edit\u548c\u5305\u542b1,017\u5f20\u7167\u7247\u7684\u6d4b\u8bd5\u96c6", "conclusion": "PhotoAgent\u901a\u8fc7\u660e\u786e\u7684\u7f8e\u5b66\u89c4\u5212\u548c\u957f\u671f\u51b3\u7b56\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u81ea\u4e3b\u56fe\u50cf\u7f16\u8f91\uff0c\u65e0\u9700\u7528\u6237\u9010\u6b65\u6307\u4ee4\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272"}}
{"id": "2602.22819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22819", "abs": "https://arxiv.org/abs/2602.22819", "authors": ["Purbayan Kar", "Ayush Ghadiya", "Vishal Chudasama", "Pankaj Wasnik", "C. V. Jawahar"], "title": "Face Time Traveller : Travel Through Ages Without Losing Identity", "comment": "Accepted at CVPR 2026 (Findings Track)", "summary": "Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.", "AI": {"tldr": "FaceTT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9762\u90e8\u5c5e\u6027\u611f\u77e5\u63d0\u793a\u7ec6\u5316\u3001\u514d\u8c03\u8c10\u89d2\u5ea6\u53cd\u6f14\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u63a7\u5236\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u3001\u8eab\u4efd\u4e00\u81f4\u7684\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u3002", "motivation": "\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u662f\u4e00\u4e2a\u75c5\u6001\u95ee\u9898\uff0c\u53d7\u73af\u5883\u548c\u9057\u4f20\u56e0\u7d20\u5f71\u54cd\uff0c\u5728\u5a31\u4e50\u3001\u6cd5\u533b\u5b66\u548c\u6570\u5b57\u6863\u6848\u7b49\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6570\u503c\u5e74\u9f84\u8868\u793a\uff0c\u5ffd\u89c6\u4e86\u751f\u7269\u548c\u4e0a\u4e0b\u6587\u7ebf\u7d22\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u5bbd\u5e74\u9f84\u53d8\u6362\u4e2d\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4e14\u6269\u6563\u6a21\u578b\u7684\u9759\u6001\u6ce8\u610f\u529b\u548c\u4f18\u5316\u7e41\u91cd\u7684\u53cd\u6f14\u9650\u5236\u4e86\u9002\u5e94\u6027\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u80cc\u666f\u4e00\u81f4\u6027\u3002", "method": "1. \u9762\u90e8\u5c5e\u6027\u611f\u77e5\u63d0\u793a\u7ec6\u5316\u7b56\u7565\uff1a\u7f16\u7801\u5185\u5728\uff08\u751f\u7269\uff09\u548c\u5916\u5728\uff08\u73af\u5883\uff09\u8001\u5316\u7ebf\u7d22\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u6761\u4ef6\u5316\uff1b2. \u514d\u8c03\u8c10\u89d2\u5ea6\u53cd\u6f14\u65b9\u6cd5\uff1a\u9ad8\u6548\u5c06\u771f\u5b9e\u9762\u90e8\u6620\u5c04\u5230\u6269\u6563\u6f5c\u5728\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u91cd\u5efa\uff1b3. \u81ea\u9002\u5e94\u6ce8\u610f\u529b\u63a7\u5236\u673a\u5236\uff1a\u52a8\u6001\u5e73\u8861\u7528\u4e8e\u8bed\u4e49\u8001\u5316\u7ebf\u7d22\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u548c\u7528\u4e8e\u7ed3\u6784\u53ca\u8eab\u4efd\u4fdd\u6301\u7684\u81ea\u6ce8\u610f\u529b\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u91ce\u5916\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFaceTT\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u80cc\u666f\u4fdd\u7559\u548c\u8001\u5316\u771f\u5b9e\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "FaceTT\u901a\u8fc7\u521b\u65b0\u7684\u63d0\u793a\u7ec6\u5316\u3001\u53cd\u6f14\u548c\u6ce8\u610f\u529b\u63a7\u5236\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u65b9\u6cd5\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u80cc\u666f\u4e00\u81f4\u6027\u548c\u8001\u5316\u771f\u5b9e\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u3002"}}
{"id": "2602.22821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22821", "abs": "https://arxiv.org/abs/2602.22821", "authors": ["Tong Wang", "Yaolei Qi", "Siwen Wang", "Imran Razzak", "Guanyu Yang", "Yutong Xie"], "title": "CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation", "comment": null, "summary": "Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.", "AI": {"tldr": "CMSA-Net\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u9891\u606f\u8089\u5206\u5272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u591a\u5c3a\u5ea6\u805a\u5408\u6a21\u5757\u548c\u52a8\u6001\u591a\u6e90\u53c2\u8003\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u9891\u606f\u8089\u5206\u5272\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u606f\u8089\u4e0e\u5468\u56f4\u9ecf\u819c\u5916\u89c2\u76f8\u4f3c\uff0c\u8bed\u4e49\u533a\u5206\u5ea6\u5f31\uff1b2\uff09\u606f\u8089\u5728\u89c6\u9891\u5e27\u95f4\u4f4d\u7f6e\u548c\u5c3a\u5ea6\u53d8\u5316\u5927\uff0c\u5bfc\u81f4\u7a33\u5b9a\u51c6\u786e\u5206\u5272\u56f0\u96be\u3002", "method": "\u63d0\u51faCMSA-Net\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u56e0\u679c\u591a\u5c3a\u5ea6\u805a\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u56e0\u679c\u6ce8\u610f\u529b\u6309\u65f6\u95f4\u987a\u5e8f\u805a\u5408\u591a\u5c3a\u5ea6\u5386\u53f2\u5e27\u8bed\u4e49\u4fe1\u606f\uff1b2\uff09\u52a8\u6001\u591a\u6e90\u53c2\u8003\u7b56\u7565\uff0c\u57fa\u4e8e\u8bed\u4e49\u53ef\u5206\u6027\u548c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u4e14\u53ef\u9760\u7684\u53c2\u8003\u5e27\u3002", "result": "\u5728SUN-SEG\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCMSA-Net\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5206\u5272\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u4e34\u5e8a\u9002\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "CMSA-Net\u901a\u8fc7\u56e0\u679c\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u548c\u52a8\u6001\u53c2\u8003\u5e27\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u606f\u8089\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u6027\u548c\u65f6\u7a7a\u53d8\u5316\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22919", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22919", "abs": "https://arxiv.org/abs/2602.22919", "authors": ["Haofan Wu", "Nay Aung", "Theodoros N. Arvanitis", "Joao A. C. Lima", "Steffen E. Petersen", "Le Zhang"], "title": "Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins", "comment": "10 pages, 8 figures. Submitted to IEEE Transactions on Medical Imaging (TMI). Code will be released after review", "summary": "A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.", "AI": {"tldr": "Chain of Flow (COF) \u662f\u4e00\u4e2a\u4ece\u5355\u6b21\u5fc3\u52a8\u5468\u671f\u5fc3\u7535\u56fe\u91cd\u5efa\u5b8c\u65744D\u5fc3\u810f\u7ed3\u6784\u548c\u8fd0\u52a8\u7684\u57fa\u7840\u6027\u751f\u6210\u6846\u67b6\uff0c\u5c06\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u4ece\u72ed\u7a84\u7684\u9884\u6d4b\u6a21\u578b\u8f6c\u53d8\u4e3a\u5b8c\u5168\u751f\u6210\u5f0f\u7684\u60a3\u8005\u7279\u5f02\u6027\u865a\u62df\u5fc3\u810f\u3002", "motivation": "\u73b0\u6709\u7684\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u6846\u67b6\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u9884\u6d4b\u5668\uff0c\u800c\u4e0d\u662f\u6784\u5efa\u60a3\u8005\u7279\u5f02\u6027\u3001\u53ef\u64cd\u4f5c\u7684\u865a\u62df\u5fc3\u810f\u3002\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u9700\u8981\u91cd\u5efa\u4e2a\u4f53\u5316\u5fc3\u810f\u89e3\u5256\u548c\u751f\u7406\uff0c\u4ece\u591a\u6a21\u6001\u4fe1\u53f7\u66f4\u65b0\u5185\u90e8\u72b6\u6001\uff0c\u5e76\u652f\u6301\u5e7f\u6cdb\u7684\u4e0b\u6e38\u6a21\u62df\u4efb\u52a1\u3002", "method": "COF\u662f\u4e00\u4e2a\u57fa\u4e8e\u5fc3\u7535\u56fe\u9a71\u52a8\u7684\u751f\u6210\u6846\u67b6\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6574\u5408\u7535\u5f71\u78c1\u5171\u632f\u6210\u50cf\u548c12\u5bfc\u8054\u5fc3\u7535\u56fe\uff0c\u5b66\u4e60\u5fc3\u810f\u51e0\u4f55\u3001\u7535\u751f\u7406\u548c\u8fd0\u52a8\u52a8\u529b\u5b66\u7684\u7edf\u4e00\u8868\u793a\uff0c\u4ece\u5355\u4e2a\u5fc3\u52a8\u5468\u671f\u91cd\u5efa\u5b8c\u6574\u76844D\u5fc3\u810f\u7ed3\u6784\u548c\u8fd0\u52a8\u3002", "result": "\u5728\u591a\u6837\u5316\u961f\u5217\u4e0a\u8bc4\u4f30\u663e\u793a\uff0cCOF\u80fd\u591f\u51c6\u786e\u6062\u590d\u5fc3\u810f\u89e3\u5256\u7ed3\u6784\u3001\u5404\u5fc3\u8154\u529f\u80fd\u548c\u52a8\u6001\u8fd0\u52a8\u6a21\u5f0f\u3002\u91cd\u5efa\u76844D\u5fc3\u810f\u8fdb\u4e00\u6b65\u652f\u6301\u4e0b\u6e38\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u4efb\u52a1\uff0c\u5982\u5bb9\u91cf\u6d4b\u91cf\u3001\u533a\u57df\u529f\u80fd\u5206\u6790\u548c\u865a\u62df\u7535\u5f71\u5408\u6210\u3002", "conclusion": "\u901a\u8fc7\u76f4\u63a5\u4ece\u5fc3\u7535\u56fe\u5b9e\u73b0\u5b8c\u6574\u76844D\u5668\u5b98\u91cd\u5efa\uff0cCOF\u5c06\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u4ece\u72ed\u7a84\u7684\u9884\u6d4b\u6a21\u578b\u8f6c\u53d8\u4e3a\u5b8c\u5168\u751f\u6210\u5f0f\u7684\u60a3\u8005\u7279\u5f02\u6027\u865a\u62df\u5fc3\u810f\uff0c\u4e3a\u4e34\u5e8a\u53ef\u64cd\u4f5c\u7684\u5fc3\u810f\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2602.23088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23088", "abs": "https://arxiv.org/abs/2602.23088", "authors": ["Matthew Sutton", "Katrin Amunts", "Timo Dickscheid", "Christian Schiffer"], "title": "Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy", "comment": "8 pages, 3 figures, submitted for inclusion at a conference", "summary": "Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6807\u7b7e\u4ecb\u5bfc\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6807\u7b7e\u800c\u975e\u914d\u5bf9\u56fe\u50cf-\u6587\u672c\u6570\u636e\uff0c\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u8fde\u63a5\uff0c\u4e3a\u663e\u5fae\u56fe\u50cf\u751f\u6210\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0", "motivation": "\u5728\u8bb8\u591a\u7814\u7a76\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\uff0c\u914d\u5bf9\u7684\u56fe\u50cf-\u6587\u672c\u6570\u636e\u7a00\u7f3a\u4e14\u96be\u4ee5\u83b7\u53d6\uff0c\u8fd9\u9650\u5236\u4e86\u89c6\u89c9-\u8bed\u8a00\u8026\u5408\u6a21\u578b\u5728\u663e\u5fae\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u5e94\u7528", "method": "\u4f7f\u7528\u6807\u7b7e\u4f5c\u4e3a\u4e2d\u4ecb\uff0c\u4ece\u76f8\u5173\u6587\u732e\u81ea\u52a8\u6316\u6398\u533a\u57df\u63cf\u8ff0\u4f5c\u4e3a\u5408\u6210\u6807\u9898\uff0c\u901a\u8fc7\u56fe\u50cf\u5230\u6587\u672c\u8bad\u7ec3\u76ee\u6807\u5c06\u7ec6\u80de\u7ed3\u6784\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08CytoNet\uff09\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fde\u63a5", "result": "\u572857\u4e2a\u8111\u533a\u4e0a\uff0c\u8be5\u65b9\u6cd5\u751f\u6210\u5408\u7406\u7684\u533a\u57df\u63cf\u8ff0\uff0c\u5bf9\u8303\u56f4\u5185\u6591\u5757\u7684\u7ec6\u80de\u7ed3\u6784\u53c2\u8003\u6807\u7b7e\u5339\u914d\u51c6\u786e\u7387\u8fbe90.6%\uff0c\u5728\u63a9\u7801\u533a\u57df\u6807\u7b7e\u76848\u9879\u6d4b\u8bd5\u4e2d\u4ecd\u80fd\u4ee568.6%\u7684\u51c6\u786e\u7387\u6062\u590d\u533a\u57df", "conclusion": "\u5f31\u6807\u7b7e\u4ecb\u5bfc\u7684\u914d\u5bf9\u8db3\u4ee5\u8fde\u63a5\u73b0\u6709\u7684\u751f\u7269\u533b\u5b66\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\uff0c\u4e3a\u914d\u5bf9\u6ce8\u91ca\u7a00\u7f3a\u7684\u9886\u57df\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u96c6\u6210\u7684\u5b9e\u7528\u65b9\u6848"}}
{"id": "2602.23165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23165", "abs": "https://arxiv.org/abs/2602.23165", "authors": ["Yichen Peng", "Jyun-Ting Song", "Siyeol Jung", "Ruofan Liu", "Haiyang Liu", "Xuangeng Chu", "Ruicong Liu", "Erwin Wu", "Hideki Koike", "Kris Kitani"], "title": "DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation", "comment": "13 pages, 9 figures", "summary": "Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.", "AI": {"tldr": "DyaDiT\uff1a\u57fa\u4e8e\u591a\u6a21\u6001\u6269\u6563Transformer\u7684\u5bf9\u8bdd\u624b\u52bf\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u53cc\u4eba\u5bf9\u8bdd\u97f3\u9891\u4e2d\u751f\u6210\u8003\u8651\u793e\u4ea4\u4e0a\u4e0b\u6587\u548c\u4e92\u52a8\u52a8\u6001\u7684\u903c\u771f\u624b\u52bf", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5c06\u5355\u4e00\u97f3\u9891\u6d41\u6620\u5c04\u5230\u5355\u4e00\u8bf4\u8bdd\u8005\u7684\u52a8\u4f5c\uff0c\u6ca1\u6709\u8003\u8651\u793e\u4ea4\u4e0a\u4e0b\u6587\u6216\u5efa\u6a21\u5bf9\u8bdd\u4e2d\u4e24\u4eba\u4e4b\u95f4\u7684\u76f8\u4e92\u52a8\u6001\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u5bf9\u8bdd\u624b\u52bf\u4e0d\u591f\u81ea\u7136\u548c\u793e\u4ea4\u53c2\u4e0e\u6027\u4e0d\u8db3", "method": "\u63d0\u51faDyaDiT\u591a\u6a21\u6001\u6269\u6563Transformer\uff0c\u4ece\u53cc\u4eba\u97f3\u9891\u4fe1\u53f7\u751f\u6210\u4e0a\u4e0b\u6587\u9002\u5f53\u7684\u4eba\u7c7b\u52a8\u4f5c\u3002\u6a21\u578b\u878d\u5408\u53cc\u8bf4\u8bdd\u8005\u4fe1\u606f\u6355\u6349\u4e92\u52a8\u52a8\u6001\uff0c\u4f7f\u7528\u52a8\u4f5c\u5b57\u5178\u7f16\u7801\u52a8\u4f5c\u5148\u9a8c\uff0c\u53ef\u9009\u62e9\u6027\u5730\u5229\u7528\u5bf9\u8bdd\u4f19\u4f34\u7684\u624b\u52bf\u751f\u6210\u66f4\u5177\u54cd\u5e94\u6027\u7684\u52a8\u4f5c", "result": "\u5728\u6807\u51c6\u52a8\u4f5c\u751f\u6210\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u7528\u6237\u5b9a\u91cf\u7814\u7a76\u663e\u793a\u7528\u6237\u5f3a\u70c8\u504f\u597dDyaDiT\u751f\u6210\u7684\u52a8\u4f5c\uff0c\u8bc1\u660e\u5176\u9c81\u68d2\u6027\u548c\u793e\u4ea4\u53cb\u597d\u7684\u52a8\u4f5c\u751f\u6210\u80fd\u529b", "conclusion": "DyaDiT\u80fd\u591f\u751f\u6210\u8003\u8651\u793e\u4ea4\u4e0a\u4e0b\u6587\u548c\u4e92\u52a8\u52a8\u6001\u7684\u903c\u771f\u5bf9\u8bdd\u624b\u52bf\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b57\u4eba\u7c7b\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u793e\u4ea4\u53c2\u4e0e\u5ea6\uff0c\u4e3a\u793e\u4ea4\u4e92\u52a8\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.23169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23169", "abs": "https://arxiv.org/abs/2602.23169", "authors": ["Xiaole Tang", "Xiaoyi He", "Jiayi Xu", "Xiang Gu", "Jian Sun"], "title": "Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration", "comment": null, "summary": "Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.", "AI": {"tldr": "BaryIR\uff1a\u4e00\u79cd\u57fa\u4e8eWasserstein\u91cd\u5fc3\u7a7a\u95f4\u7684\u5bf9\u9f50\u591a\u6e90\u9000\u5316\u7279\u5f81\u7684\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u9000\u5316\u65e0\u5173\u7684\u5171\u4eab\u5185\u5bb9\u548c\u9000\u5316\u7279\u5b9a\u7684\u77e5\u8bc6\uff0c\u63d0\u5347\u5168\u573a\u666f\u56fe\u50cf\u6062\u590d\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u7684\u4e00\u4f53\u5316\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5728\u9762\u5bf9\u5206\u5e03\u5916\u9000\u5316\u65f6\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4f5c\u8005\u89c2\u5bdf\u5230\u591a\u6e90\u9000\u5316\u7279\u5f81\u5206\u5e03\u662f\u7531\u4e0d\u540c\u9000\u5316\u7279\u5b9a\u504f\u79fb\u4ece\u5e95\u5c42\u9000\u5316\u65e0\u5173\u5206\u5e03\u4e2d\u8bf1\u5bfc\u4ea7\u751f\u7684\uff0c\u6062\u590d\u8fd9\u79cd\u5171\u4eab\u5206\u5e03\u5bf9\u4e8e\u5b9e\u73b0\u8de8\u9000\u5316\u6cdb\u5316\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51faBaryIR\u6846\u67b6\uff0c\u5728Wasserstein\u91cd\u5fc3\u7a7a\u95f4\u4e2d\u5bf9\u9f50\u591a\u6e90\u9000\u5316\u7279\u5f81\uff0c\u8be5\u7a7a\u95f4\u901a\u8fc7\u6700\u5c0f\u5316\u5230\u591a\u6e90\u9000\u5316\u5206\u5e03\u7684Wasserstein\u8ddd\u79bb\u5e73\u5747\u503c\u6765\u5efa\u6a21\u9000\u5316\u65e0\u5173\u5206\u5e03\u3002\u5f15\u5165\u6b8b\u5dee\u5b50\u7a7a\u95f4\uff0c\u5176\u5d4c\u5165\u76f8\u4e92\u5bf9\u6bd4\u540c\u65f6\u4fdd\u6301\u4e0eWB\u5d4c\u5165\u6b63\u4ea4\uff0c\u4ece\u800c\u663e\u5f0f\u89e3\u8026\u4e24\u4e2a\u6b63\u4ea4\u7a7a\u95f4\uff1a\u7f16\u7801\u8de8\u9000\u5316\u5171\u4eab\u7684\u9000\u5316\u65e0\u5173\u4e0d\u53d8\u5185\u5bb9\u7684WB\u7a7a\u95f4\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94\u4fdd\u7559\u9000\u5316\u7279\u5b9a\u77e5\u8bc6\u7684\u6b8b\u5dee\u5b50\u7a7a\u95f4", "result": "BaryIR\u5728\u6027\u80fd\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u4e00\u4f53\u5316\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u672a\u89c1\u9000\u5316\u7c7b\u578b\u548c\u7ea7\u522b\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5373\u4f7f\u5728\u6709\u9650\u9000\u5316\u7c7b\u578b\u4e0a\u8bad\u7ec3\u5e76\u5728\u6df7\u5408\u9000\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8bc4\u4f30\u65f6\uff0c\u4e5f\u80fd\u5b66\u4e60\u5230\u5177\u6709\u663e\u8457\u9c81\u68d2\u6027\u7684\u6cdb\u5316\u7279\u5f81", "conclusion": "\u901a\u8fc7\u89e3\u8026\u9000\u5316\u65e0\u5173\u7684\u5171\u4eab\u5185\u5bb9\u548c\u9000\u5316\u7279\u5b9a\u7684\u77e5\u8bc6\uff0cBaryIR\u80fd\u591f\u51cf\u8f7b\u5bf9\u5206\u5e03\u5185\u9000\u5316\u7684\u8fc7\u62df\u5408\uff0c\u5b9e\u73b0\u57fa\u4e8e\u9000\u5316\u65e0\u5173\u5171\u4eab\u4e0d\u53d8\u6027\u7684\u81ea\u9002\u5e94\u6062\u590d\uff0c\u4ece\u800c\u63d0\u5347\u4e00\u4f53\u5316\u56fe\u50cf\u6062\u590d\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.23177", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23177", "abs": "https://arxiv.org/abs/2602.23177", "authors": ["Bin Zeng", "Johannes K\u00fcnzel", "Anna Hilsmann", "Peter Eisert"], "title": "Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms", "comment": "published at VISAPP 2026", "summary": "Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u7ea6\u675f\u7684\u5b9e\u65f6\u4eba\u7fa4\u8ba1\u6570\u65b9\u6cd5\uff0c\u5229\u7528\u5b89\u88c5\u5728\u5217\u8f66\u4e0a\u7684\u5355\u6444\u50cf\u5934\u5728\u8fdb\u7ad9\u65f6\u626b\u63cf\u7ad9\u53f0\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u7684\u5361\u5c14\u66fc\u6ee4\u6ce2\u6a21\u578b\u548c\u865a\u62df\u8ba1\u6570\u5e26\u5b9e\u73b0\u7cbe\u786e\u8ba1\u6570\u3002", "motivation": "\u94c1\u8def\u7ad9\u53f0\u5b9e\u65f6\u4eba\u7fa4\u8ba1\u6570\u5bf9\u5b89\u5168\u548c\u5bb9\u91cf\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u8ddf\u8e2a\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u9759\u6001\u6444\u50cf\u5934\u6216\u5ffd\u7565\u7269\u7406\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u5728\u5217\u8f66\u8fdb\u7ad9\u65f6\u7684\u52a8\u6001\u6761\u4ef6\u4e0b\u8ba1\u6570\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u7269\u7406\u7ea6\u675f\u8ddf\u8e2a\u6846\u67b6\uff0c\u5c06\u68c0\u6d4b\u3001\u5916\u89c2\u548c3D\u8fd0\u52a8\u63a8\u7406\u7edf\u4e00\u5230\u5b9e\u65f6\u6d41\u7a0b\u4e2d\u3002\u96c6\u6210\u8fc1\u79fb\u5b66\u4e60\u7684YOLOv11m\u68c0\u6d4b\u5668\u548cEfficientNet-B0\u5916\u89c2\u7f16\u7801\u5230DeepSORT\u4e2d\uff0c\u5f15\u5165\u7269\u7406\u7ea6\u675f\u5361\u5c14\u66fc\u6a21\u578b(Phys-3D)\u901a\u8fc7\u9488\u5b54\u51e0\u4f55\u5f3a\u5236\u7269\u7406\u5408\u7406\u76843D\u8fd0\u52a8\u52a8\u529b\u5b66\u3002\u9488\u5bf9\u906e\u6321\u95ee\u9898\uff0c\u5b9e\u73b0\u5e26\u6301\u4e45\u6027\u7684\u865a\u62df\u8ba1\u6570\u5e26\u3002", "result": "\u5728MOT-RailwayPlatformCrowdHead\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u8ba1\u6570\u8bef\u5dee\u964d\u4f4e\u52302.97%\uff0c\u5728\u8fd0\u52a8\u548c\u906e\u6321\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u80fd\u3002", "conclusion": "\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u51e0\u4f55\u548c\u8fd0\u52a8\u5148\u9a8c\u80fd\u591f\u5728\u5b89\u5168\u5173\u952e\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u5b9e\u73b0\u53ef\u9760\u7684\u4eba\u7fa4\u8ba1\u6570\uff0c\u6709\u52a9\u4e8e\u6709\u6548\u7684\u5217\u8f66\u8c03\u5ea6\u548c\u7ad9\u53f0\u5b89\u5168\u7ba1\u7406\u3002"}}
{"id": "2602.23191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23191", "abs": "https://arxiv.org/abs/2602.23191", "authors": ["Xinyuan Chen", "Yao Xu", "Shaowen Wang", "Pengjie Song", "Bowen Deng"], "title": "Uni-Animator: Towards Unified Visual Colorization", "comment": "10 pages, 8 figures. Submitted to CVPR 2026", "summary": "We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.", "AI": {"tldr": "Uni-Animator\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563Transformer\u7684\u7edf\u4e00\u56fe\u50cf\u548c\u89c6\u9891\u8349\u56fe\u7740\u8272\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u989c\u8272\u8f6c\u79fb\u4e0d\u7cbe\u786e\u3001\u7269\u7406\u7ec6\u8282\u4fdd\u7559\u4e0d\u8db3\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8349\u56fe\u7740\u8272\u65b9\u6cd5\u96be\u4ee5\u7edf\u4e00\u56fe\u50cf\u548c\u89c6\u9891\u4efb\u52a1\uff0c\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a1\uff09\u4f7f\u7528\u5355\u4e2a\u6216\u591a\u4e2a\u53c2\u8003\u65f6\u989c\u8272\u8f6c\u79fb\u4e0d\u7cbe\u786e\uff1b2\uff09\u9ad8\u9891\u7269\u7406\u7ec6\u8282\u4fdd\u7559\u4e0d\u8db3\uff1b3\uff09\u5927\u8fd0\u52a8\u573a\u666f\u4e2d\u65f6\u95f4\u4e00\u81f4\u6027\u5dee\uff0c\u5b58\u5728\u8fd0\u52a8\u4f2a\u5f71\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u901a\u8fc7\u5b9e\u4f8b\u8865\u4e01\u5d4c\u5165\u5b9e\u73b0\u89c6\u89c9\u53c2\u8003\u589e\u5f3a\uff0c\u7cbe\u786e\u5bf9\u9f50\u548c\u878d\u5408\u53c2\u8003\u989c\u8272\u4fe1\u606f\uff1b2\uff09\u4f7f\u7528\u7269\u7406\u7279\u5f81\u8fdb\u884c\u7269\u7406\u7ec6\u8282\u5f3a\u5316\uff0c\u6709\u6548\u6355\u6349\u548c\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\uff1b3\uff09\u57fa\u4e8e\u8349\u56fe\u7684\u52a8\u6001RoPE\u7f16\u7801\uff0c\u81ea\u9002\u5e94\u5efa\u6a21\u8fd0\u52a8\u611f\u77e5\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUni-Animator\u5728\u56fe\u50cf\u548c\u89c6\u9891\u8349\u56fe\u7740\u8272\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4e0e\u4efb\u52a1\u7279\u5b9a\u65b9\u6cd5\u76f8\u5f53\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u8de8\u57df\u80fd\u529b\uff0c\u5177\u6709\u9ad8\u7ec6\u8282\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "conclusion": "Uni-Animator\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8349\u56fe\u7740\u8272\u4e2d\u989c\u8272\u8f6c\u79fb\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u56fe\u50cf\u548c\u89c6\u9891\u8349\u56fe\u7740\u8272\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23217", "categories": ["cs.CV", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.23217", "abs": "https://arxiv.org/abs/2602.23217", "authors": ["Alaa El Ichi", "Khalide Jbilou"], "title": "Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks", "comment": null, "summary": "This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u7ef4\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u6846\u67b6\uff0c\u57fa\u4e8e\u5e7f\u4e49\u7231\u56e0\u65af\u5766MLP\uff08GE-MLPs\uff09\uff0c\u901a\u8fc7\u7231\u56e0\u65af\u5766\u79ef\u76f4\u63a5\u5728\u5f20\u91cf\u4e0a\u64cd\u4f5c\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u77e9\u9635\u601d\u7ef4\u5bf9\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u9650\u5236\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u8868\u8ff0\u53d7\u9650\u4e8e\u57fa\u4e8e\u77e9\u9635\u7684\u601d\u7ef4\uff1a\u6807\u51c6\u67b6\u6784\u4f9d\u8d56\u77e9\u9635\u503c\u6743\u91cd\u548c\u5411\u91cf\u503c\u504f\u7f6e\uff0c\u9700\u8981\u8fdb\u884c\u7ed3\u6784\u6241\u5e73\u5316\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u7136\u53ef\u8868\u8fbe\u4efb\u52a1\u7684\u7a7a\u95f4\u3002\u4f5c\u8005\u8ba4\u4e3a\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u7684\u6570\u5b66\u6846\u67b6\u6765\u7edf\u4e00\u7406\u89e3\u4e0d\u540c\u7684\u89c6\u89c9\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49\u7231\u56e0\u65af\u5766MLP\uff08GE-MLPs\uff09\u7684\u591a\u7ef4\u4efb\u52a1\u5b66\u4e60\uff08MTL\uff09\u6846\u67b6\uff0c\u4f7f\u7528\u5f20\u91cf\u503c\u53c2\u6570\uff0c\u901a\u8fc7\u7231\u56e0\u65af\u5766\u79ef\u76f4\u63a5\u5728\u5f20\u91cf\u4e0a\u64cd\u4f5c\uff0c\u80fd\u591f\u660e\u786e\u63a7\u5236\u54ea\u4e9b\u7ef4\u5ea6\u88ab\u4fdd\u7559\u6216\u6536\u7f29\u800c\u4e0d\u4e22\u5931\u4fe1\u606f\u3002", "result": "\u901a\u8fc7\u4e25\u683c\u7684\u6570\u5b66\u63a8\u5bfc\u8bc1\u660e\u5206\u7c7b\u3001\u5206\u5272\u548c\u68c0\u6d4b\u90fd\u662fMTL\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u4ec5\u5728\u5f62\u5f0f\u5316\u5b9a\u4e49\u7684\u4efb\u52a1\u7a7a\u95f4\u4e2d\u7684\u7ef4\u5ea6\u914d\u7f6e\u4e0d\u540c\u3002\u8bc1\u660e\u8be5\u4efb\u52a1\u7a7a\u95f4\u4e25\u683c\u5927\u4e8e\u57fa\u4e8e\u77e9\u9635\u7684\u8868\u8ff0\u6240\u80fd\u539f\u751f\u8868\u8fbe\u7684\u7a7a\u95f4\uff0c\u652f\u6301\u65f6\u7a7a\u6216\u8de8\u6a21\u6001\u9884\u6d4b\u7b49\u9700\u8981\u539f\u5219\u6027\u4efb\u52a1\u914d\u7f6e\u7684\u573a\u666f\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u901a\u8fc7\u5f20\u91cf\u4ee3\u6570\u89c6\u89d2\u7406\u89e3\u3001\u6bd4\u8f83\u548c\u8bbe\u8ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u63d0\u4f9b\u4e86\u6570\u5b66\u57fa\u7840\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u77e9\u9635\u601d\u7ef4\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u66f4\u4e30\u5bcc\u7684\u4efb\u52a1\u8868\u8fbe\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2602.23297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23297", "abs": "https://arxiv.org/abs/2602.23297", "authors": ["Yiqing Wang", "Chunming He", "Ming-Chen Lu", "Mercy Pawar", "Leslie Niziol", "Maria Woodward", "Sina Farsiu"], "title": "PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM", "comment": null, "summary": "Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.", "AI": {"tldr": "PRIMA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u533b\u5b66\u8bca\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u7279\u5f81\u548c\u4e34\u5e8a\u5143\u6570\u636e\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u548c\u98ce\u9669\u96c6\u6210\u56fe\u50cf-\u5143\u6570\u636e\u5bf9\u9f50\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u75be\u75c5\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u8bca\u65ad\u65b9\u6cd5\u901a\u5e38\u5c06\u4e34\u5e8a\u5143\u6570\u636e\u89c6\u4e3a\u5b64\u7acb\u6807\u7b7e\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u4e34\u5e8a\u63cf\u8ff0\u4e2d\u4e30\u5bcc\u7684\u8bed\u4e49\u77e5\u8bc6\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6548\u679c\u53d7\u9650\u3002", "method": "1. \u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6784\u5efa\u98ce\u9669-\u75be\u75c5\u5173\u8054\u4e13\u5bb6\u8bed\u6599\u5e93\uff0c\u7cbe\u70bcClinical ModernBERT\u6587\u672c\u7f16\u7801\u5668\uff1b2. \u91c7\u7528DINOv3\u548c\u7cbe\u70bcBERT\u7684\u53cc\u7f16\u7801\u5668\u9884\u8bad\u7ec3\u7b56\u7565\uff1b3. \u8bbe\u8ba1\u56db\u79cd\u4e92\u8865\u635f\u5931\u51fd\u6570\u5b9e\u73b0\u591a\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\uff1b4. \u4f7f\u7528Qwen-3\u878d\u5408\u5bf9\u9f50\u7279\u5f81\u8fdb\u884c\u75be\u75c5\u5206\u7c7b\u3002", "result": "PRIMA\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u6709\u6548\u534f\u8c03\u50cf\u7d20\u7ea7\u7279\u5f81\u4e0e\u62bd\u8c61\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4e14\u65e0\u9700\u5927\u89c4\u6a21\u6570\u636e\u6536\u96c6\u6216\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u5373\u5b9e\u73b0\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "PRIMA\u6846\u67b6\u6210\u529f\u5c06\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u6574\u5408\u5230\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u4e2d\uff0c\u901a\u8fc7\u98ce\u9669\u96c6\u6210\u56fe\u50cf-\u5143\u6570\u636e\u5bf9\u9f50\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.23306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23306", "abs": "https://arxiv.org/abs/2602.23306", "authors": ["Yiran Guan", "Sifan Tu", "Dingkang Liang", "Linghao Zhu", "Jianzhong Ju", "Zhenbo Luo", "Jian Luan", "Yuliang Liu", "Xiang Bai"], "title": "ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding", "comment": "Accept by ICLR 2026", "summary": "Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.", "AI": {"tldr": "ThinkOmni\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u548c\u6570\u636e\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u73b0\u6210\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u6307\u5bfc\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\uff0c\u5c06\u6587\u672c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u5230\u5168\u6a21\u6001\u573a\u666f", "motivation": "\u73b0\u6709\u7684\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u80fd\u591f\u611f\u77e5\u591a\u79cd\u6a21\u6001\uff0c\u4f46\u7f3a\u4e4f\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u800c\u901a\u8fc7\u989d\u5916\u8bad\u7ec3\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u9762\u4e34\u9ad8\u8d28\u91cf\u6570\u636e\u9700\u6c42\u3001\u4efb\u52a1\u7279\u5b9a\u9002\u5e94\u548c\u5de8\u5927\u8ba1\u7b97\u6210\u672c\u7b49\u6311\u6218", "method": "\u63d0\u51faThinkOmni\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1) LRM-as-a-Guide\uff1a\u5229\u7528\u73b0\u6210\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\u6307\u5bfc\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\uff1b2) Stepwise Contrastive Scaling\uff1a\u81ea\u9002\u5e94\u5e73\u8861\u611f\u77e5\u548c\u63a8\u7406\u4fe1\u53f7\uff0c\u65e0\u9700\u624b\u52a8\u8d85\u53c2\u6570\u8c03\u4f18", "result": "\u5728\u516d\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cThinkOmni\u59cb\u7ec8\u5e26\u6765\u6027\u80fd\u63d0\u5347\uff0c\u4e3b\u8981\u7ed3\u679c\u5728MathVista\u4e0a\u8fbe\u523070.2\u5206\uff0c\u5728MMAU\u4e0a\u8fbe\u523075.5\u5206", "conclusion": "ThinkOmni\u4e3a\u5168\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u4e3a\u63a8\u7406\u80fd\u529b\u7684\u6cdb\u5316\u548c\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3"}}
{"id": "2602.23339", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23339", "abs": "https://arxiv.org/abs/2602.23339", "authors": ["Tilemachos Aravanis", "Vladan Stojni\u0107", "Bill Psomas", "Nikos Komodakis", "Giorgos Tolias"], "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?", "comment": null, "summary": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u68c0\u7d22\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u9002\u914d\u5668\uff0c\u901a\u8fc7\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u652f\u6301\u7279\u5f81\u6765\u5b66\u4e60\u8f7b\u91cf\u7ea7\u7684\u6bcf\u56fe\u50cf\u5206\u7c7b\u5668\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u96f6\u6837\u672c\u4e0e\u5168\u76d1\u7763\u5206\u5272\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "motivation": "\u5f00\u653e\u8bcd\u6c47\u5206\u5272\uff08OVS\uff09\u867d\u7136\u6269\u5c55\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u8bc6\u522b\u80fd\u529b\uff0c\u4f46\u4ecd\u843d\u540e\u4e8e\u5168\u76d1\u7763\u65b9\u6cd5\uff0c\u4e3b\u8981\u9762\u4e34\u4e24\u4e2a\u6311\u6218\uff1a\u8bad\u7ec3VLM\u65f6\u4f7f\u7528\u7684\u7c97\u7c92\u5ea6\u56fe\u50cf\u7ea7\u76d1\u7763\uff0c\u4ee5\u53ca\u81ea\u7136\u8bed\u8a00\u7684\u8bed\u4e49\u6a21\u7cca\u6027\u3002", "method": "\u5f15\u5165\u5c11\u6837\u672c\u8bbe\u7f6e\uff0c\u901a\u8fc7\u50cf\u7d20\u6807\u6ce8\u56fe\u50cf\u7684\u652f\u6301\u96c6\u589e\u5f3a\u6587\u672c\u63d0\u793a\uff1b\u63d0\u51fa\u68c0\u7d22\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u9002\u914d\u5668\uff0c\u901a\u8fc7\u5b66\u4e60\u6bcf\u56fe\u50cf\u5206\u7c7b\u5668\u6765\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u652f\u6301\u7279\u5f81\uff0c\u5b9e\u73b0\u6a21\u6001\u95f4\u7684\u66f4\u5f3a\u534f\u540c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u7f29\u5c0f\u4e86\u96f6\u6837\u672c\u5206\u5272\u4e0e\u76d1\u7763\u5206\u5272\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff0c\u5e76\u9002\u7528\u4e8e\u7ec6\u7c92\u5ea6\u4efb\u52a1\u5982\u4e2a\u6027\u5316\u5206\u5272\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u6587\u672c\u548c\u89c6\u89c9\u652f\u6301\u7279\u5f81\u7684\u68c0\u7d22\u589e\u5f3a\u6d4b\u8bd5\u65f6\u9002\u914d\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5206\u5272\u6027\u80fd\uff0c\u4e3a\u7ec6\u7c92\u5ea6\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
