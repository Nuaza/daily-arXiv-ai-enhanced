<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 28]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Interactive Medical-SAM2 GUI: A Napari-based semi-automatic annotation tool for medical images](https://arxiv.org/abs/2602.22649)
*Woojae Hong,Jong Ha Hwang,Jiyong Chung,Joongyeon Choi,Hyunngun Kim,Yong Hwy Kim*

Main category: cs.CV

TL;DR: Medical-SAM2 GUI是一个基于Napari的开源桌面应用，用于2D/3D医学图像的半自动标注，通过SAM2传播和交互式修正提高3D标注效率。


<details>
  <summary>Details</summary>
Motivation: 医学影像算法开发需要体素级标注，但3D扫描的手动标注耗时昂贵，现有工具缺乏统一的队列导向工作流程，无法在一个本地管道中整合导航、传播、交互修正和定量导出。

Method: 基于Napari构建，将3D体积视为切片序列，集成SAM2式传播和Medical-SAM2；提供本地优先工作流，支持DICOM/NIfTI格式，通过框/点提示初始化对象，支持首尾切片初始化、交互修正，最后保存标注。

Result: 开发了一个开源GUI工具，支持多病例顺序标注、单对象传播、交互式修正，导出时支持每对象体积测量和3D体积渲染，通过SimpleITK保持图像几何信息。

Conclusion: 该工具为医学影像研究提供了一个高效的3D半自动标注解决方案，整合了导航、传播、修正和导出功能，专门用于研究标注工作流。

Abstract: Interactive Medical-SAM2 GUI is an open-source desktop application for semi-automatic annotation of 2D and 3D medical images. Built on the Napari multi-dimensional viewer, box/point prompting is integrated with SAM2-style propagation by treating a 3D volume as a slice sequence, enabling mask propagation from sparse prompts using Medical-SAM2 on top of SAM2. Voxel-level annotation remains essential for developing and validating medical imaging algorithms, yet manual labeling is slow and expensive for 3D scans, and existing integrations frequently emphasize per-slice interaction without providing a unified, cohort-oriented workflow for navigation, propagation, interactive correction, and quantitative export in a single local pipeline. To address this practical limitation, a local-first Napari workflow is provided for efficient 3D annotation across multiple studies using standard DICOM series and/or NIfTI volumes. Users can annotate cases sequentially under a single root folder with explicit proceed/skip actions, initialize objects via box-first prompting (including first/last-slice initialization for single-object propagation), refine predictions with point prompts, and finalize labels through prompt-first correction prior to saving. During export, per-object volumetry and 3D volume rendering are supported, and image geometry is preserved via SimpleITK. The GUI is implemented in Python using Napari and PyTorch, with optional N4 bias-field correction, and is intended exclusively for research annotation workflows. The code is released on the project page: https://github.com/SKKU-IBE/Medical-SAM2GUI/.

</details>


### [2] [Denoising as Path Planning: Training-Free Acceleration of Diffusion Models with DPCache](https://arxiv.org/abs/2602.22654)
*Bowen Cui,Yuanbin Wang,Huajiang Xu,Biaolong Chen,Aixi Zhang,Hao Jiang,Zhengzheng Jin,Xu Liu,Pipei Huang*

Main category: cs.CV

TL;DR: DPCache是一种无需训练的扩散模型加速框架，将采样加速视为全局路径规划问题，通过动态规划选择最优关键时间步序列，在保持质量的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实际部署中受到多步迭代采样的计算开销限制。现有的基于缓存的加速方法使用固定或局部自适应调度，未考虑去噪轨迹的全局结构，容易导致误差累积和视觉伪影。

Method: DPCache将扩散采样加速构建为全局路径规划问题。首先从小型校准集构建路径感知成本张量，量化在给定前一个关键时间步条件下跳过时间步的路径相关误差。然后使用动态规划选择最小化总路径成本的关键时间步序列，在推理时只在关键时间步进行完整计算，中间输出使用缓存特征高效预测。

Result: 在DiT、FLUX和HunyuanVideo上的实验表明，DPCache在保持最小质量损失的情况下实现了显著加速。在FLUX上，4.87倍加速时ImageReward提升+0.031，3.54倍加速时甚至超过全步基线+0.028 ImageReward，优于现有加速方法。

Conclusion: DPCache通过路径感知的全局调度框架，有效解决了现有缓存方法忽略全局结构的问题，在扩散模型加速方面取得了显著进展，为实际部署提供了有效的训练免费解决方案。

Abstract: Diffusion models have demonstrated remarkable success in image and video generation, yet their practical deployment remains hindered by the substantial computational overhead of multi-step iterative sampling. Among acceleration strategies, caching-based methods offer a training-free and effective solution by reusing or predicting features across timesteps. However, existing approaches rely on fixed or locally adaptive schedules without considering the global structure of the denoising trajectory, often leading to error accumulation and visual artifacts. To overcome this limitation, we propose DPCache, a novel training-free acceleration framework that formulates diffusion sampling acceleration as a global path planning problem. DPCache constructs a Path-Aware Cost Tensor from a small calibration set to quantify the path-dependent error of skipping timesteps conditioned on the preceding key timestep. Leveraging this tensor, DPCache employs dynamic programming to select an optimal sequence of key timesteps that minimizes the total path cost while preserving trajectory fidelity. During inference, the model performs full computations only at these key timesteps, while intermediate outputs are efficiently predicted using cached features. Extensive experiments on DiT, FLUX, and HunyuanVideo demonstrate that DPCache achieves strong acceleration with minimal quality loss, outperforming prior acceleration methods by $+$0.031 ImageReward at 4.87$\times$ speedup and even surpassing the full-step baseline by $+$0.028 ImageReward at 3.54$\times$ speedup on FLUX, validating the effectiveness of our path-aware global scheduling framework. Code will be released at https://github.com/argsss/DPCache.

</details>


### [3] [ArtPro: Self-Supervised Articulated Object Reconstruction with Adaptive Integration of Mobility Proposals](https://arxiv.org/abs/2602.22666)
*Xuelu Li,Zhaonan Wang,Xiaogang Wang,Lei Wu,Manyi Li,Changhe Tu*

Main category: cs.CV

TL;DR: ArtPro：一种自监督框架，通过自适应整合运动提议来重建复杂多部件铰接物体，解决了现有方法对初始分割敏感且易陷入局部最优的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于可微分渲染的自监督方法对初始部件分割高度敏感，依赖启发式聚类或预训练模型，导致复杂多部件物体优化时容易陷入局部最小值。

Method: 提出ArtPro框架：1）基于几何特征和运动先验进行过分割初始化，生成带合理运动假设的部件提议；2）优化过程中通过分析空间邻域的运动一致性动态合并提议；3）碰撞感知的运动剪枝机制防止错误的运动学估计。

Result: 在合成和真实物体上的大量实验表明，ArtPro能够稳健地重建复杂多部件物体，在准确性和稳定性方面显著优于现有方法。

Conclusion: ArtPro通过自适应整合运动提议的自监督框架，有效解决了铰接物体重建中对初始分割敏感的问题，为复杂多部件物体的高保真数字孪生提供了可靠解决方案。

Abstract: Reconstructing articulated objects into high-fidelity digital twins is crucial for applications such as robotic manipulation and interactive simulation. Recent self-supervised methods using differentiable rendering frameworks like 3D Gaussian Splatting remain highly sensitive to the initial part segmentation. Their reliance on heuristic clustering or pre-trained models often causes optimization to converge to local minima, especially for complex multi-part objects. To address these limitations, we propose ArtPro, a novel self-supervised framework that introduces adaptive integration of mobility proposals. Our approach begins with an over-segmentation initialization guided by geometry features and motion priors, generating part proposals with plausible motion hypotheses. During optimization, we dynamically merge these proposals by analyzing motion consistency among spatial neighbors, while a collision-aware motion pruning mechanism prevents erroneous kinematic estimation. Extensive experiments on both synthetic and real-world objects demonstrate that ArtPro achieves robust reconstruction of complex multi-part objects, significantly outperforming existing methods in accuracy and stability.

</details>


### [4] [Monocular Open Vocabulary Occupancy Prediction for Indoor Scenes](https://arxiv.org/abs/2602.22667)
*Changqing Zhou,Yueru Luo,Han Zhang,Zeyu Jiang,Changhao Chen*

Main category: cs.CV

TL;DR: 提出了一种用于室内场景的开放词汇3D占用预测方法，使用仅几何监督（二值占用标签）和3D语言嵌入高斯表示，通过改进的泊松聚合和渐进温度衰减实现几何与语义的联合学习。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇3D占用方法主要针对室外驾驶场景，在室内场景中表现不佳。室内环境几何更密集、布局更复杂、语义更细粒度，需要专门的方法来处理这些挑战。

Method: 采用仅几何监督范式（仅使用二值占用标签），基于3D语言嵌入高斯作为统一中间表示。几何方面：提出不透明度感知的泊松基方法稳定体素聚合；语义方面：提出渐进温度衰减调度，在渲染过程中逐渐锐化不透明度，增强高斯-语言对齐。

Result: 在Occ-ScanNet数据集上，开放词汇设置下达到59.50 IoU和21.05 mIoU，在IoU方面超越所有现有占用方法，在mIoU方面大幅领先先前开放词汇方法。

Conclusion: 该方法成功解决了室内开放词汇3D占用的挑战，通过仅几何监督和创新的几何/语义对齐技术，实现了在复杂室内环境中的高性能占用预测。

Abstract: Open-vocabulary 3D occupancy is vital for embodied agents, which need to understand complex indoor environments where semantic categories are abundant and evolve beyond fixed taxonomies. While recent work has explored open-vocabulary occupancy in outdoor driving scenarios, such methods transfer poorly indoors, where geometry is denser, layouts are more intricate, and semantics are far more fine-grained. To address these challenges, we adopt a geometry-only supervision paradigm that uses only binary occupancy labels (occupied vs free). Our framework builds upon 3D Language-Embedded Gaussians, which serve as a unified intermediate representation coupling fine-grained 3D geometry with a language-aligned semantic embedding. On the geometry side, we find that existing Gaussian-to-Occupancy operators fail to converge under such weak supervision, and we introduce an opacity-aware, Poisson-based approach that stabilizes volumetric aggregation. On the semantic side, direct alignment between rendered features and open-vocabulary segmentation features suffers from feature mixing; we therefore propose a Progressive Temperature Decay schedule that gradually sharpens opacities during splatting, strengthening Gaussian-language alignment. On Occ-ScanNet, our framework achieves 59.50 IoU and 21.05 mIoU in the open-vocabulary setting, surpassing all existing occupancy methods in IoU and outperforming prior open-vocabulary approaches by a large margin in mIoU. Code will be released at https://github.com/JuIvyy/LegoOcc.

</details>


### [5] [ViCLIP-OT: The First Foundation Vision-Language Model for Vietnamese Image-Text Retrieval with Optimal Transport](https://arxiv.org/abs/2602.22678)
*Quoc-Khang Tran,Minh-Thien Nguyen,Nguyen-Khang Pham*

Main category: cs.CV

TL;DR: ViCLIP-OT：针对越南语图像-文本检索的视觉语言基础模型，结合CLIP对比学习和SIGROT损失，在低资源语言场景下显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要针对高资源语言优化，在越南语等低资源语言场景下表现不佳，需要专门设计适合低资源语言的跨模态检索模型。

Method: 提出ViCLIP-OT框架，集成CLIP风格的对比学习与相似性图正则化最优传输（SIGROT）损失，增强全局跨模态一致性并缓解模态差距问题。

Result: 在三个越南语基准测试（UITOpenViIC、KTVIC、Crossmodal-3600）上均优于CLIP和SigLIP基线。在UIT-OpenViIC上平均Recall@K达67.34%，比CLIP提升5.75个百分点；在Crossmodal-3600零样本评估中比CLIP提升11.72个百分点。

Conclusion: SIGROT集成为低资源语言的跨模态检索提供了有效且可扩展的策略，对越南语及其他代表性不足语言环境的智能多媒体检索系统具有实际应用价值。

Abstract: Image-text retrieval has become a fundamental component in intelligent multimedia systems; however, most existing vision-language models are optimized for highresource languages and remain suboptimal for low-resource settings such as Vietnamese. This work introduces ViCLIP-OT, a foundation vision-language model specifically designed for Vietnamese image-text retrieval. The proposed framework integrates CLIP-style contrastive learning with a Similarity-Graph Regularized Optimal Transport (SIGROT) loss to enhance global cross-modal consistency and mitigate modality gap issues. Extensive experiments on three Vietnamese benchmarks (UITOpenViIC, KTVIC, and Crossmodal-3600) demonstrate that ViCLIP-OT consistently outperforms CLIP and SigLIP baselines in both in-domain and zero-shot settings. On UIT-OpenViIC, the model achieves an average Recall@K of 67.34%, improving upon CLIP by 5.75 percentage points. In zero-shot evaluation on Crossmodal-3600, ViCLIPOT surpasses CLIP by 11.72 percentage points. Embedding-space analysis further confirms improved alignment and reduced modality gap. The results indicate that integrating SIGROT provides an effective and scalable strategy for cross-modal retrieval in low-resource languages, offering practical implications for intelligent multimedia retrieval systems in Vietnamese and other underrepresented linguistic contexts.

</details>


### [6] [SoPE: Spherical Coordinate-Based Positional Embedding for Enhancing Spatial Perception of 3D LVLMs](https://arxiv.org/abs/2602.22716)
*Guanting Ye,Qiyan Zhao,Wenhao Yu,Liangyu Yuan,Mingkai Li,Xiaofeng Zhang,Jianmin Ji,Yanyong Zhang,Qing Jiang,Ka-Veng Yuen*

Main category: cs.CV

TL;DR: 论文提出SoPE方法，通过球坐标位置嵌入改进3D大视觉语言模型的位置编码机制，解决传统RoPE在3D空间建模中的不足


<details>
  <summary>Details</summary>
Motivation: 现有的3D大视觉语言模型继承了传统RoPE位置编码机制，但这种位置依赖建模方式在3D多模态理解中存在局限性。传统RoPE在编码3D token时无法保持三维空间结构，其相对距离计算忽略了角度依赖，限制了模型捕捉视觉表示中方向变化的能力。

Method: 提出球坐标位置嵌入(SoPE)方法，将点云token索引映射到3D球坐标空间，实现空间位置和方向角的统一建模。同时引入多尺度频率混合策略，融合不同频率域的特征信息。

Result: 在多个3D场景基准测试上的实验结果表明该方法有效，真实世界部署实验进一步证明了其强大的泛化能力。

Conclusion: SoPE方法能够保持点云数据的固有几何结构，增强空间感知能力，为多模态学习提供更一致和更具表现力的几何表示。

Abstract: 3D Large Vision-Language Models (3D LVLMs) built upon Large Language Models (LLMs) have achieved remarkable progress across various multimodal tasks. However, their inherited position-dependent modeling mechanism, Rotary Position Embedding (RoPE), remains suboptimal for 3D multimodal understanding. The vanilla RoPE formulation fails to preserve essential three-dimensional spatial structures when encoding 3D tokens, and its relative distance computation overlooks angular dependencies, hindering the model's ability to capture directional variations in visual representations. To overcome these limitations, we introduce Spherical Coordinate-based Positional Embedding (SoPE). Our method maps point-cloud token indices into a 3D spherical coordinate space, enabling unified modeling of spatial locations and directional angles. This formulation preserves the inherent geometric structure of point-cloud data, enhances spatial awareness, and yields more consistent and expressive geometric representations for multimodal learning. In addition, we introduce a multi-scale frequency mixing strategy to fuse feature information across different frequency domains. Experimental results on multiple 3D scene benchmarks validate the effectiveness of our approach, while real-world deployment experiments further demonstrate its strong generalization capability.

</details>


### [7] [IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling](https://arxiv.org/abs/2602.22717)
*Shuoqi Chen,Yujia Wu,Geoffrey P. Luke*

Main category: cs.CV

TL;DR: 提出基于扩散模型的超声图像去斑方法，通过模拟数据训练，在保持解剖结构的同时有效抑制斑点噪声，优于传统方法和现有学习基线。


<details>
  <summary>Details</summary>
Motivation: 超声成像虽然实时无创，但斑点噪声和相关伪影会降低图像质量，影响诊断解读。需要开发有效的去斑方法来提高超声图像质量。

Method: 基于图像恢复随机微分方程框架构建扩散模型，使用Matlab超声工具箱从无斑点的磁共振图像模拟超声图像创建大规模配对数据集进行监督训练。

Result: 在模拟测试集上，该方法在抑制斑点噪声的同时保留了有意义的解剖边缘和对比度，优于经典滤波器和近期学习基线。通过交叉模型方差量化预测不确定性，发现高不确定性区域与高重建误差相关。

Conclusion: 提出的扩散模型能有效去斑并保持解剖结构，预测不确定性可作为困难区域的实用指标。观察到对模拟探头设置的敏感性导致的域偏移，表明需要多样化训练和适应以实现稳健的临床部署。

Abstract: Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.

</details>


### [8] [HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2602.22727)
*Yangguang Lin,Quan Fang,Yufei Li,Jiachen Sun,Junyu Gao,Jitao Sang*

Main category: cs.CV

TL;DR: HulluEdit：一种单次前向、无需参考模型的干预框架，通过正交子空间编辑选择性抑制大视觉语言模型中的物体幻觉，同时保持视觉基础能力。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型中的物体幻觉严重阻碍了其可靠部署。现有方法在效率和准确性之间难以平衡：要么需要昂贵的参考模型和多次前向传播，要么采用静态编辑可能抑制真实的视觉证据。

Method: 提出正交子空间编辑方法，将模型的隐藏状态分解为三个正交子空间：视觉证据、冲突先验和残差不确定性，从而能够选择性抑制幻觉模式而不干扰视觉基础。该方法在数学上保证对先验子空间的编辑完全不影响视觉成分。

Result: 在POPE和CHAIR等基准测试中实现了最先进的幻觉减少效果，同时在MME上保持通用能力，推理效率高。在各种架构上始终优于对比解码和静态子空间编辑基线方法。

Conclusion: HulluEdit为大视觉语言模型提供了一种新的可信赖路径，通过正交子空间编辑有效减少物体幻觉，同时保持模型性能和推理效率。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.

</details>


### [9] [AMLRIS: Alignment-aware Masked Learning for Referring Image Segmentation](https://arxiv.org/abs/2602.22740)
*Tongfei Chen,Shuo Yang,Yuguang Yang,Linlin Yang,Runtang Guo,Changbai Li,He Long,Chunyu Xie,Dawei Leng,Baochang Zhang*

Main category: cs.CV

TL;DR: 本文提出Alignment-Aware Masked Learning (AML)训练策略，通过显式估计像素级视觉-语言对齐、过滤对齐不佳区域、聚焦可信线索来提升Referring Image Segmentation性能，在RefCOCO数据集上达到SOTA，并增强了对多样化描述和场景的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Referring Image Segmentation (RIS)任务需要根据自然语言描述分割图像中的目标对象。现有方法在像素级视觉-语言对齐方面存在不足，导致对多样化描述和复杂场景的鲁棒性有限。需要一种能显式建模对齐关系、过滤不可靠区域、聚焦可信线索的训练策略。

Method: 提出Alignment-Aware Masked Learning (AML)训练策略：1) 显式估计像素级视觉-语言对齐分数；2) 基于对齐分数过滤对齐不佳的区域，避免这些区域对优化产生负面影响；3) 聚焦于对齐良好的可信线索进行训练优化。

Result: 在RefCOCO系列数据集上取得了state-of-the-art性能。同时，该方法显著增强了对多样化语言描述和复杂场景的鲁棒性，表现出更好的泛化能力。

Conclusion: AML通过显式建模像素级视觉-语言对齐、过滤不可靠区域、聚焦可信线索，有效提升了RIS任务的性能和对多样化描述的鲁棒性，为视觉-语言对齐任务提供了新的训练策略思路。

Abstract: Referring Image Segmentation (RIS) aims to segment an object in an image identified by a natural language expression. The paper introduces Alignment-Aware Masked Learning (AML), a training strategy to enhance RIS by explicitly estimating pixel-level vision-language alignment, filtering out poorly aligned regions during optimization, and focusing on trustworthy cues. This approach results in state-of-the-art performance on RefCOCO datasets and also enhances robustness to diverse descriptions and scenarios

</details>


### [10] [ProjFlow: Projection Sampling with Flow Matching for Zero-Shot Exact Spatial Motion Control](https://arxiv.org/abs/2602.22742)
*Akihisa Watanabe,Qing Yu,Edgar Simo-Serra,Kent Fujiwara*

Main category: cs.CV

TL;DR: ProjFlow是一个无需训练、零样本的采样器，通过线性逆问题方法实现精确的空间约束满足，同时保持运动自然性，解决了现有方法需要任务特定训练或优化慢的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成具有精确空间控制的人类运动时存在局限性：通常需要任务特定训练或缓慢优化，且硬约束经常破坏运动的自然性。许多动画任务可以表述为线性逆问题，这为开发更有效的解决方案提供了机会。

Method: ProjFlow基于线性逆问题框架，引入了一种新颖的骨骼感知度量来编码骨骼拓扑结构。该度量使采样器能够通过在整个骨骼上协调分布修正来强制执行硬约束，避免朴素投影方法产生的不自然伪影。对于稀疏输入（如填充关键帧之间的长间隙），还引入了使用伪观测的时间变化公式，在采样过程中逐渐淡化。

Result: 在代表性应用（运动修复和2D到3D提升）上的广泛实验表明，ProjFlow实现了精确的约束满足，在零样本基线上匹配或提高了真实感，同时与基于训练的控制器保持竞争力。

Conclusion: ProjFlow通过线性逆问题方法和骨骼感知度量，实现了无需训练、零样本的精确空间约束满足，同时保持运动自然性，为人类运动生成提供了有效的解决方案。

Abstract: Generating human motion with precise spatial control is a challenging problem. Existing approaches often require task-specific training or slow optimization, and enforcing hard constraints frequently disrupts motion naturalness. Building on the observation that many animation tasks can be formulated as a linear inverse problem, we introduce ProjFlow, a training-free sampler that achieves zero-shot, exact satisfaction of linear spatial constraints while preserving motion realism. Our key advance is a novel kinematics-aware metric that encodes skeletal topology. This metric allows the sampler to enforce hard constraints by distributing corrections coherently across the entire skeleton, avoiding the unnatural artifacts of naive projection. Furthermore, for sparse inputs, such as filling in long gaps between a few keyframes, we introduce a time-varying formulation using pseudo-observations that fade during sampling. Extensive experiments on representative applications, motion inpainting, and 2D-to-3D lifting, demonstrate that ProjFlow achieves exact constraint satisfaction and matches or improves realism over zero-shot baselines, while remaining competitive with training-based controllers.

</details>


### [11] [Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval](https://arxiv.org/abs/2602.22759)
*Yuan-Chih Chen,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 提出统一隐藏码恢复框架，实现篡改图像的检索与恢复，兼容多种水印方案，并在ImageNet-S基准上验证性能


<details>
  <summary>Details</summary>
Motivation: 当前图像真实性研究主要关注深度伪造检测和定位，而篡改内容的恢复和事实检索相对未被充分探索，需要建立统一的恢复框架

Method: 通过多尺度向量量化将语义和感知信息编码为紧凑隐藏码表示，利用条件Transformer模块增强上下文推理，支持事后和生成时水印范式

Result: 在ImageNet-S基准测试中展现出有前景的检索和重建性能，完全兼容多种水印流程，为通用图像恢复奠定基础

Conclusion: 该框架超越了传统的检测和定位方法，为通用图像恢复建立了基础，支持篡改内容的检索和恢复

Abstract: Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.

</details>


### [12] [TrajTok: Learning Trajectory Tokens enables better Video Understanding](https://arxiv.org/abs/2602.22779)
*Chenhao Zheng,Jieyu Zhang,Jianing Zhang,Weikai Huang,Ashutosh Kumar,Quan Kong,Oncel Tuzel,Chun-Liang Li,Ranjay Krishna*

Main category: cs.CV

TL;DR: TrajTok是一种端到端视频分词器，通过时空像素聚类直接生成物体轨迹，解决了传统视频模型分词冗余问题，可动态适应语义复杂度，提高视频理解性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频模型通过分块化进行分词会产生大量冗余token，严重限制视频处理效率和可扩展性。现有的轨迹分词器虽然能解耦视频时长与token数量，但依赖复杂的外部分割和跟踪流程，速度慢且与任务无关。

Method: 提出TrajTok端到端视频分词器模块，完全集成并与视频模型协同训练，包含统一的segmenter，在时空维度对像素进行隐式聚类，单次前向传播直接生成物体轨迹，动态调整token粒度以适应语义复杂度。

Result: 基于TrajTok实现的TrajViT2视频CLIP模型在分类和检索基准测试中达到最佳精度，同时保持与最佳token合并方法相当的效率。TrajTok还可作为预训练视觉特征的探测头（TrajAdapter）或视觉语言模型的对齐连接器（TrajVLM），在长视频推理中表现优异。

Conclusion: TrajTok通过端到端学习生成语义感知的轨迹token，解决了视频分词冗余问题，提高了视频理解性能和效率，同时展现出作为多功能组件的潜力。

Abstract: Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.

</details>


### [13] [SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation](https://arxiv.org/abs/2602.22785)
*Ling Wang,Hao-Xiang Guo,Xinzhou Wang,Fuchun Sun,Kai Sun,Pengkun Liu,Hang Xiao,Zhong Wang,Guangyuan Fu,Eric Li,Yang Liu,Yikai Wang*

Main category: cs.CV

TL;DR: SceneTransporter是一个端到端框架，用于从单张图像生成结构化3D场景。它通过最优传输约束解决了现有方法在开放世界场景中无法将部件级3D对象组织成不同实例的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然能生成部件级3D对象，但在开放世界场景中往往无法将这些部件组织成不同的实例。通过去偏聚类探针分析发现，这种失败源于模型内部分配机制缺乏结构约束。

Method: 将结构化3D场景生成重新定义为全局相关分配问题。在组合DiT模型的去噪循环中，制定并求解熵最优传输目标。该公式施加两种结构约束：1）传输计划通过门控交叉注意力强制图像块到部件级3D潜在表示的一对一独占路由；2）传输的竞争性鼓励相似块的分组，并通过基于边缘的成本进行正则化。

Result: 大量实验表明，SceneTransporter在开放世界场景生成方面优于现有方法，显著提高了实例级连贯性和几何保真度。

Conclusion: SceneTransporter通过最优传输约束成功解决了结构化3D场景生成中的实例组织问题，为从单张图像生成高质量3D场景提供了有效解决方案。

Abstract: We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.

</details>


### [14] [Robust Human Trajectory Prediction via Self-Supervised Skeleton Representation Learning](https://arxiv.org/abs/2602.22791)
*Taishu Arashima,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: 提出一种结合自监督骨架表示模型的鲁棒轨迹预测方法，通过掩码自编码预训练处理遮挡导致的关节缺失问题，在遮挡场景下提升预测鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现实环境中的人体骨架数据常因遮挡导致关节缺失，这些干扰显著降低轨迹预测精度，需要更鲁棒的骨架表示方法

Method: 提出鲁棒轨迹预测方法，整合基于掩码自编码预训练的自监督骨架表示模型，增强对缺失骨架数据的处理能力

Result: 在遮挡场景下的实验结果显示，该方法在不牺牲预测精度的前提下提升了对缺失骨架数据的鲁棒性，在清洁到中度缺失情况下持续优于基线模型

Conclusion: 通过自监督骨架表示模型，能够有效处理现实环境中因遮挡导致的骨架数据缺失问题，提升轨迹预测的鲁棒性和准确性

Abstract: Human trajectory prediction plays a crucial role in applications such as autonomous navigation and video surveillance. While recent works have explored the integration of human skeleton sequences to complement trajectory information, skeleton data in real-world environments often suffer from missing joints caused by occlusions. These disturbances significantly degrade prediction accuracy, indicating the need for more robust skeleton representations. We propose a robust trajectory prediction method that incorporates a self-supervised skeleton representation model pretrained with masked autoencoding. Experimental results in occlusion-prone scenarios show that our method improves robustness to missing skeletal data without sacrificing prediction accuracy, and consistently outperforms baseline models in clean-to-moderate missingness regimes.

</details>


### [15] [GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation](https://arxiv.org/abs/2602.22800)
*Hanliang Du,Zhangji Lu,Zewei Cai,Qijian Tang,Qifeng Yu,Xiaoli Liu*

Main category: cs.CV

TL;DR: 提出GSTurb框架，结合光流引导的倾斜校正和高斯泼溅建模非等晕模糊，有效缓解大气湍流引起的图像退化


<details>
  <summary>Details</summary>
Motivation: 大气湍流导致长距离成像中的像素位移（倾斜）和模糊，严重影响图像质量，需要有效恢复方法

Method: 提出GSTurb框架，使用高斯参数表示倾斜和模糊，通过光流引导倾斜校正和高斯泼溅建模非等晕模糊，在多帧上优化参数

Result: 在ATSyn-static数据集上达到PSNR 27.67 dB和SSIM 0.8735，相比SOTA方法提升PSNR 1.3 dB（4.5%）和SSIM 0.048（5.8%），在真实数据集上也表现优异

Conclusion: 光流引导的倾斜校正与高斯泼溅结合能有效提升合成和真实湍流条件下的图像恢复效果

Abstract: Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.

</details>


### [16] [PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning](https://arxiv.org/abs/2602.22809)
*Mingde Yao,Zhiyuan You,Tam-King Man,Menglu Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: PhotoAgent是一个自主图像编辑系统，通过美学规划将图像编辑建模为长期决策问题，无需用户逐步指令即可自动执行多步编辑操作


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑方法质量高度依赖精心设计的指令，将任务分解和顺序规划的负担完全放在用户身上，需要实现更自主的图像编辑系统

Method: 将自主图像编辑建模为长期决策问题，通过理解用户美学意图、基于树搜索规划多步编辑动作、利用记忆和视觉反馈进行闭环迭代执行

Result: 相比基线方法，PhotoAgent在指令遵循和视觉质量方面都有显著提升，并引入了包含7,000张照片的美学评估基准UGC-Edit和包含1,017张照片的测试集

Conclusion: PhotoAgent通过明确的美学规划和长期决策方法，实现了高质量的自主图像编辑，无需用户逐步指令，在真实场景中表现出色

Abstract: With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.

</details>


### [17] [Face Time Traveller : Travel Through Ages Without Losing Identity](https://arxiv.org/abs/2602.22819)
*Purbayan Kar,Ayush Ghadiya,Vishal Chudasama,Pankaj Wasnik,C. V. Jawahar*

Main category: cs.CV

TL;DR: FaceTT是一个基于扩散模型的框架，通过面部属性感知提示细化、免调谐角度反演和自适应注意力控制机制，实现了高保真、身份一致的面部年龄变换。


<details>
  <summary>Details</summary>
Motivation: 面部年龄变换是一个病态问题，受环境和遗传因素影响，在娱乐、法医学和数字档案等领域有重要应用。现有方法依赖数值年龄表示，忽视了生物和上下文线索的相互作用，在宽年龄变换中难以保持身份一致性，且扩散模型的静态注意力和优化繁重的反演限制了适应性、细粒度控制和背景一致性。

Method: 1. 面部属性感知提示细化策略：编码内在（生物）和外在（环境）老化线索进行上下文感知条件化；2. 免调谐角度反演方法：高效将真实面部映射到扩散潜在空间，实现快速准确重建；3. 自适应注意力控制机制：动态平衡用于语义老化线索的交叉注意力和用于结构及身份保持的自注意力。

Result: 在基准数据集和野外测试集上的广泛实验表明，FaceTT在身份保持、背景保留和老化真实性方面优于最先进的方法。

Conclusion: FaceTT通过创新的提示细化、反演和注意力控制机制，成功解决了现有面部年龄变换方法在身份保持、背景一致性和老化真实性方面的局限性，实现了高质量的面部年龄变换。

Abstract: Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.

</details>


### [18] [CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation](https://arxiv.org/abs/2602.22821)
*Tong Wang,Yaolei Qi,Siwen Wang,Imran Razzak,Guanyu Yang,Yutong Xie*

Main category: cs.CV

TL;DR: CMSA-Net是一个用于视频息肉分割的框架，通过因果多尺度聚合模块和动态多源参考策略，在保持实时性的同时提高了分割准确性。


<details>
  <summary>Details</summary>
Motivation: 视频息肉分割面临两个主要挑战：1）息肉与周围黏膜外观相似，语义区分度弱；2）息肉在视频帧间位置和尺度变化大，导致稳定准确分割困难。

Method: 提出CMSA-Net框架，包含两个核心组件：1）因果多尺度聚合模块，通过因果注意力按时间顺序聚合多尺度历史帧语义信息；2）动态多源参考策略，基于语义可分性和预测置信度自适应选择信息丰富且可靠的参考帧。

Result: 在SUN-SEG数据集上的大量实验表明，CMSA-Net达到了最先进的性能，在分割准确性和实时临床适用性之间取得了良好平衡。

Conclusion: CMSA-Net通过因果多尺度特征聚合和动态参考帧选择，有效解决了视频息肉分割中的语义模糊性和时空变化问题，为临床实时应用提供了可靠解决方案。

Abstract: Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.

</details>


### [19] [Chain of Flow: A Foundational Generative Framework for ECG-to-4D Cardiac Digital Twins](https://arxiv.org/abs/2602.22919)
*Haofan Wu,Nay Aung,Theodoros N. Arvanitis,Joao A. C. Lima,Steffen E. Petersen,Le Zhang*

Main category: cs.CV

TL;DR: Chain of Flow (COF) 是一个从单次心动周期心电图重建完整4D心脏结构和运动的基础性生成框架，将心脏数字孪生从狭窄的预测模型转变为完全生成式的患者特异性虚拟心脏。


<details>
  <summary>Details</summary>
Motivation: 现有的心脏数字孪生框架局限于特定任务的预测器，而不是构建患者特异性、可操作的虚拟心脏。临床可操作的心脏数字孪生需要重建个体化心脏解剖和生理，从多模态信号更新内部状态，并支持广泛的下游模拟任务。

Method: COF是一个基于心电图驱动的生成框架，在训练过程中整合电影磁共振成像和12导联心电图，学习心脏几何、电生理和运动动力学的统一表示，从单个心动周期重建完整的4D心脏结构和运动。

Result: 在多样化队列上评估显示，COF能够准确恢复心脏解剖结构、各心腔功能和动态运动模式。重建的4D心脏进一步支持下游心脏数字孪生任务，如容量测量、区域功能分析和虚拟电影合成。

Conclusion: 通过直接从心电图实现完整的4D器官重建，COF将心脏数字孪生从狭窄的预测模型转变为完全生成式的患者特异性虚拟心脏，为临床可操作的心脏数字孪生提供了基础框架。

Abstract: A clinically actionable Cardiac Digital Twin (CDT) should reconstruct individualised cardiac anatomy and physiology, update its internal state from multimodal signals, and enable a broad range of downstream simulations beyond isolated tasks. However, existing CDT frameworks remain limited to task-specific predictors rather than building a patient-specific, manipulable virtual heart. In this work, we introduce Chain of Flow (COF), a foundational ECG-driven generative framework that reconstructs full 4D cardiac structure and motion from a single cardiac cycle. The method integrates cine-CMR and 12-lead ECG during training to learn a unified representation of cardiac geometry, electrophysiology, and motion dynamics. We evaluate Chain of Flow on diverse cohorts and demonstrate accurate recovery of cardiac anatomy, chamber-wise function, and dynamic motion patterns. The reconstructed 4D hearts further support downstream CDT tasks such as volumetry, regional function analysis, and virtual cine synthesis. By enabling full 4D organ reconstruction directly from ECG, COF transforms cardiac digital twins from narrow predictive models into fully generative, patient-specific virtual hearts. Code will be released after review.

</details>


### [20] [Cytoarchitecture in Words: Weakly Supervised Vision-Language Modeling for Human Brain Microscopy](https://arxiv.org/abs/2602.23088)
*Matthew Sutton,Katrin Amunts,Timo Dickscheid,Christian Schiffer*

Main category: cs.CV

TL;DR: 提出一种标签介导的方法，通过标签而非配对图像-文本数据，将视觉基础模型与语言模型连接，为显微图像生成自然语言描述


<details>
  <summary>Details</summary>
Motivation: 在许多研究和临床环境中，配对的图像-文本数据稀缺且难以获取，这限制了视觉-语言耦合模型在显微图像分析中的应用

Method: 使用标签作为中介，从相关文献自动挖掘区域描述作为合成标题，通过图像到文本训练目标将细胞结构视觉基础模型（CytoNet）与大型语言模型连接

Result: 在57个脑区上，该方法生成合理的区域描述，对范围内斑块的细胞结构参考标签匹配准确率达90.6%，在掩码区域标签的8项测试中仍能以68.6%的准确率恢复区域

Conclusion: 弱标签介导的配对足以连接现有的生物医学视觉基础模型与语言模型，为配对注释稀缺的领域提供自然语言集成的实用方案

Abstract: Foundation models increasingly offer potential to support interactive, agentic workflows that assist researchers during analysis and interpretation of image data. Such workflows often require coupling vision to language to provide a natural-language interface. However, paired image-text data needed to learn this coupling are scarce and difficult to obtain in many research and clinical settings. One such setting is microscopic analysis of cell-body-stained histological human brain sections, which enables the study of cytoarchitecture: cell density and morphology and their laminar and areal organization. Here, we propose a label-mediated method that generates meaningful captions from images by linking images and text only through a label, without requiring curated paired image-text data. Given the label, we automatically mine area descriptions from related literature and use them as synthetic captions reflecting canonical cytoarchitectonic attributes. An existing cytoarchitectonic vision foundation model (CytoNet) is then coupled to a large language model via an image-to-text training objective, enabling microscopy regions to be described in natural language. Across 57 brain areas, the resulting method produces plausible area-level descriptions and supports open-set use through explicit rejection of unseen areas. It matches the cytoarchitectonic reference label for in-scope patches with 90.6% accuracy and, with the area label masked, its descriptions remain discriminative enough to recover the area in an 8-way test with 68.6% accuracy. These results suggest that weak, label-mediated pairing can suffice to connect existing biomedical vision foundation models to language, providing a practical recipe for integrating natural-language in domains where fine-grained paired annotations are scarce.

</details>


### [21] [DyaDiT: A Multi-Modal Diffusion Transformer for Socially Favorable Dyadic Gesture Generation](https://arxiv.org/abs/2602.23165)
*Yichen Peng,Jyun-Ting Song,Siyeol Jung,Ruofan Liu,Haiyang Liu,Xuangeng Chu,Ruicong Liu,Erwin Wu,Hideki Koike,Kris Kitani*

Main category: cs.CV

TL;DR: DyaDiT：基于多模态扩散Transformer的对话手势生成模型，能够从双人对话音频中生成考虑社交上下文和互动动态的逼真手势


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将单一音频流映射到单一说话者的动作，没有考虑社交上下文或建模对话中两人之间的相互动态，导致生成的对话手势不够自然和社交参与性不足

Method: 提出DyaDiT多模态扩散Transformer，从双人音频信号生成上下文适当的人类动作。模型融合双说话者信息捕捉互动动态，使用动作字典编码动作先验，可选择性地利用对话伙伴的手势生成更具响应性的动作

Result: 在标准动作生成指标上超越现有方法，用户定量研究显示用户强烈偏好DyaDiT生成的动作，证明其鲁棒性和社交友好的动作生成能力

Conclusion: DyaDiT能够生成考虑社交上下文和互动动态的逼真对话手势，显著提升数字人类交互的自然性和社交参与度，为社交互动场景提供了有效的解决方案

Abstract: Generating realistic conversational gestures are essential for achieving natural, socially engaging interactions with digital humans. However, existing methods typically map a single audio stream to a single speaker's motion, without considering social context or modeling the mutual dynamics between two people engaging in conversation. We present DyaDiT, a multi-modal diffusion transformer that generates contextually appropriate human motion from dyadic audio signals. Trained on Seamless Interaction Dataset, DyaDiT takes dyadic audio with optional social-context tokens to produce context-appropriate motion. It fuses information from both speakers to capture interaction dynamics, uses a motion dictionary to encode motion priors, and can optionally utilize the conversational partner's gestures to produce more responsive motion. We evaluate DyaDiT on standard motion generation metrics and conduct quantitative user studies, demonstrating that it not only surpasses existing methods on objective metrics but is also strongly preferred by users, highlighting its robustness and socially favorable motion generation. Code and models will be released upon acceptance.

</details>


### [22] [Learning Continuous Wasserstein Barycenter Space for Generalized All-in-One Image Restoration](https://arxiv.org/abs/2602.23169)
*Xiaole Tang,Xiaoyi He,Jiayi Xu,Xiang Gu,Jian Sun*

Main category: cs.CV

TL;DR: BaryIR：一种基于Wasserstein重心空间的对齐多源退化特征的表示学习框架，通过解耦退化无关的共享内容和退化特定的知识，提升全场景图像恢复模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有的一体化图像恢复方法在面对分布外退化时泛化能力有限，作者观察到多源退化特征分布是由不同退化特定偏移从底层退化无关分布中诱导产生的，恢复这种共享分布对于实现跨退化泛化至关重要

Method: 提出BaryIR框架，在Wasserstein重心空间中对齐多源退化特征，该空间通过最小化到多源退化分布的Wasserstein距离平均值来建模退化无关分布。引入残差子空间，其嵌入相互对比同时保持与WB嵌入正交，从而显式解耦两个正交空间：编码跨退化共享的退化无关不变内容的WB空间，以及自适应保留退化特定知识的残差子空间

Result: BaryIR在性能上与最先进的一体化方法相当，在未见退化类型和级别上表现出良好的泛化能力，即使在有限退化类型上训练并在混合退化的真实世界数据上评估时，也能学习到具有显著鲁棒性的泛化特征

Conclusion: 通过解耦退化无关的共享内容和退化特定的知识，BaryIR能够减轻对分布内退化的过拟合，实现基于退化无关共享不变性的自适应恢复，从而提升一体化图像恢复模型在真实场景中的泛化能力

Abstract: Despite substantial advances in all-in-one image restoration for addressing diverse degradations within a unified model, existing methods remain vulnerable to out-of-distribution degradations, thereby limiting their generalization in real-world scenarios. To tackle the challenge, this work is motivated by the intuition that multisource degraded feature distributions are induced by different degradation-specific shifts from an underlying degradation-agnostic distribution, and recovering such a shared distribution is thus crucial for achieving generalization across degradations. With this insight, we propose BaryIR, a representation learning framework that aligns multisource degraded features in the Wasserstein barycenter (WB) space, which models a degradation-agnostic distribution by minimizing the average of Wasserstein distances to multisource degraded distributions. We further introduce residual subspaces, whose embeddings are mutually contrasted while remaining orthogonal to the WB embeddings. Consequently, BaryIR explicitly decouples two orthogonal spaces: a WB space that encodes the degradation-agnostic invariant contents shared across degradations, and residual subspaces that adaptively preserve the degradation-specific knowledge. This disentanglement mitigates overfitting to in-distribution degradations and enables adaptive restoration grounded on the degradation-agnostic shared invariance. Extensive experiments demonstrate that BaryIR performs competitively against state-of-the-art all-in-one methods. Notably, BaryIR generalizes well to unseen degradations (\textit{e.g.,} types and levels) and shows remarkable robustness in learning generalized features, even when trained on limited degradation types and evaluated on real-world data with mixed degradations.

</details>


### [23] [Phys-3D: Physics-Constrained Real-Time Crowd Tracking and Counting on Railway Platforms](https://arxiv.org/abs/2602.23177)
*Bin Zeng,Johannes Künzel,Anna Hilsmann,Peter Eisert*

Main category: cs.CV

TL;DR: 提出一种基于物理约束的实时人群计数方法，利用安装在列车上的单摄像头在进站时扫描站台，通过物理约束的卡尔曼滤波模型和虚拟计数带实现精确计数。


<details>
  <summary>Details</summary>
Motivation: 铁路站台实时人群计数对安全和容量管理至关重要。现有跟踪检测方法通常假设静态摄像头或忽略物理运动一致性，在列车进站时的动态条件下计数不可靠。

Method: 提出物理约束跟踪框架，将检测、外观和3D运动推理统一到实时流程中。集成迁移学习的YOLOv11m检测器和EfficientNet-B0外观编码到DeepSORT中，引入物理约束卡尔曼模型(Phys-3D)通过针孔几何强制物理合理的3D运动动力学。针对遮挡问题，实现带持久性的虚拟计数带。

Result: 在MOT-RailwayPlatformCrowdHead数据集上，该方法将计数误差降低到2.97%，在运动和遮挡条件下表现出鲁棒性能。

Conclusion: 结合第一性原理几何和运动先验能够在安全关键的交通场景中实现可靠的人群计数，有助于有效的列车调度和站台安全管理。

Abstract: Accurate, real-time crowd counting on railway platforms is essential for safety and capacity management. We propose to use a single camera mounted in a train, scanning the platform while arriving. While hardware constraints are simple, counting remains challenging due to dense occlusions, camera motion, and perspective distortions during train arrivals. Most existing tracking-by-detection approaches assume static cameras or ignore physical consistency in motion modeling, leading to unreliable counting under dynamic conditions. We propose a physics-constrained tracking framework that unifies detection, appearance, and 3D motion reasoning in a real-time pipeline. Our approach integrates a transfer-learned YOLOv11m detector with EfficientNet-B0 appearance encoding within DeepSORT, while introducing a physics-constrained Kalman model (Phys-3D) that enforces physically plausible 3D motion dynamics through pinhole geometry. To address counting brittleness under occlusions, we implement a virtual counting band with persistence. On our platform benchmark, MOT-RailwayPlatformCrowdHead Dataset(MOT-RPCH), our method reduces counting error to 2.97%, demonstrating robust performance despite motion and occlusions. Our results show that incorporating first-principles geometry and motion priors enables reliable crowd counting in safety-critical transportation scenarios, facilitating effective train scheduling and platform safety management.

</details>


### [24] [Uni-Animator: Towards Unified Visual Colorization](https://arxiv.org/abs/2602.23191)
*Xinyuan Chen,Yao Xu,Shaowen Wang,Pengjie Song,Bowen Deng*

Main category: cs.CV

TL;DR: Uni-Animator是一个基于扩散Transformer的统一图像和视频草图着色框架，解决了现有方法在颜色转移不精确、物理细节保留不足和时间一致性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有草图着色方法难以统一图像和视频任务，存在以下问题：1）使用单个或多个参考时颜色转移不精确；2）高频物理细节保留不足；3）大运动场景中时间一致性差，存在运动伪影。

Method: 提出三个关键技术：1）通过实例补丁嵌入实现视觉参考增强，精确对齐和融合参考颜色信息；2）使用物理特征进行物理细节强化，有效捕捉和保留高频纹理；3）基于草图的动态RoPE编码，自适应建模运动感知的时空依赖关系。

Result: 大量实验结果表明，Uni-Animator在图像和视频草图着色任务上都取得了有竞争力的性能，与任务特定方法相当，同时实现了统一的跨域能力，具有高细节保真度和鲁棒的时间一致性。

Conclusion: Uni-Animator是一个创新的统一框架，成功解决了草图着色中颜色转移、细节保留和时间一致性的关键挑战，为图像和视频草图着色提供了统一的解决方案。

Abstract: We propose Uni-Animator, a novel Diffusion Transformer (DiT)-based framework for unified image and video sketch colorization. Existing sketch colorization methods struggle to unify image and video tasks, suffering from imprecise color transfer with single or multiple references, inadequate preservation of high-frequency physical details, and compromised temporal coherence with motion artifacts in large-motion scenes. To tackle imprecise color transfer, we introduce visual reference enhancement via instance patch embedding, enabling precise alignment and fusion of reference color information. To resolve insufficient physical detail preservation, we design physical detail reinforcement using physical features that effectively capture and retain high-frequency textures. To mitigate motion-induced temporal inconsistency, we propose sketch-based dynamic RoPE encoding that adaptively models motion-aware spatial-temporal dependencies. Extensive experimental results demonstrate that Uni-Animator achieves competitive performance on both image and video sketch colorization, matching that of task-specific methods while unlocking unified cross-domain capabilities with high detail fidelity and robust temporal consistency.

</details>


### [25] [Multidimensional Task Learning: A Unified Tensor Framework for Computer Vision Tasks](https://arxiv.org/abs/2602.23217)
*Alaa El Ichi,Khalide Jbilou*

Main category: cs.CV

TL;DR: 本文提出了多维任务学习（MTL）框架，基于广义爱因斯坦MLP（GE-MLPs），通过爱因斯坦积直接在张量上操作，突破了传统矩阵思维对计算机视觉任务的限制。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉任务表述受限于基于矩阵的思维：标准架构依赖矩阵值权重和向量值偏置，需要进行结构扁平化，这限制了自然可表达任务的空间。作者认为需要一种更通用的数学框架来统一理解不同的视觉任务。

Method: 提出基于广义爱因斯坦MLP（GE-MLPs）的多维任务学习（MTL）框架，使用张量值参数，通过爱因斯坦积直接在张量上操作，能够明确控制哪些维度被保留或收缩而不丢失信息。

Result: 通过严格的数学推导证明分类、分割和检测都是MTL的特殊情况，仅在形式化定义的任务空间中的维度配置不同。证明该任务空间严格大于基于矩阵的表述所能原生表达的空间，支持时空或跨模态预测等需要原则性任务配置的场景。

Conclusion: 这项工作为通过张量代数视角理解、比较和设计计算机视觉任务提供了数学基础，突破了传统矩阵思维的局限性，为更丰富的任务表达提供了理论框架。

Abstract: This paper introduces Multidimensional Task Learning (MTL), a unified mathematical framework based on Generalized Einstein MLPs (GE-MLPs) that operate directly on tensors via the Einstein product. We argue that current computer vision task formulations are inherently constrained by matrix-based thinking: standard architectures rely on matrix-valued weights and vectorvalued biases, requiring structural flattening that restricts the space of naturally expressible tasks. GE-MLPs lift this constraint by operating with tensor-valued parameters, enabling explicit control over which dimensions are preserved or contracted without information loss. Through rigorous mathematical derivations, we demonstrate that classification, segmentation, and detection are special cases of MTL, differing only in their dimensional configuration within a formally defined task space. We further prove that this task space is strictly larger than what matrix-based formulations can natively express, enabling principled task configurations such as spatiotemporal or cross modal predictions that require destructive flattening under conventional approaches. This work provides a mathematical foundation for understanding, comparing, and designing computer vision tasks through the lens of tensor algebra.

</details>


### [26] [PRIMA: Pre-training with Risk-integrated Image-Metadata Alignment for Medical Diagnosis via LLM](https://arxiv.org/abs/2602.23297)
*Yiqing Wang,Chunming He,Ming-Chen Lu,Mercy Pawar,Leslie Niziol,Maria Woodward,Sina Farsiu*

Main category: cs.CV

TL;DR: PRIMA是一个多模态医学诊断框架，通过整合视觉特征和临床元数据，利用预训练和风险集成图像-元数据对齐技术，显著提升疾病分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学诊断方法通常将临床元数据视为孤立标签，未能充分利用临床描述中丰富的语义知识，导致多模态表示学习效果受限。

Method: 1. 通过检索增强生成(RAG)构建风险-疾病关联专家语料库，精炼Clinical ModernBERT文本编码器；2. 采用DINOv3和精炼BERT的双编码器预训练策略；3. 设计四种互补损失函数实现多粒度语义对齐；4. 使用Qwen-3融合对齐特征进行疾病分类。

Result: PRIMA在实验中显著优于其他最先进方法，有效协调像素级特征与抽象临床专业知识，且无需大规模数据收集或大量计算资源即实现优越的鲁棒性。

Conclusion: PRIMA框架成功将领域特定知识整合到多模态表示学习中，通过风险集成图像-元数据对齐有效提升了医学诊断的准确性和鲁棒性。

Abstract: Medical diagnosis requires the effective synthesis of visual manifestations and clinical metadata. However, existing methods often treat metadata as isolated tags, failing to exploit the rich semantic knowledge embedded in clinical descriptions. We propose PRIMA (Pre-training with Risk-integrated Image-Metadata Alignment), a framework that integrates domain-specific knowledge into multi-modal representation learning. We first curate an expert corpus of risk-disease correlations via Retrieval-Augmented Generation (RAG) to refine Clinical ModernBERT, embedding diagnostic priors into the text encoder. To bridge the modality gap, we introduce a dual-encoder pre-training strategy utilizing DINOv3 and our refined BERT, optimized by a suite of four complementary loss functions. These losses are designed to capture multi-granular semantic alignment and handle the ambiguity of clinical correlations through soft labels. Finally, we leverage Qwen-3 to fuse these aligned features for precise disease classification. Extensive experiments demonstrate that PRIMA effectively harmonizes pixel-level features with abstract clinical expertise, significantly outperforming other state-of-the-art methods. Notably, our framework achieves superior robustness without the need for massive data collection or exhaustive computational resources. Our code will be made public upon acceptance.

</details>


### [27] [ThinkOmni: Lifting Textual Reasoning to Omni-modal Scenarios via Guidance Decoding](https://arxiv.org/abs/2602.23306)
*Yiran Guan,Sifan Tu,Dingkang Liang,Linghao Zhu,Jianzhong Ju,Zhenbo Luo,Jian Luan,Yuliang Liu,Xiang Bai*

Main category: cs.CV

TL;DR: ThinkOmni是一个无需训练和数据的框架，通过利用现成的大型推理模型指导全模态大语言模型的解码过程，将文本推理能力提升到全模态场景


<details>
  <summary>Details</summary>
Motivation: 现有的全模态大语言模型虽然能够感知多种模态，但缺乏复杂推理能力，而通过额外训练增强推理能力面临高质量数据需求、任务特定适应和巨大计算成本等挑战

Method: 提出ThinkOmni框架，包含两个关键组件：1) LRM-as-a-Guide：利用现成的大型推理模型指导全模态大语言模型的解码过程；2) Stepwise Contrastive Scaling：自适应平衡感知和推理信号，无需手动超参数调优

Result: 在六个多模态推理基准测试中，ThinkOmni始终带来性能提升，主要结果在MathVista上达到70.2分，在MMAU上达到75.5分

Conclusion: ThinkOmni为全模态推理提供了一个灵活且可泛化的解决方案，并为推理能力的泛化和应用提供了新的见解

Abstract: Omni-modal reasoning is essential for intelligent systems to understand and draw inferences from diverse data sources. While existing omni-modal large language models (OLLM) excel at perceiving diverse modalities, they lack the complex reasoning abilities of recent large reasoning models (LRM). However, enhancing the reasoning ability of OLLMs through additional training presents significant challenges, including the need for high-quality data, task-specific adaptation, and substantial computational costs. To address these limitations, we propose ThinkOmni, a training-free and data-free framework that lifts textual reasoning to omni-modal scenarios. ThinkOmni introduces two key components: 1) LRM-as-a-Guide, which leverages off-the-shelf LRMs to guide the OLLM decoding process; 2) Stepwise Contrastive Scaling, which adaptively balances perception and reasoning signals without manual hyperparameter tuning. Experiments on six multi-modal reasoning benchmarks demonstrate that ThinkOmni consistently delivers performance improvements, with main results achieving 70.2 on MathVista and 75.5 on MMAU. Overall, ThinkOmni offers a flexible and generalizable solution for omni-modal reasoning and provides new insights into the generalization and application of reasoning capabilities.

</details>


### [28] [Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?](https://arxiv.org/abs/2602.23339)
*Tilemachos Aravanis,Vladan Stojnić,Bill Psomas,Nikos Komodakis,Giorgos Tolias*

Main category: cs.CV

TL;DR: 本文提出了一种检索增强的测试时适配器，通过融合文本和视觉支持特征来学习轻量级的每图像分类器，显著缩小了零样本与全监督分割之间的差距。


<details>
  <summary>Details</summary>
Motivation: 开放词汇分割（OVS）虽然扩展了视觉语言模型的零样本识别能力，但仍落后于全监督方法，主要面临两个挑战：训练VLM时使用的粗粒度图像级监督，以及自然语言的语义模糊性。

Method: 引入少样本设置，通过像素标注图像的支持集增强文本提示；提出检索增强的测试时适配器，通过学习每图像分类器来融合文本和视觉支持特征，实现模态间的更强协同。

Result: 实验表明，该方法显著缩小了零样本分割与监督分割之间的差距，同时保持了开放词汇能力，并适用于细粒度任务如个性化分割。

Conclusion: 通过融合文本和视觉支持特征的检索增强测试时适配器，有效解决了开放词汇分割中的语义模糊性问题，提升了分割性能，为细粒度分割任务提供了有效解决方案。

Abstract: Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.

</details>
